{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dcde952ff3a94cf78e9c9375b6338647": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e03a93f6221402dac1f1806b3ebc39d",
              "IPY_MODEL_33a956fad69a4f26a1e26773a29f541e",
              "IPY_MODEL_9be5bc2a7a82462facc0575b120f77eb"
            ],
            "layout": "IPY_MODEL_5676de9f2156424d8ca25f401cbe5410"
          }
        },
        "8e03a93f6221402dac1f1806b3ebc39d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0966ab8563944f3e942e727be1a39bb4",
            "placeholder": "​",
            "style": "IPY_MODEL_3be0b4fc111b4e4680ed2258c5394d2c",
            "value": "Map: 100%"
          }
        },
        "33a956fad69a4f26a1e26773a29f541e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b42686e1ab7438a844ac2df37718a39",
            "max": 735,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b5fcf400b2c444f8aac9a1b5acff8ed",
            "value": 735
          }
        },
        "9be5bc2a7a82462facc0575b120f77eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_230b0577fccd499cb63c09d56c3308bc",
            "placeholder": "​",
            "style": "IPY_MODEL_e001ce6890c04dc7aa22088fbc2a1fb0",
            "value": " 735/735 [00:13&lt;00:00, 56.13 examples/s]"
          }
        },
        "5676de9f2156424d8ca25f401cbe5410": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0966ab8563944f3e942e727be1a39bb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3be0b4fc111b4e4680ed2258c5394d2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b42686e1ab7438a844ac2df37718a39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b5fcf400b2c444f8aac9a1b5acff8ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "230b0577fccd499cb63c09d56c3308bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e001ce6890c04dc7aa22088fbc2a1fb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57526687cb1f434ea4757a0f8db734e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fc2a7cc76bde48a68e567475b48a71ab",
              "IPY_MODEL_7d534351f1ed45c588a57499c0f9b9a7",
              "IPY_MODEL_643a2c6c7151496fa3f04a42b83811a9"
            ],
            "layout": "IPY_MODEL_a867a72d4ae04e6bb12fb23e378d7a9f"
          }
        },
        "fc2a7cc76bde48a68e567475b48a71ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b97129bb397e423292b67277039c0bac",
            "placeholder": "​",
            "style": "IPY_MODEL_29b16939a0834d89b8a3a67d7cebfe23",
            "value": "Map: 100%"
          }
        },
        "7d534351f1ed45c588a57499c0f9b9a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b5ff6120c674f6c9090953a528dbb5e",
            "max": 82,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_27e269ad370b488babde724601015726",
            "value": 82
          }
        },
        "643a2c6c7151496fa3f04a42b83811a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b08dba783e14622911f27a41a93e366",
            "placeholder": "​",
            "style": "IPY_MODEL_488a98546f7b4bc89d922e5b03b798a3",
            "value": " 82/82 [00:01&lt;00:00, 62.66 examples/s]"
          }
        },
        "a867a72d4ae04e6bb12fb23e378d7a9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b97129bb397e423292b67277039c0bac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29b16939a0834d89b8a3a67d7cebfe23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b5ff6120c674f6c9090953a528dbb5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27e269ad370b488babde724601015726": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2b08dba783e14622911f27a41a93e366": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "488a98546f7b4bc89d922e5b03b798a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from datasets import Dataset  # instead of load_dataset\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "# Model and tokenizer\n",
        "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    attn_implementation=\"flex_attention\",\n",
        "    device_map=\"auto\"\n",
        ").to(\"cuda\")\n",
        "#model.config.sliding_window = None\n",
        "model.config.use_cache = False\n",
        "\n",
        "# Step 1: Load Excel file\n",
        "df = pd.read_excel(\"/content/output_with_token_counts.xlsx\")  # Update with your actual file path\n",
        "\n",
        "# Optional: Check required columns\n",
        "assert {\"question\", \"solution\", \"answer\"}.issubset(df.columns), \"Excel file must contain 'question', 'solution', 'answer' columns\"\n",
        "\n",
        "# Step 2: Convert to HuggingFace Dataset\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# Optional: Split into train/validation\n",
        "dataset = dataset.train_test_split(test_size=0.1)\n",
        "train_dataset = dataset[\"train\"]\n",
        "eval_dataset = dataset[\"test\"]\n",
        "\n",
        "# Step 3: Tokenization\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "# Step 3: Tokenization\n",
        "tokenizer.pad_token = tokenizer.eos_token  # ensure pad_token_id is set\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "MAX_LEN = 2048\n",
        "\n",
        "def tokenize_for_chain(examples):\n",
        "    input_id_batches = []\n",
        "    label_batches = []\n",
        "\n",
        "    for q, sol, ans in zip(examples[\"question\"],\n",
        "                           examples[\"solution\"],\n",
        "                           examples[\"answer\"]):\n",
        "        prompt = f\"Question: {q}\\nReasoning:\"\n",
        "        target = f\" {sol} Answer: {ans}\"\n",
        "\n",
        "        # Tokenize prompt with special tokens\n",
        "        p_ids = tokenizer.encode(prompt, add_special_tokens=True)\n",
        "        # Tokenize target without special tokens\n",
        "        t_ids = tokenizer.encode(target, add_special_tokens=False)\n",
        "\n",
        "        # How many tokens we can allocate to t_ids + eos\n",
        "        avail_len = MAX_LEN - len(p_ids)\n",
        "        if avail_len <= 0:\n",
        "            # prompt alone is too long—truncate the prompt\n",
        "            p_ids = p_ids[-MAX_LEN:]\n",
        "            input_ids = p_ids\n",
        "            labels = [-100] * len(p_ids)\n",
        "        else:\n",
        "            # we need at least 1 slot for eos\n",
        "            t_ids = t_ids[: avail_len - 1]\n",
        "            # Concat prompt + truncated target + eos\n",
        "            input_ids = p_ids + t_ids + [tokenizer.eos_token_id]\n",
        "            # Mask prompt, keep target+eos as labels\n",
        "            labels   = [-100] * len(p_ids) + t_ids + [tokenizer.eos_token_id]\n",
        "\n",
        "        input_id_batches.append(input_ids)\n",
        "        label_batches.append(labels)\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_id_batches,\n",
        "        \"labels\":   label_batches\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# use a collator that respects padding and returns tensors\n",
        "#data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "#tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
        "#tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
        "#tokenized_train = tokenized_train.remove_columns([\"question\", \"solution\", \"answer\", \"__index__\"])\n",
        "# After you map with tokenize_function, drop the three text columns:\n",
        "# Now map it over your datasets:\n",
        "tokenized_train = train_dataset.map(\n",
        "    tokenize_for_chain,\n",
        "    batched=True,\n",
        "    remove_columns=[\"question\",\"solution\",\"answer\"],\n",
        "    load_from_cache_file=False,     # <<< disable the cache\n",
        ")\n",
        "tokenized_eval = eval_dataset.map(\n",
        "    tokenize_for_chain,\n",
        "    batched=True,\n",
        "    remove_columns=[\"question\",\"solution\",\"answer\"],\n",
        "    load_from_cache_file=False,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "#tokenized_eval = eval_dataset.map(tokenize_function, batched=True)\n",
        "from transformers import DataCollatorWithPadding\n",
        "import torch\n",
        "\n",
        "# 1) Base collator just pads inputs+labels with pad_token_id\n",
        "base_collator = DataCollatorWithPadding(\n",
        "    tokenizer=tokenizer,\n",
        "    padding=\"longest\",\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# 2) Wrap it to fix the labels post-pad\n",
        "def data_collator(batch):\n",
        "    batch = base_collator(batch)\n",
        "    pad = tokenizer.pad_token_id\n",
        "    # wherever labels == pad, set to -100\n",
        "    batch[\"labels\"] = torch.where(\n",
        "        batch[\"labels\"] == pad,\n",
        "        torch.full_like(batch[\"labels\"], -100),\n",
        "        batch[\"labels\"],\n",
        "    )\n",
        "    return batch\n",
        "from transformers import TrainerCallback\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./qwen2.5_finetuned_limo\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=15,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=1,\n",
        "    #learning_rate=5.0e-6,\n",
        "    learning_rate=5e-5,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.0,\n",
        "    warmup_steps=10,\n",
        "    logging_steps=1,\n",
        "    save_strategy=\"epoch\",\n",
        "    ddp_timeout=180000000,\n",
        "    bf16=True,\n",
        "    fp16=False,\n",
        "    push_to_hub=False,\n",
        "    remove_unused_columns=False,   # ← add it here\n",
        "    eval_strategy=\"no\",\n",
        "   # eval_steps=10\n",
        ")\n",
        "class DebugMetricsCallback(TrainerCallback):\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        print(f\"\\n>>> Step {state.global_step} metrics:\", logs)\n",
        "\n",
        "class InputLoggingCallback(TrainerCallback):\n",
        "    def __init__(self):\n",
        "        self.train_iter = None\n",
        "\n",
        "    def on_train_begin(self, args, state, control, **kwargs):\n",
        "        self.train_iter = iter(kwargs[\"train_dataloader\"])\n",
        "\n",
        "    def on_step_begin(self, args, state, control, **kwargs):\n",
        "        if state.global_step < 5:\n",
        "            batch = next(self.train_iter)\n",
        "            print(f\"\\nStep {state.global_step}:\")\n",
        "            print(\"Input IDs Shape:\", batch[\"input_ids\"].shape)\n",
        "            print(\"Labels Shape:\", batch[\"labels\"].shape)\n",
        "            print(\"Decoded Input:\", tokenizer.decode(batch[\"input_ids\"][0].tolist()))\n",
        "            print(\"Decoded Labels (ignoring -100):\", tokenizer.decode([x for x in batch[\"labels\"][0].tolist() if x != -100]))\n",
        "\n",
        "# Add callback to trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "   # callbacks=[InputLoggingCallback()]\n",
        "    callbacks=[DebugMetricsCallback()],\n",
        ")\n",
        "\n",
        "#trainer.train()\n",
        "# Disable torch.compile explicitly to avoid potential compilation issues with flex_attention and specific data shapes\n",
        "torch._dynamo.config.disable = True\n",
        "trainer.train()\n",
        "# Save final model and tokenizer\n",
        "model.save_pretrained(\"./qwen2.5_finetuned_limo\")\n",
        "tokenizer.save_pretrained(\"./qwen2.5_finetuned_limo\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "dcde952ff3a94cf78e9c9375b6338647",
            "8e03a93f6221402dac1f1806b3ebc39d",
            "33a956fad69a4f26a1e26773a29f541e",
            "9be5bc2a7a82462facc0575b120f77eb",
            "5676de9f2156424d8ca25f401cbe5410",
            "0966ab8563944f3e942e727be1a39bb4",
            "3be0b4fc111b4e4680ed2258c5394d2c",
            "0b42686e1ab7438a844ac2df37718a39",
            "6b5fcf400b2c444f8aac9a1b5acff8ed",
            "230b0577fccd499cb63c09d56c3308bc",
            "e001ce6890c04dc7aa22088fbc2a1fb0",
            "57526687cb1f434ea4757a0f8db734e1",
            "fc2a7cc76bde48a68e567475b48a71ab",
            "7d534351f1ed45c588a57499c0f9b9a7",
            "643a2c6c7151496fa3f04a42b83811a9",
            "a867a72d4ae04e6bb12fb23e378d7a9f",
            "b97129bb397e423292b67277039c0bac",
            "29b16939a0834d89b8a3a67d7cebfe23",
            "4b5ff6120c674f6c9090953a528dbb5e",
            "27e269ad370b488babde724601015726",
            "2b08dba783e14622911f27a41a93e366",
            "488a98546f7b4bc89d922e5b03b798a3"
          ]
        },
        "id": "prLnzYjPLFAp",
        "outputId": "501f0c69-407b-48ac-cd84-c23b6f1c8d54"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dcde952ff3a94cf78e9c9375b6338647",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/735 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57526687cb1f434ea4757a0f8db734e1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/82 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "<ipython-input-20-b666e5426984>:182: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='780' max='11025' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  780/11025 55:40 < 12:13:13, 0.23 it/s, Epoch 1.06/15]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.379900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.949100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.026500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.451100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.110200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.993700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.213700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.624200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.187800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.892500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.301200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.811600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.099800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.090800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.981500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.230100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.239900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.393000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.692700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.955900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.876200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.549800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.496600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.245700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.662900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.360500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.037300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.244500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.367400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.132000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>1.004100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.175900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.576600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.763900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.311400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.805200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>1.913800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.593500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.129600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.027600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.868400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>1.074400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.987500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>1.174300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.745300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>1.205800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>1.695700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.785700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.959700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.125600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>1.266100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.733400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>1.074200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.605200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.913000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.950200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>1.121600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>1.166700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>1.191800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.879100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>1.276300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.804600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.488200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.705400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.913700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.826900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>1.077200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>1.073100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.899000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.895200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>1.517700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.705000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>1.086000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>1.172700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.861900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.725600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.875800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>1.159400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>1.469800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.999700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>0.698000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>1.338800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>1.114500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>1.265200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.967300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>1.582300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.698100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>1.264600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>1.075900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.116300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>1.143800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.695800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.763600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>1.188800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>1.086700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.961800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>1.169800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.936100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>1.237200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.020800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>1.104100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>1.110800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>0.864100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>0.709800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>1.238700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>1.085400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>0.817400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>0.977500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>1.149700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.774800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>1.205400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>1.020900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>1.414100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>0.958900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.879900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>0.848800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>0.946300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>0.621600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>0.983900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.892400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>1.033500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>0.743700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>0.795100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>0.725900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>1.265400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>0.766700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>1.467100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>1.407800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>1.543300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.525300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>1.237600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>1.163700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>0.921600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>1.377400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>0.677000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>1.339700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>0.833100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>1.099300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>1.297400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.403800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>0.780600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>1.445300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143</td>\n",
              "      <td>0.716000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>1.012400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>1.464200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>0.759700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>0.453400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>0.940400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149</td>\n",
              "      <td>1.083000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.577400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151</td>\n",
              "      <td>0.781300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>0.452300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153</td>\n",
              "      <td>1.058100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>0.723300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>1.325400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>0.816900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157</td>\n",
              "      <td>1.287200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158</td>\n",
              "      <td>1.026700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159</td>\n",
              "      <td>0.840000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.911400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161</td>\n",
              "      <td>0.530900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162</td>\n",
              "      <td>1.577200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>163</td>\n",
              "      <td>1.200200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164</td>\n",
              "      <td>1.050300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>0.778900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166</td>\n",
              "      <td>1.752600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>167</td>\n",
              "      <td>0.832000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>0.657400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>169</td>\n",
              "      <td>0.696700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.920800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>1.132000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172</td>\n",
              "      <td>0.696500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>173</td>\n",
              "      <td>1.082100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>174</td>\n",
              "      <td>0.953500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.786300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>176</td>\n",
              "      <td>1.209400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>177</td>\n",
              "      <td>0.584700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>178</td>\n",
              "      <td>0.787600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>179</td>\n",
              "      <td>0.842900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.833300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>181</td>\n",
              "      <td>1.723800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>182</td>\n",
              "      <td>0.746800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>183</td>\n",
              "      <td>0.717500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>184</td>\n",
              "      <td>1.286600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>1.460100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>186</td>\n",
              "      <td>0.762000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>187</td>\n",
              "      <td>0.650300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>188</td>\n",
              "      <td>1.005200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>189</td>\n",
              "      <td>1.213500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>1.338300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>191</td>\n",
              "      <td>0.831100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>192</td>\n",
              "      <td>0.641500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>193</td>\n",
              "      <td>1.303900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>194</td>\n",
              "      <td>1.111200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>0.781800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>0.807400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>197</td>\n",
              "      <td>0.813600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>198</td>\n",
              "      <td>0.943500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>199</td>\n",
              "      <td>0.871500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.687400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>201</td>\n",
              "      <td>1.619300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>202</td>\n",
              "      <td>1.038800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>203</td>\n",
              "      <td>1.041700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>204</td>\n",
              "      <td>0.832700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>205</td>\n",
              "      <td>0.920700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>206</td>\n",
              "      <td>1.414400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>207</td>\n",
              "      <td>1.042900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>208</td>\n",
              "      <td>0.578700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>209</td>\n",
              "      <td>0.835700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.835200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>211</td>\n",
              "      <td>1.219900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>212</td>\n",
              "      <td>0.865400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>213</td>\n",
              "      <td>1.246200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>214</td>\n",
              "      <td>0.995400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>215</td>\n",
              "      <td>1.576700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>216</td>\n",
              "      <td>1.181800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>217</td>\n",
              "      <td>0.655500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>218</td>\n",
              "      <td>0.929000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>219</td>\n",
              "      <td>0.794400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>1.190300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>221</td>\n",
              "      <td>0.846700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>222</td>\n",
              "      <td>1.024300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>223</td>\n",
              "      <td>1.005300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>224</td>\n",
              "      <td>0.700300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>1.044200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>226</td>\n",
              "      <td>1.127700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>227</td>\n",
              "      <td>1.546800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>228</td>\n",
              "      <td>0.523700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>229</td>\n",
              "      <td>0.730500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>1.127500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>231</td>\n",
              "      <td>1.102900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>232</td>\n",
              "      <td>1.112500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>233</td>\n",
              "      <td>1.078800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>234</td>\n",
              "      <td>0.971400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>235</td>\n",
              "      <td>1.443000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>236</td>\n",
              "      <td>0.980700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>237</td>\n",
              "      <td>0.948400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>238</td>\n",
              "      <td>0.832600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>239</td>\n",
              "      <td>0.870900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>1.044100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>241</td>\n",
              "      <td>1.438500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>242</td>\n",
              "      <td>0.820900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>243</td>\n",
              "      <td>1.401600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>244</td>\n",
              "      <td>0.840700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>245</td>\n",
              "      <td>0.900900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>246</td>\n",
              "      <td>0.988700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>247</td>\n",
              "      <td>0.834500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>248</td>\n",
              "      <td>1.465000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>249</td>\n",
              "      <td>0.724000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.780100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>251</td>\n",
              "      <td>1.218500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>252</td>\n",
              "      <td>1.386300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>253</td>\n",
              "      <td>1.773100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>254</td>\n",
              "      <td>0.939300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>255</td>\n",
              "      <td>1.472700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>256</td>\n",
              "      <td>0.864700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>257</td>\n",
              "      <td>0.940400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>258</td>\n",
              "      <td>1.518400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>259</td>\n",
              "      <td>0.850800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.948600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>261</td>\n",
              "      <td>1.149900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>262</td>\n",
              "      <td>0.886300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>263</td>\n",
              "      <td>0.679400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>264</td>\n",
              "      <td>0.908900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>265</td>\n",
              "      <td>0.931100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>266</td>\n",
              "      <td>0.745700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>267</td>\n",
              "      <td>1.215700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>268</td>\n",
              "      <td>0.859700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>269</td>\n",
              "      <td>1.034400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>1.398800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>271</td>\n",
              "      <td>0.860800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>272</td>\n",
              "      <td>0.690800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>273</td>\n",
              "      <td>1.324400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>274</td>\n",
              "      <td>0.742100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>0.874100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>276</td>\n",
              "      <td>0.929200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>277</td>\n",
              "      <td>0.887100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>278</td>\n",
              "      <td>0.647400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>279</td>\n",
              "      <td>1.375100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>1.280300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>281</td>\n",
              "      <td>0.986400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>282</td>\n",
              "      <td>1.356000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>283</td>\n",
              "      <td>0.865800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>284</td>\n",
              "      <td>1.115100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>285</td>\n",
              "      <td>0.782100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>286</td>\n",
              "      <td>1.129200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>287</td>\n",
              "      <td>1.452900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>288</td>\n",
              "      <td>0.887500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>289</td>\n",
              "      <td>1.024000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.849700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>291</td>\n",
              "      <td>0.853800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>292</td>\n",
              "      <td>0.999900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>293</td>\n",
              "      <td>0.632000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>294</td>\n",
              "      <td>0.730800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>295</td>\n",
              "      <td>1.131900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>296</td>\n",
              "      <td>0.953900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>297</td>\n",
              "      <td>0.777400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>298</td>\n",
              "      <td>0.602600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>299</td>\n",
              "      <td>0.778600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>301</td>\n",
              "      <td>0.533300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>302</td>\n",
              "      <td>0.803500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>303</td>\n",
              "      <td>0.702800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>304</td>\n",
              "      <td>1.296700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>305</td>\n",
              "      <td>1.171000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>306</td>\n",
              "      <td>1.287900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>307</td>\n",
              "      <td>0.998300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>308</td>\n",
              "      <td>0.566300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>309</td>\n",
              "      <td>1.107100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.779200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>311</td>\n",
              "      <td>0.887500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>312</td>\n",
              "      <td>1.287900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>313</td>\n",
              "      <td>1.151900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>314</td>\n",
              "      <td>1.195800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>315</td>\n",
              "      <td>0.745500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>316</td>\n",
              "      <td>0.691900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>317</td>\n",
              "      <td>1.004200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>318</td>\n",
              "      <td>1.332300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>319</td>\n",
              "      <td>0.866700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.583300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>321</td>\n",
              "      <td>0.910400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>322</td>\n",
              "      <td>0.643200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>323</td>\n",
              "      <td>1.241500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>324</td>\n",
              "      <td>1.554400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>325</td>\n",
              "      <td>0.717500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>326</td>\n",
              "      <td>1.280400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>327</td>\n",
              "      <td>0.597800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>328</td>\n",
              "      <td>1.264700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>329</td>\n",
              "      <td>0.604400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.638000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>331</td>\n",
              "      <td>1.460400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>332</td>\n",
              "      <td>1.187100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>333</td>\n",
              "      <td>1.177300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>334</td>\n",
              "      <td>1.192000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>335</td>\n",
              "      <td>0.827800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>336</td>\n",
              "      <td>0.884500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>337</td>\n",
              "      <td>0.742300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>338</td>\n",
              "      <td>0.996300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>339</td>\n",
              "      <td>0.947300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.804400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>341</td>\n",
              "      <td>1.304600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>342</td>\n",
              "      <td>1.036400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>343</td>\n",
              "      <td>1.187200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>344</td>\n",
              "      <td>1.065900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>345</td>\n",
              "      <td>0.825600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>346</td>\n",
              "      <td>0.969700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>347</td>\n",
              "      <td>1.168400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>348</td>\n",
              "      <td>1.007300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>349</td>\n",
              "      <td>0.957300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.166400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>351</td>\n",
              "      <td>1.024300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>352</td>\n",
              "      <td>0.749400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>353</td>\n",
              "      <td>0.774900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>354</td>\n",
              "      <td>0.708400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>355</td>\n",
              "      <td>1.128400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>356</td>\n",
              "      <td>0.808400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>357</td>\n",
              "      <td>1.317700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>358</td>\n",
              "      <td>0.910100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>359</td>\n",
              "      <td>1.147900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>1.506500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>361</td>\n",
              "      <td>0.621900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>362</td>\n",
              "      <td>0.871100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>363</td>\n",
              "      <td>0.818500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>364</td>\n",
              "      <td>0.888100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>365</td>\n",
              "      <td>1.312700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>366</td>\n",
              "      <td>0.815700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>367</td>\n",
              "      <td>1.226200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>368</td>\n",
              "      <td>0.790300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>369</td>\n",
              "      <td>1.295200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.995500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>371</td>\n",
              "      <td>1.199100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>372</td>\n",
              "      <td>1.003300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>373</td>\n",
              "      <td>0.711700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>374</td>\n",
              "      <td>1.075300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>375</td>\n",
              "      <td>0.758900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>376</td>\n",
              "      <td>0.592300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>377</td>\n",
              "      <td>1.300500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>378</td>\n",
              "      <td>0.548100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>379</td>\n",
              "      <td>1.629500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>1.323900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>381</td>\n",
              "      <td>0.980300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>382</td>\n",
              "      <td>0.979500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>383</td>\n",
              "      <td>0.953700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>384</td>\n",
              "      <td>0.955000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>385</td>\n",
              "      <td>0.829200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>386</td>\n",
              "      <td>1.202800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>387</td>\n",
              "      <td>1.189600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>388</td>\n",
              "      <td>0.687800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>389</td>\n",
              "      <td>1.498800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.976000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>391</td>\n",
              "      <td>1.208500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>392</td>\n",
              "      <td>0.867300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>393</td>\n",
              "      <td>0.917300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>394</td>\n",
              "      <td>0.985700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>395</td>\n",
              "      <td>1.075800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>396</td>\n",
              "      <td>0.891600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>397</td>\n",
              "      <td>0.787800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>398</td>\n",
              "      <td>1.478600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>399</td>\n",
              "      <td>0.792300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.693500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>401</td>\n",
              "      <td>0.958300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>402</td>\n",
              "      <td>0.867800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>403</td>\n",
              "      <td>1.296900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>404</td>\n",
              "      <td>1.007700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>405</td>\n",
              "      <td>1.069100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>406</td>\n",
              "      <td>0.749600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>407</td>\n",
              "      <td>0.737100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>408</td>\n",
              "      <td>1.197000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>409</td>\n",
              "      <td>0.918100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.898000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>411</td>\n",
              "      <td>0.684700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>412</td>\n",
              "      <td>0.877900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>413</td>\n",
              "      <td>1.023200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>414</td>\n",
              "      <td>0.875100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>415</td>\n",
              "      <td>0.749400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>416</td>\n",
              "      <td>1.411200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>417</td>\n",
              "      <td>1.122300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>418</td>\n",
              "      <td>1.815400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>419</td>\n",
              "      <td>1.167100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.787200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>421</td>\n",
              "      <td>1.055700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>422</td>\n",
              "      <td>1.391300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>423</td>\n",
              "      <td>1.434200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>424</td>\n",
              "      <td>0.980500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>425</td>\n",
              "      <td>0.810400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>426</td>\n",
              "      <td>1.117500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>427</td>\n",
              "      <td>1.118200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>428</td>\n",
              "      <td>1.432700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>429</td>\n",
              "      <td>0.578500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.553300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>431</td>\n",
              "      <td>0.817700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>432</td>\n",
              "      <td>0.836000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>433</td>\n",
              "      <td>0.764500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>434</td>\n",
              "      <td>1.106400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>435</td>\n",
              "      <td>0.892300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>436</td>\n",
              "      <td>0.974000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>437</td>\n",
              "      <td>1.080700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>438</td>\n",
              "      <td>1.259600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>439</td>\n",
              "      <td>0.990900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.695900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>441</td>\n",
              "      <td>0.780500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>442</td>\n",
              "      <td>0.955800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>443</td>\n",
              "      <td>0.907800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>444</td>\n",
              "      <td>1.263500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>445</td>\n",
              "      <td>0.746000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>446</td>\n",
              "      <td>0.530500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>447</td>\n",
              "      <td>0.591200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>448</td>\n",
              "      <td>0.896600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>449</td>\n",
              "      <td>1.170000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.830300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>451</td>\n",
              "      <td>0.641700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>452</td>\n",
              "      <td>0.887500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>453</td>\n",
              "      <td>1.035700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>454</td>\n",
              "      <td>1.591300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>455</td>\n",
              "      <td>1.451800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>456</td>\n",
              "      <td>1.157900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>457</td>\n",
              "      <td>1.293400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>458</td>\n",
              "      <td>1.069900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>459</td>\n",
              "      <td>0.630000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>1.234400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>461</td>\n",
              "      <td>0.998800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>462</td>\n",
              "      <td>0.765900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>463</td>\n",
              "      <td>1.105800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>464</td>\n",
              "      <td>1.307700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>465</td>\n",
              "      <td>0.887700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>466</td>\n",
              "      <td>0.876700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>467</td>\n",
              "      <td>1.323500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>468</td>\n",
              "      <td>0.486500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>469</td>\n",
              "      <td>0.622700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>0.958000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>471</td>\n",
              "      <td>0.870400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>472</td>\n",
              "      <td>0.960500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>473</td>\n",
              "      <td>0.756300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>474</td>\n",
              "      <td>0.763900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>475</td>\n",
              "      <td>0.837900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>476</td>\n",
              "      <td>1.301900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>477</td>\n",
              "      <td>0.826100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>478</td>\n",
              "      <td>1.012300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>479</td>\n",
              "      <td>0.399100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.716700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>481</td>\n",
              "      <td>0.639200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>482</td>\n",
              "      <td>1.723000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>483</td>\n",
              "      <td>1.244100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>484</td>\n",
              "      <td>0.839700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>485</td>\n",
              "      <td>0.611200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>486</td>\n",
              "      <td>0.421900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>487</td>\n",
              "      <td>1.201400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>488</td>\n",
              "      <td>0.749300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>489</td>\n",
              "      <td>0.656000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>1.054300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>491</td>\n",
              "      <td>0.690400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>492</td>\n",
              "      <td>0.797200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>493</td>\n",
              "      <td>1.175800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>494</td>\n",
              "      <td>0.881500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>495</td>\n",
              "      <td>0.559000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>496</td>\n",
              "      <td>1.010400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>497</td>\n",
              "      <td>0.804900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>498</td>\n",
              "      <td>0.673400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>499</td>\n",
              "      <td>0.900700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.803100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>501</td>\n",
              "      <td>0.928800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>502</td>\n",
              "      <td>1.116500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>503</td>\n",
              "      <td>0.602500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>504</td>\n",
              "      <td>1.289300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>505</td>\n",
              "      <td>1.299300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>506</td>\n",
              "      <td>1.026100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>507</td>\n",
              "      <td>0.583600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>508</td>\n",
              "      <td>0.744300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>509</td>\n",
              "      <td>1.019800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>1.406500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>511</td>\n",
              "      <td>0.689600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>512</td>\n",
              "      <td>0.983200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>513</td>\n",
              "      <td>0.911600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>514</td>\n",
              "      <td>1.130700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>515</td>\n",
              "      <td>1.007000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>516</td>\n",
              "      <td>1.032500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>517</td>\n",
              "      <td>1.406900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>518</td>\n",
              "      <td>1.004500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>519</td>\n",
              "      <td>1.325000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>1.081800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>521</td>\n",
              "      <td>0.780800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>522</td>\n",
              "      <td>1.282600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>523</td>\n",
              "      <td>0.775900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>524</td>\n",
              "      <td>1.058200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>525</td>\n",
              "      <td>0.739900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>526</td>\n",
              "      <td>1.318000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>527</td>\n",
              "      <td>0.713800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>528</td>\n",
              "      <td>0.606800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>529</td>\n",
              "      <td>0.952700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>0.700600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>531</td>\n",
              "      <td>0.842300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>532</td>\n",
              "      <td>0.895800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>533</td>\n",
              "      <td>1.356900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>534</td>\n",
              "      <td>0.970200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>535</td>\n",
              "      <td>0.835300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>536</td>\n",
              "      <td>0.592900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>537</td>\n",
              "      <td>1.175200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>538</td>\n",
              "      <td>1.067300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>539</td>\n",
              "      <td>0.844200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>1.282300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>541</td>\n",
              "      <td>0.916400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>542</td>\n",
              "      <td>0.968900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>543</td>\n",
              "      <td>1.236500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>544</td>\n",
              "      <td>0.945400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>545</td>\n",
              "      <td>0.845100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>546</td>\n",
              "      <td>0.883400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>547</td>\n",
              "      <td>1.073700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>548</td>\n",
              "      <td>0.834000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>549</td>\n",
              "      <td>0.852000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.752000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>551</td>\n",
              "      <td>0.835300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>552</td>\n",
              "      <td>1.002300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>553</td>\n",
              "      <td>0.992500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>554</td>\n",
              "      <td>1.023600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>555</td>\n",
              "      <td>1.000800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>556</td>\n",
              "      <td>1.513800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>557</td>\n",
              "      <td>0.768600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>558</td>\n",
              "      <td>1.045900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>559</td>\n",
              "      <td>1.374300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.851200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>561</td>\n",
              "      <td>0.891400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>562</td>\n",
              "      <td>0.922200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>563</td>\n",
              "      <td>1.136500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>564</td>\n",
              "      <td>1.012200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>565</td>\n",
              "      <td>0.996200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>566</td>\n",
              "      <td>1.381400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>567</td>\n",
              "      <td>1.302100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>568</td>\n",
              "      <td>1.598200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>569</td>\n",
              "      <td>0.969600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.652100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>571</td>\n",
              "      <td>1.486000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>572</td>\n",
              "      <td>0.988800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>573</td>\n",
              "      <td>1.043100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>574</td>\n",
              "      <td>1.143900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>575</td>\n",
              "      <td>0.978700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>576</td>\n",
              "      <td>0.731500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>577</td>\n",
              "      <td>0.884500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>578</td>\n",
              "      <td>1.050800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>579</td>\n",
              "      <td>1.441400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>0.856500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>581</td>\n",
              "      <td>1.369200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>582</td>\n",
              "      <td>1.017700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>583</td>\n",
              "      <td>0.962500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>584</td>\n",
              "      <td>1.211000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>585</td>\n",
              "      <td>1.325200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>586</td>\n",
              "      <td>1.054100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>587</td>\n",
              "      <td>1.429700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>588</td>\n",
              "      <td>1.103200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>589</td>\n",
              "      <td>1.431800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>1.225300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>591</td>\n",
              "      <td>0.823100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>592</td>\n",
              "      <td>0.882200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>593</td>\n",
              "      <td>1.093900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>594</td>\n",
              "      <td>1.137600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>595</td>\n",
              "      <td>1.375500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>596</td>\n",
              "      <td>0.690000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>597</td>\n",
              "      <td>0.715400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>598</td>\n",
              "      <td>1.401800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>599</td>\n",
              "      <td>1.034100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.894500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>601</td>\n",
              "      <td>1.094100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>602</td>\n",
              "      <td>1.565700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>603</td>\n",
              "      <td>0.717000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>604</td>\n",
              "      <td>0.958800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>605</td>\n",
              "      <td>0.712700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>606</td>\n",
              "      <td>0.793100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>607</td>\n",
              "      <td>1.096500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>608</td>\n",
              "      <td>1.103400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>609</td>\n",
              "      <td>1.383000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>610</td>\n",
              "      <td>0.530900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>611</td>\n",
              "      <td>0.708000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>612</td>\n",
              "      <td>1.426300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>613</td>\n",
              "      <td>0.585100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>614</td>\n",
              "      <td>1.226100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>615</td>\n",
              "      <td>0.832800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>616</td>\n",
              "      <td>0.959200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>617</td>\n",
              "      <td>0.693400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>618</td>\n",
              "      <td>0.836500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>619</td>\n",
              "      <td>0.591600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>1.114400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>621</td>\n",
              "      <td>0.939000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>622</td>\n",
              "      <td>0.901100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>623</td>\n",
              "      <td>1.682700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>624</td>\n",
              "      <td>1.340900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>625</td>\n",
              "      <td>1.027000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>626</td>\n",
              "      <td>0.702400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>627</td>\n",
              "      <td>1.174400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>628</td>\n",
              "      <td>0.741300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>629</td>\n",
              "      <td>0.917100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>630</td>\n",
              "      <td>0.621000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>631</td>\n",
              "      <td>0.783700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>632</td>\n",
              "      <td>1.161600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>633</td>\n",
              "      <td>0.890900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>634</td>\n",
              "      <td>0.891300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>635</td>\n",
              "      <td>0.788000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>636</td>\n",
              "      <td>0.947800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>637</td>\n",
              "      <td>1.359200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>638</td>\n",
              "      <td>1.115500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>639</td>\n",
              "      <td>0.922500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>0.882400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>641</td>\n",
              "      <td>1.111300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>642</td>\n",
              "      <td>0.890900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>643</td>\n",
              "      <td>0.722900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>644</td>\n",
              "      <td>0.880700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>645</td>\n",
              "      <td>0.792400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>646</td>\n",
              "      <td>1.364900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>647</td>\n",
              "      <td>0.931800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>648</td>\n",
              "      <td>0.722200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>649</td>\n",
              "      <td>0.668300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.944200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>651</td>\n",
              "      <td>0.813800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>652</td>\n",
              "      <td>1.188800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>653</td>\n",
              "      <td>1.097300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>654</td>\n",
              "      <td>1.250700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>655</td>\n",
              "      <td>1.002500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>656</td>\n",
              "      <td>1.062900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>657</td>\n",
              "      <td>0.825300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>658</td>\n",
              "      <td>1.468900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>659</td>\n",
              "      <td>0.749300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>1.059500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>661</td>\n",
              "      <td>0.876400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>662</td>\n",
              "      <td>1.032700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>663</td>\n",
              "      <td>1.275100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>664</td>\n",
              "      <td>1.004600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>665</td>\n",
              "      <td>1.029400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>666</td>\n",
              "      <td>1.226200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>667</td>\n",
              "      <td>1.198600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>668</td>\n",
              "      <td>1.273700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>669</td>\n",
              "      <td>0.808700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>670</td>\n",
              "      <td>0.879700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>671</td>\n",
              "      <td>0.826300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>672</td>\n",
              "      <td>0.663700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>673</td>\n",
              "      <td>0.841600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>674</td>\n",
              "      <td>0.607300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>675</td>\n",
              "      <td>0.922000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>676</td>\n",
              "      <td>1.017800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>677</td>\n",
              "      <td>0.822800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>678</td>\n",
              "      <td>0.599300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>679</td>\n",
              "      <td>0.631100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>0.728400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>681</td>\n",
              "      <td>0.641900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>682</td>\n",
              "      <td>0.671800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>683</td>\n",
              "      <td>0.832100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>684</td>\n",
              "      <td>0.960000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>685</td>\n",
              "      <td>0.621900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>686</td>\n",
              "      <td>1.059800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>687</td>\n",
              "      <td>1.388800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>688</td>\n",
              "      <td>1.570200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>689</td>\n",
              "      <td>1.032700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>0.949100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>691</td>\n",
              "      <td>1.175300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>692</td>\n",
              "      <td>1.464900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>693</td>\n",
              "      <td>1.128500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>694</td>\n",
              "      <td>0.946100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>695</td>\n",
              "      <td>0.957800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>696</td>\n",
              "      <td>1.390700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>697</td>\n",
              "      <td>1.103700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>698</td>\n",
              "      <td>1.236300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>699</td>\n",
              "      <td>1.288000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.901300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>701</td>\n",
              "      <td>0.883300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>702</td>\n",
              "      <td>1.115000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>703</td>\n",
              "      <td>0.831800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>704</td>\n",
              "      <td>0.719500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>705</td>\n",
              "      <td>0.837400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>706</td>\n",
              "      <td>0.857500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>707</td>\n",
              "      <td>1.199600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>708</td>\n",
              "      <td>1.174300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>709</td>\n",
              "      <td>1.789300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>710</td>\n",
              "      <td>1.082600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>711</td>\n",
              "      <td>0.949800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>712</td>\n",
              "      <td>1.278000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>713</td>\n",
              "      <td>1.188800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>714</td>\n",
              "      <td>1.157300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>715</td>\n",
              "      <td>0.845500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>716</td>\n",
              "      <td>0.857100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>717</td>\n",
              "      <td>1.097300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>718</td>\n",
              "      <td>0.826500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>719</td>\n",
              "      <td>0.924700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>1.706200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>721</td>\n",
              "      <td>0.499500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>722</td>\n",
              "      <td>1.110100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>723</td>\n",
              "      <td>1.210400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>724</td>\n",
              "      <td>0.753000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>725</td>\n",
              "      <td>1.137600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>726</td>\n",
              "      <td>0.661300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>727</td>\n",
              "      <td>1.212800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>728</td>\n",
              "      <td>0.673200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>729</td>\n",
              "      <td>0.806200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>730</td>\n",
              "      <td>0.965200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>731</td>\n",
              "      <td>0.800500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>732</td>\n",
              "      <td>1.052500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>733</td>\n",
              "      <td>0.939800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>734</td>\n",
              "      <td>0.879100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>735</td>\n",
              "      <td>0.948200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>736</td>\n",
              "      <td>1.102200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>737</td>\n",
              "      <td>0.765100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>738</td>\n",
              "      <td>0.916600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>739</td>\n",
              "      <td>0.579900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>740</td>\n",
              "      <td>0.623500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>741</td>\n",
              "      <td>0.343200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>742</td>\n",
              "      <td>1.122900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>743</td>\n",
              "      <td>0.418500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>744</td>\n",
              "      <td>0.388500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>745</td>\n",
              "      <td>0.604600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>746</td>\n",
              "      <td>0.598000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>747</td>\n",
              "      <td>0.568500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>748</td>\n",
              "      <td>0.362100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>749</td>\n",
              "      <td>0.465400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>1.043200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>751</td>\n",
              "      <td>0.562100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>752</td>\n",
              "      <td>0.647900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>753</td>\n",
              "      <td>0.805300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>754</td>\n",
              "      <td>0.659200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>755</td>\n",
              "      <td>0.684600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>756</td>\n",
              "      <td>0.903900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>757</td>\n",
              "      <td>0.567500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>758</td>\n",
              "      <td>0.700100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>759</td>\n",
              "      <td>0.430500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>760</td>\n",
              "      <td>0.318900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>761</td>\n",
              "      <td>0.371800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>762</td>\n",
              "      <td>0.578700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>763</td>\n",
              "      <td>0.899200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>764</td>\n",
              "      <td>0.636000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>765</td>\n",
              "      <td>0.561400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>766</td>\n",
              "      <td>0.430100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>767</td>\n",
              "      <td>0.973100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>768</td>\n",
              "      <td>0.391800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>769</td>\n",
              "      <td>0.698600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>770</td>\n",
              "      <td>0.731100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>771</td>\n",
              "      <td>0.502900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>772</td>\n",
              "      <td>0.587100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>773</td>\n",
              "      <td>0.913200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>774</td>\n",
              "      <td>1.017400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>775</td>\n",
              "      <td>0.710400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>776</td>\n",
              "      <td>0.657300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>777</td>\n",
              "      <td>0.573600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>778</td>\n",
              "      <td>0.544500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ">>> Step 1 metrics: {'loss': 1.3799, 'grad_norm': 10.375, 'learning_rate': 0.0, 'epoch': 0.0013605442176870747}\n",
            "\n",
            ">>> Step 2 metrics: {'loss': 0.9491, 'grad_norm': 9.8125, 'learning_rate': 5e-06, 'epoch': 0.0027210884353741495}\n",
            "\n",
            ">>> Step 3 metrics: {'loss': 1.0265, 'grad_norm': 10.6875, 'learning_rate': 1e-05, 'epoch': 0.004081632653061225}\n",
            "\n",
            ">>> Step 4 metrics: {'loss': 1.4511, 'grad_norm': 9.875, 'learning_rate': 1.5e-05, 'epoch': 0.005442176870748299}\n",
            "\n",
            ">>> Step 5 metrics: {'loss': 1.1102, 'grad_norm': 9.375, 'learning_rate': 2e-05, 'epoch': 0.006802721088435374}\n",
            "\n",
            ">>> Step 6 metrics: {'loss': 0.9937, 'grad_norm': 10.3125, 'learning_rate': 2.5e-05, 'epoch': 0.00816326530612245}\n",
            "\n",
            ">>> Step 7 metrics: {'loss': 1.2137, 'grad_norm': 9.75, 'learning_rate': 3e-05, 'epoch': 0.009523809523809525}\n",
            "\n",
            ">>> Step 8 metrics: {'loss': 1.6242, 'grad_norm': 10.375, 'learning_rate': 3.5e-05, 'epoch': 0.010884353741496598}\n",
            "\n",
            ">>> Step 9 metrics: {'loss': 1.1878, 'grad_norm': 10.3125, 'learning_rate': 4e-05, 'epoch': 0.012244897959183673}\n",
            "\n",
            ">>> Step 10 metrics: {'loss': 0.8925, 'grad_norm': 10.625, 'learning_rate': 4.5e-05, 'epoch': 0.013605442176870748}\n",
            "\n",
            ">>> Step 11 metrics: {'loss': 1.3012, 'grad_norm': 10.5, 'learning_rate': 5e-05, 'epoch': 0.014965986394557823}\n",
            "\n",
            ">>> Step 12 metrics: {'loss': 0.8116, 'grad_norm': 8.5, 'learning_rate': 4.999999898318779e-05, 'epoch': 0.0163265306122449}\n",
            "\n",
            ">>> Step 13 metrics: {'loss': 1.0998, 'grad_norm': 10.875, 'learning_rate': 4.999999593275125e-05, 'epoch': 0.017687074829931974}\n",
            "\n",
            ">>> Step 14 metrics: {'loss': 1.0908, 'grad_norm': 8.3125, 'learning_rate': 4.999999084869061e-05, 'epoch': 0.01904761904761905}\n",
            "\n",
            ">>> Step 15 metrics: {'loss': 0.9815, 'grad_norm': 8.8125, 'learning_rate': 4.999998373100631e-05, 'epoch': 0.02040816326530612}\n",
            "\n",
            ">>> Step 16 metrics: {'loss': 1.2301, 'grad_norm': 11.125, 'learning_rate': 4.999997457969889e-05, 'epoch': 0.021768707482993196}\n",
            "\n",
            ">>> Step 17 metrics: {'loss': 1.2399, 'grad_norm': 10.0, 'learning_rate': 4.999996339476914e-05, 'epoch': 0.02312925170068027}\n",
            "\n",
            ">>> Step 18 metrics: {'loss': 1.393, 'grad_norm': 10.625, 'learning_rate': 4.999995017621794e-05, 'epoch': 0.024489795918367346}\n",
            "\n",
            ">>> Step 19 metrics: {'loss': 0.6927, 'grad_norm': 7.875, 'learning_rate': 4.999993492404638e-05, 'epoch': 0.02585034013605442}\n",
            "\n",
            ">>> Step 20 metrics: {'loss': 0.9559, 'grad_norm': 9.375, 'learning_rate': 4.9999917638255686e-05, 'epoch': 0.027210884353741496}\n",
            "\n",
            ">>> Step 21 metrics: {'loss': 0.8762, 'grad_norm': 8.4375, 'learning_rate': 4.9999898318847274e-05, 'epoch': 0.02857142857142857}\n",
            "\n",
            ">>> Step 22 metrics: {'loss': 1.5498, 'grad_norm': 10.6875, 'learning_rate': 4.999987696582272e-05, 'epoch': 0.029931972789115645}\n",
            "\n",
            ">>> Step 23 metrics: {'loss': 0.4966, 'grad_norm': 6.78125, 'learning_rate': 4.9999853579183755e-05, 'epoch': 0.031292517006802724}\n",
            "\n",
            ">>> Step 24 metrics: {'loss': 1.2457, 'grad_norm': 11.375, 'learning_rate': 4.999982815893227e-05, 'epoch': 0.0326530612244898}\n",
            "\n",
            ">>> Step 25 metrics: {'loss': 0.6629, 'grad_norm': 6.6875, 'learning_rate': 4.9999800705070356e-05, 'epoch': 0.034013605442176874}\n",
            "\n",
            ">>> Step 26 metrics: {'loss': 1.3605, 'grad_norm': 10.75, 'learning_rate': 4.9999771217600226e-05, 'epoch': 0.03537414965986395}\n",
            "\n",
            ">>> Step 27 metrics: {'loss': 1.0373, 'grad_norm': 8.5, 'learning_rate': 4.9999739696524295e-05, 'epoch': 0.036734693877551024}\n",
            "\n",
            ">>> Step 28 metrics: {'loss': 1.2445, 'grad_norm': 10.125, 'learning_rate': 4.999970614184512e-05, 'epoch': 0.0380952380952381}\n",
            "\n",
            ">>> Step 29 metrics: {'loss': 1.3674, 'grad_norm': 10.625, 'learning_rate': 4.999967055356541e-05, 'epoch': 0.03945578231292517}\n",
            "\n",
            ">>> Step 30 metrics: {'loss': 1.132, 'grad_norm': 11.0, 'learning_rate': 4.99996329316881e-05, 'epoch': 0.04081632653061224}\n",
            "\n",
            ">>> Step 31 metrics: {'loss': 1.0041, 'grad_norm': 8.625, 'learning_rate': 4.999959327621623e-05, 'epoch': 0.04217687074829932}\n",
            "\n",
            ">>> Step 32 metrics: {'loss': 1.1759, 'grad_norm': 10.3125, 'learning_rate': 4.9999551587153014e-05, 'epoch': 0.04353741496598639}\n",
            "\n",
            ">>> Step 33 metrics: {'loss': 0.5766, 'grad_norm': 6.3125, 'learning_rate': 4.999950786450186e-05, 'epoch': 0.044897959183673466}\n",
            "\n",
            ">>> Step 34 metrics: {'loss': 0.7639, 'grad_norm': 8.75, 'learning_rate': 4.9999462108266326e-05, 'epoch': 0.04625850340136054}\n",
            "\n",
            ">>> Step 35 metrics: {'loss': 1.3114, 'grad_norm': 9.3125, 'learning_rate': 4.999941431845012e-05, 'epoch': 0.047619047619047616}\n",
            "\n",
            ">>> Step 36 metrics: {'loss': 0.8052, 'grad_norm': 9.3125, 'learning_rate': 4.999936449505713e-05, 'epoch': 0.04897959183673469}\n",
            "\n",
            ">>> Step 37 metrics: {'loss': 1.9138, 'grad_norm': 13.375, 'learning_rate': 4.9999312638091434e-05, 'epoch': 0.050340136054421766}\n",
            "\n",
            ">>> Step 38 metrics: {'loss': 0.5935, 'grad_norm': 7.375, 'learning_rate': 4.999925874755722e-05, 'epoch': 0.05170068027210884}\n",
            "\n",
            ">>> Step 39 metrics: {'loss': 1.1296, 'grad_norm': 8.4375, 'learning_rate': 4.999920282345889e-05, 'epoch': 0.053061224489795916}\n",
            "\n",
            ">>> Step 40 metrics: {'loss': 1.0276, 'grad_norm': 8.75, 'learning_rate': 4.999914486580098e-05, 'epoch': 0.05442176870748299}\n",
            "\n",
            ">>> Step 41 metrics: {'loss': 0.8684, 'grad_norm': 7.8125, 'learning_rate': 4.999908487458821e-05, 'epoch': 0.055782312925170066}\n",
            "\n",
            ">>> Step 42 metrics: {'loss': 1.0744, 'grad_norm': 7.71875, 'learning_rate': 4.999902284982547e-05, 'epoch': 0.05714285714285714}\n",
            "\n",
            ">>> Step 43 metrics: {'loss': 0.9875, 'grad_norm': 8.5625, 'learning_rate': 4.99989587915178e-05, 'epoch': 0.058503401360544216}\n",
            "\n",
            ">>> Step 44 metrics: {'loss': 1.1743, 'grad_norm': 9.5625, 'learning_rate': 4.9998892699670395e-05, 'epoch': 0.05986394557823129}\n",
            "\n",
            ">>> Step 45 metrics: {'loss': 0.7453, 'grad_norm': 6.6875, 'learning_rate': 4.999882457428865e-05, 'epoch': 0.061224489795918366}\n",
            "\n",
            ">>> Step 46 metrics: {'loss': 1.2058, 'grad_norm': 8.6875, 'learning_rate': 4.99987544153781e-05, 'epoch': 0.06258503401360545}\n",
            "\n",
            ">>> Step 47 metrics: {'loss': 1.6957, 'grad_norm': 12.5625, 'learning_rate': 4.999868222294446e-05, 'epoch': 0.06394557823129252}\n",
            "\n",
            ">>> Step 48 metrics: {'loss': 0.7857, 'grad_norm': 7.21875, 'learning_rate': 4.999860799699358e-05, 'epoch': 0.0653061224489796}\n",
            "\n",
            ">>> Step 49 metrics: {'loss': 0.9597, 'grad_norm': 7.96875, 'learning_rate': 4.999853173753153e-05, 'epoch': 0.06666666666666667}\n",
            "\n",
            ">>> Step 50 metrics: {'loss': 1.1256, 'grad_norm': 8.625, 'learning_rate': 4.9998453444564494e-05, 'epoch': 0.06802721088435375}\n",
            "\n",
            ">>> Step 51 metrics: {'loss': 1.2661, 'grad_norm': 11.5625, 'learning_rate': 4.9998373118098834e-05, 'epoch': 0.06938775510204082}\n",
            "\n",
            ">>> Step 52 metrics: {'loss': 0.7334, 'grad_norm': 6.96875, 'learning_rate': 4.99982907581411e-05, 'epoch': 0.0707482993197279}\n",
            "\n",
            ">>> Step 53 metrics: {'loss': 1.0742, 'grad_norm': 8.1875, 'learning_rate': 4.999820636469798e-05, 'epoch': 0.07210884353741497}\n",
            "\n",
            ">>> Step 54 metrics: {'loss': 0.6052, 'grad_norm': 6.96875, 'learning_rate': 4.999811993777634e-05, 'epoch': 0.07346938775510205}\n",
            "\n",
            ">>> Step 55 metrics: {'loss': 0.913, 'grad_norm': 7.78125, 'learning_rate': 4.9998031477383225e-05, 'epoch': 0.07482993197278912}\n",
            "\n",
            ">>> Step 56 metrics: {'loss': 0.9502, 'grad_norm': 8.6875, 'learning_rate': 4.999794098352582e-05, 'epoch': 0.0761904761904762}\n",
            "\n",
            ">>> Step 57 metrics: {'loss': 1.1216, 'grad_norm': 7.53125, 'learning_rate': 4.999784845621148e-05, 'epoch': 0.07755102040816327}\n",
            "\n",
            ">>> Step 58 metrics: {'loss': 1.1667, 'grad_norm': 10.3125, 'learning_rate': 4.999775389544774e-05, 'epoch': 0.07891156462585033}\n",
            "\n",
            ">>> Step 59 metrics: {'loss': 1.1918, 'grad_norm': 9.25, 'learning_rate': 4.9997657301242294e-05, 'epoch': 0.08027210884353742}\n",
            "\n",
            ">>> Step 60 metrics: {'loss': 0.8791, 'grad_norm': 6.75, 'learning_rate': 4.9997558673602994e-05, 'epoch': 0.08163265306122448}\n",
            "\n",
            ">>> Step 61 metrics: {'loss': 1.2763, 'grad_norm': 9.1875, 'learning_rate': 4.9997458012537865e-05, 'epoch': 0.08299319727891157}\n",
            "\n",
            ">>> Step 62 metrics: {'loss': 0.8046, 'grad_norm': 6.1875, 'learning_rate': 4.999735531805509e-05, 'epoch': 0.08435374149659863}\n",
            "\n",
            ">>> Step 63 metrics: {'loss': 0.4882, 'grad_norm': 6.03125, 'learning_rate': 4.999725059016302e-05, 'epoch': 0.08571428571428572}\n",
            "\n",
            ">>> Step 64 metrics: {'loss': 0.7054, 'grad_norm': 7.59375, 'learning_rate': 4.9997143828870196e-05, 'epoch': 0.08707482993197278}\n",
            "\n",
            ">>> Step 65 metrics: {'loss': 0.9137, 'grad_norm': 9.1875, 'learning_rate': 4.9997035034185286e-05, 'epoch': 0.08843537414965986}\n",
            "\n",
            ">>> Step 66 metrics: {'loss': 0.8269, 'grad_norm': 9.0, 'learning_rate': 4.999692420611714e-05, 'epoch': 0.08979591836734693}\n",
            "\n",
            ">>> Step 67 metrics: {'loss': 1.0772, 'grad_norm': 7.9375, 'learning_rate': 4.9996811344674775e-05, 'epoch': 0.09115646258503401}\n",
            "\n",
            ">>> Step 68 metrics: {'loss': 1.0731, 'grad_norm': 11.375, 'learning_rate': 4.999669644986737e-05, 'epoch': 0.09251700680272108}\n",
            "\n",
            ">>> Step 69 metrics: {'loss': 0.899, 'grad_norm': 8.6875, 'learning_rate': 4.999657952170427e-05, 'epoch': 0.09387755102040816}\n",
            "\n",
            ">>> Step 70 metrics: {'loss': 0.8952, 'grad_norm': 8.1875, 'learning_rate': 4.9996460560194996e-05, 'epoch': 0.09523809523809523}\n",
            "\n",
            ">>> Step 71 metrics: {'loss': 1.5177, 'grad_norm': 9.4375, 'learning_rate': 4.9996339565349214e-05, 'epoch': 0.09659863945578231}\n",
            "\n",
            ">>> Step 72 metrics: {'loss': 0.705, 'grad_norm': 8.5, 'learning_rate': 4.9996216537176773e-05, 'epoch': 0.09795918367346938}\n",
            "\n",
            ">>> Step 73 metrics: {'loss': 1.086, 'grad_norm': 9.0, 'learning_rate': 4.999609147568768e-05, 'epoch': 0.09931972789115646}\n",
            "\n",
            ">>> Step 74 metrics: {'loss': 1.1727, 'grad_norm': 10.8125, 'learning_rate': 4.9995964380892104e-05, 'epoch': 0.10068027210884353}\n",
            "\n",
            ">>> Step 75 metrics: {'loss': 0.8619, 'grad_norm': 7.5625, 'learning_rate': 4.999583525280038e-05, 'epoch': 0.10204081632653061}\n",
            "\n",
            ">>> Step 76 metrics: {'loss': 0.7256, 'grad_norm': 8.125, 'learning_rate': 4.999570409142303e-05, 'epoch': 0.10340136054421768}\n",
            "\n",
            ">>> Step 77 metrics: {'loss': 0.8758, 'grad_norm': 7.8125, 'learning_rate': 4.9995570896770706e-05, 'epoch': 0.10476190476190476}\n",
            "\n",
            ">>> Step 78 metrics: {'loss': 1.1594, 'grad_norm': 8.75, 'learning_rate': 4.999543566885425e-05, 'epoch': 0.10612244897959183}\n",
            "\n",
            ">>> Step 79 metrics: {'loss': 1.4698, 'grad_norm': 11.125, 'learning_rate': 4.9995298407684655e-05, 'epoch': 0.10748299319727891}\n",
            "\n",
            ">>> Step 80 metrics: {'loss': 0.9997, 'grad_norm': 8.125, 'learning_rate': 4.999515911327309e-05, 'epoch': 0.10884353741496598}\n",
            "\n",
            ">>> Step 81 metrics: {'loss': 0.698, 'grad_norm': 7.40625, 'learning_rate': 4.999501778563089e-05, 'epoch': 0.11020408163265306}\n",
            "\n",
            ">>> Step 82 metrics: {'loss': 1.3388, 'grad_norm': 9.0625, 'learning_rate': 4.9994874424769554e-05, 'epoch': 0.11156462585034013}\n",
            "\n",
            ">>> Step 83 metrics: {'loss': 1.1145, 'grad_norm': 7.625, 'learning_rate': 4.999472903070074e-05, 'epoch': 0.11292517006802721}\n",
            "\n",
            ">>> Step 84 metrics: {'loss': 1.2652, 'grad_norm': 7.625, 'learning_rate': 4.9994581603436265e-05, 'epoch': 0.11428571428571428}\n",
            "\n",
            ">>> Step 85 metrics: {'loss': 0.9673, 'grad_norm': 6.9375, 'learning_rate': 4.999443214298814e-05, 'epoch': 0.11564625850340136}\n",
            "\n",
            ">>> Step 86 metrics: {'loss': 1.5823, 'grad_norm': 9.5, 'learning_rate': 4.99942806493685e-05, 'epoch': 0.11700680272108843}\n",
            "\n",
            ">>> Step 87 metrics: {'loss': 0.6981, 'grad_norm': 6.90625, 'learning_rate': 4.999412712258969e-05, 'epoch': 0.11836734693877551}\n",
            "\n",
            ">>> Step 88 metrics: {'loss': 1.2646, 'grad_norm': 9.6875, 'learning_rate': 4.999397156266419e-05, 'epoch': 0.11972789115646258}\n",
            "\n",
            ">>> Step 89 metrics: {'loss': 1.0759, 'grad_norm': 7.90625, 'learning_rate': 4.999381396960465e-05, 'epoch': 0.12108843537414966}\n",
            "\n",
            ">>> Step 90 metrics: {'loss': 1.1163, 'grad_norm': 9.0, 'learning_rate': 4.9993654343423894e-05, 'epoch': 0.12244897959183673}\n",
            "\n",
            ">>> Step 91 metrics: {'loss': 1.1438, 'grad_norm': 11.125, 'learning_rate': 4.999349268413491e-05, 'epoch': 0.12380952380952381}\n",
            "\n",
            ">>> Step 92 metrics: {'loss': 0.6958, 'grad_norm': 8.5625, 'learning_rate': 4.999332899175084e-05, 'epoch': 0.1251700680272109}\n",
            "\n",
            ">>> Step 93 metrics: {'loss': 0.7636, 'grad_norm': 7.0625, 'learning_rate': 4.999316326628501e-05, 'epoch': 0.12653061224489795}\n",
            "\n",
            ">>> Step 94 metrics: {'loss': 1.1888, 'grad_norm': 7.6875, 'learning_rate': 4.9992995507750885e-05, 'epoch': 0.12789115646258503}\n",
            "\n",
            ">>> Step 95 metrics: {'loss': 1.0867, 'grad_norm': 7.96875, 'learning_rate': 4.999282571616213e-05, 'epoch': 0.1292517006802721}\n",
            "\n",
            ">>> Step 96 metrics: {'loss': 0.9618, 'grad_norm': 8.125, 'learning_rate': 4.999265389153254e-05, 'epoch': 0.1306122448979592}\n",
            "\n",
            ">>> Step 97 metrics: {'loss': 1.1698, 'grad_norm': 8.3125, 'learning_rate': 4.9992480033876105e-05, 'epoch': 0.13197278911564625}\n",
            "\n",
            ">>> Step 98 metrics: {'loss': 0.9361, 'grad_norm': 8.1875, 'learning_rate': 4.9992304143206956e-05, 'epoch': 0.13333333333333333}\n",
            "\n",
            ">>> Step 99 metrics: {'loss': 1.2372, 'grad_norm': 7.90625, 'learning_rate': 4.999212621953942e-05, 'epoch': 0.1346938775510204}\n",
            "\n",
            ">>> Step 100 metrics: {'loss': 1.0208, 'grad_norm': 9.0, 'learning_rate': 4.999194626288795e-05, 'epoch': 0.1360544217687075}\n",
            "\n",
            ">>> Step 101 metrics: {'loss': 1.1041, 'grad_norm': 12.8125, 'learning_rate': 4.999176427326718e-05, 'epoch': 0.13741496598639455}\n",
            "\n",
            ">>> Step 102 metrics: {'loss': 1.1108, 'grad_norm': 9.875, 'learning_rate': 4.999158025069194e-05, 'epoch': 0.13877551020408163}\n",
            "\n",
            ">>> Step 103 metrics: {'loss': 0.8641, 'grad_norm': 7.6875, 'learning_rate': 4.9991394195177175e-05, 'epoch': 0.1401360544217687}\n",
            "\n",
            ">>> Step 104 metrics: {'loss': 0.7098, 'grad_norm': 6.84375, 'learning_rate': 4.9991206106738043e-05, 'epoch': 0.1414965986394558}\n",
            "\n",
            ">>> Step 105 metrics: {'loss': 1.2387, 'grad_norm': 7.8125, 'learning_rate': 4.999101598538982e-05, 'epoch': 0.14285714285714285}\n",
            "\n",
            ">>> Step 106 metrics: {'loss': 1.0854, 'grad_norm': 7.5, 'learning_rate': 4.999082383114799e-05, 'epoch': 0.14421768707482993}\n",
            "\n",
            ">>> Step 107 metrics: {'loss': 0.8174, 'grad_norm': 8.0625, 'learning_rate': 4.999062964402817e-05, 'epoch': 0.145578231292517}\n",
            "\n",
            ">>> Step 108 metrics: {'loss': 0.9775, 'grad_norm': 9.25, 'learning_rate': 4.9990433424046166e-05, 'epoch': 0.1469387755102041}\n",
            "\n",
            ">>> Step 109 metrics: {'loss': 1.1497, 'grad_norm': 7.78125, 'learning_rate': 4.9990235171217936e-05, 'epoch': 0.14829931972789115}\n",
            "\n",
            ">>> Step 110 metrics: {'loss': 0.7748, 'grad_norm': 8.0, 'learning_rate': 4.99900348855596e-05, 'epoch': 0.14965986394557823}\n",
            "\n",
            ">>> Step 111 metrics: {'loss': 1.2054, 'grad_norm': 9.1875, 'learning_rate': 4.9989832567087455e-05, 'epoch': 0.1510204081632653}\n",
            "\n",
            ">>> Step 112 metrics: {'loss': 1.0209, 'grad_norm': 8.4375, 'learning_rate': 4.9989628215817974e-05, 'epoch': 0.1523809523809524}\n",
            "\n",
            ">>> Step 113 metrics: {'loss': 1.4141, 'grad_norm': 8.375, 'learning_rate': 4.998942183176776e-05, 'epoch': 0.15374149659863945}\n",
            "\n",
            ">>> Step 114 metrics: {'loss': 0.9589, 'grad_norm': 8.75, 'learning_rate': 4.998921341495361e-05, 'epoch': 0.15510204081632653}\n",
            "\n",
            ">>> Step 115 metrics: {'loss': 0.8799, 'grad_norm': 7.8125, 'learning_rate': 4.998900296539246e-05, 'epoch': 0.1564625850340136}\n",
            "\n",
            ">>> Step 116 metrics: {'loss': 0.8488, 'grad_norm': 7.75, 'learning_rate': 4.998879048310146e-05, 'epoch': 0.15782312925170067}\n",
            "\n",
            ">>> Step 117 metrics: {'loss': 0.9463, 'grad_norm': 8.4375, 'learning_rate': 4.998857596809787e-05, 'epoch': 0.15918367346938775}\n",
            "\n",
            ">>> Step 118 metrics: {'loss': 0.6216, 'grad_norm': 7.125, 'learning_rate': 4.9988359420399154e-05, 'epoch': 0.16054421768707483}\n",
            "\n",
            ">>> Step 119 metrics: {'loss': 0.9839, 'grad_norm': 8.875, 'learning_rate': 4.998814084002292e-05, 'epoch': 0.1619047619047619}\n",
            "\n",
            ">>> Step 120 metrics: {'loss': 0.8924, 'grad_norm': 8.0625, 'learning_rate': 4.998792022698695e-05, 'epoch': 0.16326530612244897}\n",
            "\n",
            ">>> Step 121 metrics: {'loss': 1.0335, 'grad_norm': 8.1875, 'learning_rate': 4.998769758130919e-05, 'epoch': 0.16462585034013605}\n",
            "\n",
            ">>> Step 122 metrics: {'loss': 0.7437, 'grad_norm': 6.625, 'learning_rate': 4.9987472903007756e-05, 'epoch': 0.16598639455782313}\n",
            "\n",
            ">>> Step 123 metrics: {'loss': 0.7951, 'grad_norm': 9.3125, 'learning_rate': 4.9987246192100913e-05, 'epoch': 0.1673469387755102}\n",
            "\n",
            ">>> Step 124 metrics: {'loss': 0.7259, 'grad_norm': 6.5625, 'learning_rate': 4.998701744860711e-05, 'epoch': 0.16870748299319727}\n",
            "\n",
            ">>> Step 125 metrics: {'loss': 1.2654, 'grad_norm': 9.75, 'learning_rate': 4.998678667254495e-05, 'epoch': 0.17006802721088435}\n",
            "\n",
            ">>> Step 126 metrics: {'loss': 0.7667, 'grad_norm': 7.96875, 'learning_rate': 4.9986553863933204e-05, 'epoch': 0.17142857142857143}\n",
            "\n",
            ">>> Step 127 metrics: {'loss': 1.4671, 'grad_norm': 9.125, 'learning_rate': 4.998631902279083e-05, 'epoch': 0.1727891156462585}\n",
            "\n",
            ">>> Step 128 metrics: {'loss': 1.4078, 'grad_norm': 10.3125, 'learning_rate': 4.9986082149136904e-05, 'epoch': 0.17414965986394557}\n",
            "\n",
            ">>> Step 129 metrics: {'loss': 1.5433, 'grad_norm': 9.5, 'learning_rate': 4.99858432429907e-05, 'epoch': 0.17551020408163265}\n",
            "\n",
            ">>> Step 130 metrics: {'loss': 0.5253, 'grad_norm': 6.65625, 'learning_rate': 4.998560230437168e-05, 'epoch': 0.17687074829931973}\n",
            "\n",
            ">>> Step 131 metrics: {'loss': 1.2376, 'grad_norm': 9.5625, 'learning_rate': 4.99853593332994e-05, 'epoch': 0.1782312925170068}\n",
            "\n",
            ">>> Step 132 metrics: {'loss': 1.1637, 'grad_norm': 7.90625, 'learning_rate': 4.9985114329793655e-05, 'epoch': 0.17959183673469387}\n",
            "\n",
            ">>> Step 133 metrics: {'loss': 0.9216, 'grad_norm': 8.1875, 'learning_rate': 4.998486729387436e-05, 'epoch': 0.18095238095238095}\n",
            "\n",
            ">>> Step 134 metrics: {'loss': 1.3774, 'grad_norm': 8.1875, 'learning_rate': 4.998461822556162e-05, 'epoch': 0.18231292517006803}\n",
            "\n",
            ">>> Step 135 metrics: {'loss': 0.677, 'grad_norm': 6.5, 'learning_rate': 4.9984367124875694e-05, 'epoch': 0.1836734693877551}\n",
            "\n",
            ">>> Step 136 metrics: {'loss': 1.3397, 'grad_norm': 8.6875, 'learning_rate': 4.9984113991837e-05, 'epoch': 0.18503401360544217}\n",
            "\n",
            ">>> Step 137 metrics: {'loss': 0.8331, 'grad_norm': 8.625, 'learning_rate': 4.998385882646613e-05, 'epoch': 0.18639455782312925}\n",
            "\n",
            ">>> Step 138 metrics: {'loss': 1.0993, 'grad_norm': 7.90625, 'learning_rate': 4.998360162878385e-05, 'epoch': 0.18775510204081633}\n",
            "\n",
            ">>> Step 139 metrics: {'loss': 1.2974, 'grad_norm': 7.625, 'learning_rate': 4.998334239881107e-05, 'epoch': 0.1891156462585034}\n",
            "\n",
            ">>> Step 140 metrics: {'loss': 1.4038, 'grad_norm': 9.25, 'learning_rate': 4.9983081136568885e-05, 'epoch': 0.19047619047619047}\n",
            "\n",
            ">>> Step 141 metrics: {'loss': 0.7806, 'grad_norm': 7.15625, 'learning_rate': 4.998281784207855e-05, 'epoch': 0.19183673469387755}\n",
            "\n",
            ">>> Step 142 metrics: {'loss': 1.4453, 'grad_norm': 8.875, 'learning_rate': 4.9982552515361476e-05, 'epoch': 0.19319727891156463}\n",
            "\n",
            ">>> Step 143 metrics: {'loss': 0.716, 'grad_norm': 8.3125, 'learning_rate': 4.998228515643924e-05, 'epoch': 0.1945578231292517}\n",
            "\n",
            ">>> Step 144 metrics: {'loss': 1.0124, 'grad_norm': 8.375, 'learning_rate': 4.998201576533361e-05, 'epoch': 0.19591836734693877}\n",
            "\n",
            ">>> Step 145 metrics: {'loss': 1.4642, 'grad_norm': 9.5625, 'learning_rate': 4.998174434206648e-05, 'epoch': 0.19727891156462585}\n",
            "\n",
            ">>> Step 146 metrics: {'loss': 0.7597, 'grad_norm': 6.8125, 'learning_rate': 4.998147088665994e-05, 'epoch': 0.19863945578231293}\n",
            "\n",
            ">>> Step 147 metrics: {'loss': 0.4534, 'grad_norm': 6.65625, 'learning_rate': 4.998119539913624e-05, 'epoch': 0.2}\n",
            "\n",
            ">>> Step 148 metrics: {'loss': 0.9404, 'grad_norm': 7.8125, 'learning_rate': 4.9980917879517766e-05, 'epoch': 0.20136054421768707}\n",
            "\n",
            ">>> Step 149 metrics: {'loss': 1.083, 'grad_norm': 8.125, 'learning_rate': 4.998063832782711e-05, 'epoch': 0.20272108843537415}\n",
            "\n",
            ">>> Step 150 metrics: {'loss': 0.5774, 'grad_norm': 6.90625, 'learning_rate': 4.9980356744087016e-05, 'epoch': 0.20408163265306123}\n",
            "\n",
            ">>> Step 151 metrics: {'loss': 0.7813, 'grad_norm': 7.09375, 'learning_rate': 4.998007312832038e-05, 'epoch': 0.2054421768707483}\n",
            "\n",
            ">>> Step 152 metrics: {'loss': 0.4523, 'grad_norm': 6.28125, 'learning_rate': 4.9979787480550275e-05, 'epoch': 0.20680272108843537}\n",
            "\n",
            ">>> Step 153 metrics: {'loss': 1.0581, 'grad_norm': 6.84375, 'learning_rate': 4.997949980079993e-05, 'epoch': 0.20816326530612245}\n",
            "\n",
            ">>> Step 154 metrics: {'loss': 0.7233, 'grad_norm': 10.375, 'learning_rate': 4.997921008909277e-05, 'epoch': 0.20952380952380953}\n",
            "\n",
            ">>> Step 155 metrics: {'loss': 1.3254, 'grad_norm': 7.8125, 'learning_rate': 4.997891834545234e-05, 'epoch': 0.2108843537414966}\n",
            "\n",
            ">>> Step 156 metrics: {'loss': 0.8169, 'grad_norm': 8.5625, 'learning_rate': 4.997862456990236e-05, 'epoch': 0.21224489795918366}\n",
            "\n",
            ">>> Step 157 metrics: {'loss': 1.2872, 'grad_norm': 8.75, 'learning_rate': 4.997832876246676e-05, 'epoch': 0.21360544217687075}\n",
            "\n",
            ">>> Step 158 metrics: {'loss': 1.0267, 'grad_norm': 6.78125, 'learning_rate': 4.997803092316959e-05, 'epoch': 0.21496598639455783}\n",
            "\n",
            ">>> Step 159 metrics: {'loss': 0.84, 'grad_norm': 9.4375, 'learning_rate': 4.9977731052035076e-05, 'epoch': 0.2163265306122449}\n",
            "\n",
            ">>> Step 160 metrics: {'loss': 0.9114, 'grad_norm': 7.90625, 'learning_rate': 4.9977429149087604e-05, 'epoch': 0.21768707482993196}\n",
            "\n",
            ">>> Step 161 metrics: {'loss': 0.5309, 'grad_norm': 6.78125, 'learning_rate': 4.997712521435174e-05, 'epoch': 0.21904761904761905}\n",
            "\n",
            ">>> Step 162 metrics: {'loss': 1.5772, 'grad_norm': 8.1875, 'learning_rate': 4.9976819247852206e-05, 'epoch': 0.22040816326530613}\n",
            "\n",
            ">>> Step 163 metrics: {'loss': 1.2002, 'grad_norm': 7.625, 'learning_rate': 4.997651124961389e-05, 'epoch': 0.2217687074829932}\n",
            "\n",
            ">>> Step 164 metrics: {'loss': 1.0503, 'grad_norm': 7.96875, 'learning_rate': 4.997620121966185e-05, 'epoch': 0.22312925170068026}\n",
            "\n",
            ">>> Step 165 metrics: {'loss': 0.7789, 'grad_norm': 9.3125, 'learning_rate': 4.997588915802129e-05, 'epoch': 0.22448979591836735}\n",
            "\n",
            ">>> Step 166 metrics: {'loss': 1.7526, 'grad_norm': 8.9375, 'learning_rate': 4.9975575064717615e-05, 'epoch': 0.22585034013605443}\n",
            "\n",
            ">>> Step 167 metrics: {'loss': 0.832, 'grad_norm': 8.0625, 'learning_rate': 4.997525893977637e-05, 'epoch': 0.2272108843537415}\n",
            "\n",
            ">>> Step 168 metrics: {'loss': 0.6574, 'grad_norm': 5.78125, 'learning_rate': 4.9974940783223256e-05, 'epoch': 0.22857142857142856}\n",
            "\n",
            ">>> Step 169 metrics: {'loss': 0.6967, 'grad_norm': 6.84375, 'learning_rate': 4.997462059508417e-05, 'epoch': 0.22993197278911565}\n",
            "\n",
            ">>> Step 170 metrics: {'loss': 0.9208, 'grad_norm': 8.1875, 'learning_rate': 4.9974298375385156e-05, 'epoch': 0.23129251700680273}\n",
            "\n",
            ">>> Step 171 metrics: {'loss': 1.132, 'grad_norm': 7.84375, 'learning_rate': 4.997397412415241e-05, 'epoch': 0.23265306122448978}\n",
            "\n",
            ">>> Step 172 metrics: {'loss': 0.6965, 'grad_norm': 6.59375, 'learning_rate': 4.997364784141233e-05, 'epoch': 0.23401360544217686}\n",
            "\n",
            ">>> Step 173 metrics: {'loss': 1.0821, 'grad_norm': 8.9375, 'learning_rate': 4.997331952719144e-05, 'epoch': 0.23537414965986395}\n",
            "\n",
            ">>> Step 174 metrics: {'loss': 0.9535, 'grad_norm': 9.125, 'learning_rate': 4.997298918151646e-05, 'epoch': 0.23673469387755103}\n",
            "\n",
            ">>> Step 175 metrics: {'loss': 0.7863, 'grad_norm': 10.0, 'learning_rate': 4.997265680441425e-05, 'epoch': 0.23809523809523808}\n",
            "\n",
            ">>> Step 176 metrics: {'loss': 1.2094, 'grad_norm': 7.9375, 'learning_rate': 4.9972322395911856e-05, 'epoch': 0.23945578231292516}\n",
            "\n",
            ">>> Step 177 metrics: {'loss': 0.5847, 'grad_norm': 6.40625, 'learning_rate': 4.997198595603647e-05, 'epoch': 0.24081632653061225}\n",
            "\n",
            ">>> Step 178 metrics: {'loss': 0.7876, 'grad_norm': 6.96875, 'learning_rate': 4.997164748481547e-05, 'epoch': 0.24217687074829933}\n",
            "\n",
            ">>> Step 179 metrics: {'loss': 0.8429, 'grad_norm': 8.375, 'learning_rate': 4.997130698227639e-05, 'epoch': 0.24353741496598638}\n",
            "\n",
            ">>> Step 180 metrics: {'loss': 0.8333, 'grad_norm': 8.125, 'learning_rate': 4.997096444844692e-05, 'epoch': 0.24489795918367346}\n",
            "\n",
            ">>> Step 181 metrics: {'loss': 1.7238, 'grad_norm': 9.0625, 'learning_rate': 4.997061988335492e-05, 'epoch': 0.24625850340136055}\n",
            "\n",
            ">>> Step 182 metrics: {'loss': 0.7468, 'grad_norm': 7.1875, 'learning_rate': 4.9970273287028435e-05, 'epoch': 0.24761904761904763}\n",
            "\n",
            ">>> Step 183 metrics: {'loss': 0.7175, 'grad_norm': 6.46875, 'learning_rate': 4.996992465949565e-05, 'epoch': 0.24897959183673468}\n",
            "\n",
            ">>> Step 184 metrics: {'loss': 1.2866, 'grad_norm': 8.0625, 'learning_rate': 4.9969574000784914e-05, 'epoch': 0.2503401360544218}\n",
            "\n",
            ">>> Step 185 metrics: {'loss': 1.4601, 'grad_norm': 7.8125, 'learning_rate': 4.9969221310924775e-05, 'epoch': 0.25170068027210885}\n",
            "\n",
            ">>> Step 186 metrics: {'loss': 0.762, 'grad_norm': 7.09375, 'learning_rate': 4.9968866589943895e-05, 'epoch': 0.2530612244897959}\n",
            "\n",
            ">>> Step 187 metrics: {'loss': 0.6503, 'grad_norm': 6.53125, 'learning_rate': 4.9968509837871147e-05, 'epoch': 0.254421768707483}\n",
            "\n",
            ">>> Step 188 metrics: {'loss': 1.0052, 'grad_norm': 7.09375, 'learning_rate': 4.9968151054735547e-05, 'epoch': 0.25578231292517006}\n",
            "\n",
            ">>> Step 189 metrics: {'loss': 1.2135, 'grad_norm': 7.8125, 'learning_rate': 4.996779024056628e-05, 'epoch': 0.2571428571428571}\n",
            "\n",
            ">>> Step 190 metrics: {'loss': 1.3383, 'grad_norm': 8.375, 'learning_rate': 4.9967427395392695e-05, 'epoch': 0.2585034013605442}\n",
            "\n",
            ">>> Step 191 metrics: {'loss': 0.8311, 'grad_norm': 8.125, 'learning_rate': 4.996706251924431e-05, 'epoch': 0.2598639455782313}\n",
            "\n",
            ">>> Step 192 metrics: {'loss': 0.6415, 'grad_norm': 6.84375, 'learning_rate': 4.99666956121508e-05, 'epoch': 0.2612244897959184}\n",
            "\n",
            ">>> Step 193 metrics: {'loss': 1.3039, 'grad_norm': 8.0625, 'learning_rate': 4.996632667414203e-05, 'epoch': 0.26258503401360545}\n",
            "\n",
            ">>> Step 194 metrics: {'loss': 1.1112, 'grad_norm': 7.34375, 'learning_rate': 4.996595570524798e-05, 'epoch': 0.2639455782312925}\n",
            "\n",
            ">>> Step 195 metrics: {'loss': 0.7818, 'grad_norm': 7.34375, 'learning_rate': 4.996558270549885e-05, 'epoch': 0.2653061224489796}\n",
            "\n",
            ">>> Step 196 metrics: {'loss': 0.8074, 'grad_norm': 7.65625, 'learning_rate': 4.996520767492497e-05, 'epoch': 0.26666666666666666}\n",
            "\n",
            ">>> Step 197 metrics: {'loss': 0.8136, 'grad_norm': 7.4375, 'learning_rate': 4.9964830613556855e-05, 'epoch': 0.2680272108843537}\n",
            "\n",
            ">>> Step 198 metrics: {'loss': 0.9435, 'grad_norm': 7.8125, 'learning_rate': 4.996445152142517e-05, 'epoch': 0.2693877551020408}\n",
            "\n",
            ">>> Step 199 metrics: {'loss': 0.8715, 'grad_norm': 6.46875, 'learning_rate': 4.9964070398560765e-05, 'epoch': 0.2707482993197279}\n",
            "\n",
            ">>> Step 200 metrics: {'loss': 0.6874, 'grad_norm': 7.59375, 'learning_rate': 4.9963687244994626e-05, 'epoch': 0.272108843537415}\n",
            "\n",
            ">>> Step 201 metrics: {'loss': 1.6193, 'grad_norm': 8.75, 'learning_rate': 4.996330206075793e-05, 'epoch': 0.27346938775510204}\n",
            "\n",
            ">>> Step 202 metrics: {'loss': 1.0388, 'grad_norm': 7.15625, 'learning_rate': 4.9962914845882006e-05, 'epoch': 0.2748299319727891}\n",
            "\n",
            ">>> Step 203 metrics: {'loss': 1.0417, 'grad_norm': 6.78125, 'learning_rate': 4.9962525600398356e-05, 'epoch': 0.2761904761904762}\n",
            "\n",
            ">>> Step 204 metrics: {'loss': 0.8327, 'grad_norm': 6.28125, 'learning_rate': 4.996213432433864e-05, 'epoch': 0.27755102040816326}\n",
            "\n",
            ">>> Step 205 metrics: {'loss': 0.9207, 'grad_norm': 6.53125, 'learning_rate': 4.9961741017734686e-05, 'epoch': 0.2789115646258503}\n",
            "\n",
            ">>> Step 206 metrics: {'loss': 1.4144, 'grad_norm': 8.75, 'learning_rate': 4.9961345680618494e-05, 'epoch': 0.2802721088435374}\n",
            "\n",
            ">>> Step 207 metrics: {'loss': 1.0429, 'grad_norm': 7.6875, 'learning_rate': 4.9960948313022214e-05, 'epoch': 0.2816326530612245}\n",
            "\n",
            ">>> Step 208 metrics: {'loss': 0.5787, 'grad_norm': 5.8125, 'learning_rate': 4.996054891497817e-05, 'epoch': 0.2829931972789116}\n",
            "\n",
            ">>> Step 209 metrics: {'loss': 0.8357, 'grad_norm': 6.65625, 'learning_rate': 4.996014748651886e-05, 'epoch': 0.28435374149659864}\n",
            "\n",
            ">>> Step 210 metrics: {'loss': 0.8352, 'grad_norm': 6.40625, 'learning_rate': 4.9959744027676926e-05, 'epoch': 0.2857142857142857}\n",
            "\n",
            ">>> Step 211 metrics: {'loss': 1.2199, 'grad_norm': 8.125, 'learning_rate': 4.99593385384852e-05, 'epoch': 0.2870748299319728}\n",
            "\n",
            ">>> Step 212 metrics: {'loss': 0.8654, 'grad_norm': 7.375, 'learning_rate': 4.995893101897666e-05, 'epoch': 0.28843537414965986}\n",
            "\n",
            ">>> Step 213 metrics: {'loss': 1.2462, 'grad_norm': 7.71875, 'learning_rate': 4.995852146918445e-05, 'epoch': 0.2897959183673469}\n",
            "\n",
            ">>> Step 214 metrics: {'loss': 0.9954, 'grad_norm': 8.375, 'learning_rate': 4.99581098891419e-05, 'epoch': 0.291156462585034}\n",
            "\n",
            ">>> Step 215 metrics: {'loss': 1.5767, 'grad_norm': 9.1875, 'learning_rate': 4.9957696278882474e-05, 'epoch': 0.2925170068027211}\n",
            "\n",
            ">>> Step 216 metrics: {'loss': 1.1818, 'grad_norm': 8.1875, 'learning_rate': 4.9957280638439824e-05, 'epoch': 0.2938775510204082}\n",
            "\n",
            ">>> Step 217 metrics: {'loss': 0.6555, 'grad_norm': 6.6875, 'learning_rate': 4.995686296784776e-05, 'epoch': 0.29523809523809524}\n",
            "\n",
            ">>> Step 218 metrics: {'loss': 0.929, 'grad_norm': 7.03125, 'learning_rate': 4.9956443267140266e-05, 'epoch': 0.2965986394557823}\n",
            "\n",
            ">>> Step 219 metrics: {'loss': 0.7944, 'grad_norm': 7.875, 'learning_rate': 4.995602153635146e-05, 'epoch': 0.2979591836734694}\n",
            "\n",
            ">>> Step 220 metrics: {'loss': 1.1903, 'grad_norm': 9.8125, 'learning_rate': 4.995559777551567e-05, 'epoch': 0.29931972789115646}\n",
            "\n",
            ">>> Step 221 metrics: {'loss': 0.8467, 'grad_norm': 7.9375, 'learning_rate': 4.995517198466736e-05, 'epoch': 0.3006802721088435}\n",
            "\n",
            ">>> Step 222 metrics: {'loss': 1.0243, 'grad_norm': 8.1875, 'learning_rate': 4.995474416384116e-05, 'epoch': 0.3020408163265306}\n",
            "\n",
            ">>> Step 223 metrics: {'loss': 1.0053, 'grad_norm': 7.46875, 'learning_rate': 4.9954314313071884e-05, 'epoch': 0.3034013605442177}\n",
            "\n",
            ">>> Step 224 metrics: {'loss': 0.7003, 'grad_norm': 8.0625, 'learning_rate': 4.9953882432394484e-05, 'epoch': 0.3047619047619048}\n",
            "\n",
            ">>> Step 225 metrics: {'loss': 1.0442, 'grad_norm': 7.5625, 'learning_rate': 4.995344852184409e-05, 'epoch': 0.30612244897959184}\n",
            "\n",
            ">>> Step 226 metrics: {'loss': 1.1277, 'grad_norm': 7.96875, 'learning_rate': 4.995301258145602e-05, 'epoch': 0.3074829931972789}\n",
            "\n",
            ">>> Step 227 metrics: {'loss': 1.5468, 'grad_norm': 7.9375, 'learning_rate': 4.995257461126571e-05, 'epoch': 0.308843537414966}\n",
            "\n",
            ">>> Step 228 metrics: {'loss': 0.5237, 'grad_norm': 7.21875, 'learning_rate': 4.9952134611308806e-05, 'epoch': 0.31020408163265306}\n",
            "\n",
            ">>> Step 229 metrics: {'loss': 0.7305, 'grad_norm': 8.0625, 'learning_rate': 4.995169258162109e-05, 'epoch': 0.3115646258503401}\n",
            "\n",
            ">>> Step 230 metrics: {'loss': 1.1275, 'grad_norm': 7.90625, 'learning_rate': 4.995124852223851e-05, 'epoch': 0.3129251700680272}\n",
            "\n",
            ">>> Step 231 metrics: {'loss': 1.1029, 'grad_norm': 9.0625, 'learning_rate': 4.9950802433197215e-05, 'epoch': 0.3142857142857143}\n",
            "\n",
            ">>> Step 232 metrics: {'loss': 1.1125, 'grad_norm': 8.0, 'learning_rate': 4.9950354314533463e-05, 'epoch': 0.31564625850340133}\n",
            "\n",
            ">>> Step 233 metrics: {'loss': 1.0788, 'grad_norm': 6.8125, 'learning_rate': 4.9949904166283727e-05, 'epoch': 0.31700680272108844}\n",
            "\n",
            ">>> Step 234 metrics: {'loss': 0.9714, 'grad_norm': 8.8125, 'learning_rate': 4.9949451988484616e-05, 'epoch': 0.3183673469387755}\n",
            "\n",
            ">>> Step 235 metrics: {'loss': 1.443, 'grad_norm': 7.53125, 'learning_rate': 4.994899778117291e-05, 'epoch': 0.3197278911564626}\n",
            "\n",
            ">>> Step 236 metrics: {'loss': 0.9807, 'grad_norm': 7.3125, 'learning_rate': 4.994854154438555e-05, 'epoch': 0.32108843537414966}\n",
            "\n",
            ">>> Step 237 metrics: {'loss': 0.9484, 'grad_norm': 7.03125, 'learning_rate': 4.9948083278159674e-05, 'epoch': 0.3224489795918367}\n",
            "\n",
            ">>> Step 238 metrics: {'loss': 0.8326, 'grad_norm': 6.78125, 'learning_rate': 4.994762298253253e-05, 'epoch': 0.3238095238095238}\n",
            "\n",
            ">>> Step 239 metrics: {'loss': 0.8709, 'grad_norm': 6.625, 'learning_rate': 4.994716065754158e-05, 'epoch': 0.3251700680272109}\n",
            "\n",
            ">>> Step 240 metrics: {'loss': 1.0441, 'grad_norm': 7.59375, 'learning_rate': 4.994669630322443e-05, 'epoch': 0.32653061224489793}\n",
            "\n",
            ">>> Step 241 metrics: {'loss': 1.4385, 'grad_norm': 10.4375, 'learning_rate': 4.994622991961884e-05, 'epoch': 0.32789115646258504}\n",
            "\n",
            ">>> Step 242 metrics: {'loss': 0.8209, 'grad_norm': 7.75, 'learning_rate': 4.994576150676276e-05, 'epoch': 0.3292517006802721}\n",
            "\n",
            ">>> Step 243 metrics: {'loss': 1.4016, 'grad_norm': 11.375, 'learning_rate': 4.994529106469429e-05, 'epoch': 0.3306122448979592}\n",
            "\n",
            ">>> Step 244 metrics: {'loss': 0.8407, 'grad_norm': 7.59375, 'learning_rate': 4.9944818593451706e-05, 'epoch': 0.33197278911564626}\n",
            "\n",
            ">>> Step 245 metrics: {'loss': 0.9009, 'grad_norm': 7.21875, 'learning_rate': 4.994434409307342e-05, 'epoch': 0.3333333333333333}\n",
            "\n",
            ">>> Step 246 metrics: {'loss': 0.9887, 'grad_norm': 7.34375, 'learning_rate': 4.994386756359805e-05, 'epoch': 0.3346938775510204}\n",
            "\n",
            ">>> Step 247 metrics: {'loss': 0.8345, 'grad_norm': 7.59375, 'learning_rate': 4.9943389005064346e-05, 'epoch': 0.3360544217687075}\n",
            "\n",
            ">>> Step 248 metrics: {'loss': 1.465, 'grad_norm': 7.5625, 'learning_rate': 4.994290841751125e-05, 'epoch': 0.33741496598639453}\n",
            "\n",
            ">>> Step 249 metrics: {'loss': 0.724, 'grad_norm': 6.125, 'learning_rate': 4.994242580097784e-05, 'epoch': 0.33877551020408164}\n",
            "\n",
            ">>> Step 250 metrics: {'loss': 0.7801, 'grad_norm': 6.3125, 'learning_rate': 4.994194115550339e-05, 'epoch': 0.3401360544217687}\n",
            "\n",
            ">>> Step 251 metrics: {'loss': 1.2185, 'grad_norm': 7.59375, 'learning_rate': 4.9941454481127315e-05, 'epoch': 0.3414965986394558}\n",
            "\n",
            ">>> Step 252 metrics: {'loss': 1.3863, 'grad_norm': 7.5625, 'learning_rate': 4.994096577788921e-05, 'epoch': 0.34285714285714286}\n",
            "\n",
            ">>> Step 253 metrics: {'loss': 1.7731, 'grad_norm': 11.875, 'learning_rate': 4.994047504582882e-05, 'epoch': 0.3442176870748299}\n",
            "\n",
            ">>> Step 254 metrics: {'loss': 0.9393, 'grad_norm': 7.375, 'learning_rate': 4.9939982284986055e-05, 'epoch': 0.345578231292517}\n",
            "\n",
            ">>> Step 255 metrics: {'loss': 1.4727, 'grad_norm': 8.0, 'learning_rate': 4.993948749540102e-05, 'epoch': 0.3469387755102041}\n",
            "\n",
            ">>> Step 256 metrics: {'loss': 0.8647, 'grad_norm': 8.125, 'learning_rate': 4.9938990677113954e-05, 'epoch': 0.34829931972789113}\n",
            "\n",
            ">>> Step 257 metrics: {'loss': 0.9404, 'grad_norm': 6.875, 'learning_rate': 4.993849183016527e-05, 'epoch': 0.34965986394557824}\n",
            "\n",
            ">>> Step 258 metrics: {'loss': 1.5184, 'grad_norm': 7.59375, 'learning_rate': 4.993799095459555e-05, 'epoch': 0.3510204081632653}\n",
            "\n",
            ">>> Step 259 metrics: {'loss': 0.8508, 'grad_norm': 6.5, 'learning_rate': 4.993748805044553e-05, 'epoch': 0.3523809523809524}\n",
            "\n",
            ">>> Step 260 metrics: {'loss': 0.9486, 'grad_norm': 6.4375, 'learning_rate': 4.9936983117756126e-05, 'epoch': 0.35374149659863946}\n",
            "\n",
            ">>> Step 261 metrics: {'loss': 1.1499, 'grad_norm': 8.625, 'learning_rate': 4.9936476156568416e-05, 'epoch': 0.3551020408163265}\n",
            "\n",
            ">>> Step 262 metrics: {'loss': 0.8863, 'grad_norm': 6.90625, 'learning_rate': 4.993596716692362e-05, 'epoch': 0.3564625850340136}\n",
            "\n",
            ">>> Step 263 metrics: {'loss': 0.6794, 'grad_norm': 6.71875, 'learning_rate': 4.993545614886316e-05, 'epoch': 0.3578231292517007}\n",
            "\n",
            ">>> Step 264 metrics: {'loss': 0.9089, 'grad_norm': 6.46875, 'learning_rate': 4.99349431024286e-05, 'epoch': 0.35918367346938773}\n",
            "\n",
            ">>> Step 265 metrics: {'loss': 0.9311, 'grad_norm': 6.1875, 'learning_rate': 4.9934428027661674e-05, 'epoch': 0.36054421768707484}\n",
            "\n",
            ">>> Step 266 metrics: {'loss': 0.7457, 'grad_norm': 7.28125, 'learning_rate': 4.993391092460428e-05, 'epoch': 0.3619047619047619}\n",
            "\n",
            ">>> Step 267 metrics: {'loss': 1.2157, 'grad_norm': 6.28125, 'learning_rate': 4.993339179329847e-05, 'epoch': 0.363265306122449}\n",
            "\n",
            ">>> Step 268 metrics: {'loss': 0.8597, 'grad_norm': 6.53125, 'learning_rate': 4.9932870633786496e-05, 'epoch': 0.36462585034013606}\n",
            "\n",
            ">>> Step 269 metrics: {'loss': 1.0344, 'grad_norm': 7.75, 'learning_rate': 4.993234744611073e-05, 'epoch': 0.3659863945578231}\n",
            "\n",
            ">>> Step 270 metrics: {'loss': 1.3988, 'grad_norm': 9.375, 'learning_rate': 4.993182223031375e-05, 'epoch': 0.3673469387755102}\n",
            "\n",
            ">>> Step 271 metrics: {'loss': 0.8608, 'grad_norm': 6.3125, 'learning_rate': 4.993129498643826e-05, 'epoch': 0.3687074829931973}\n",
            "\n",
            ">>> Step 272 metrics: {'loss': 0.6908, 'grad_norm': 6.5625, 'learning_rate': 4.993076571452717e-05, 'epoch': 0.37006802721088433}\n",
            "\n",
            ">>> Step 273 metrics: {'loss': 1.3244, 'grad_norm': 7.125, 'learning_rate': 4.9930234414623513e-05, 'epoch': 0.37142857142857144}\n",
            "\n",
            ">>> Step 274 metrics: {'loss': 0.7421, 'grad_norm': 8.5, 'learning_rate': 4.992970108677052e-05, 'epoch': 0.3727891156462585}\n",
            "\n",
            ">>> Step 275 metrics: {'loss': 0.8741, 'grad_norm': 7.125, 'learning_rate': 4.9929165731011565e-05, 'epoch': 0.3741496598639456}\n",
            "\n",
            ">>> Step 276 metrics: {'loss': 0.9292, 'grad_norm': 7.625, 'learning_rate': 4.992862834739022e-05, 'epoch': 0.37551020408163266}\n",
            "\n",
            ">>> Step 277 metrics: {'loss': 0.8871, 'grad_norm': 6.71875, 'learning_rate': 4.9928088935950164e-05, 'epoch': 0.3768707482993197}\n",
            "\n",
            ">>> Step 278 metrics: {'loss': 0.6474, 'grad_norm': 7.25, 'learning_rate': 4.99275474967353e-05, 'epoch': 0.3782312925170068}\n",
            "\n",
            ">>> Step 279 metrics: {'loss': 1.3751, 'grad_norm': 7.96875, 'learning_rate': 4.992700402978966e-05, 'epoch': 0.3795918367346939}\n",
            "\n",
            ">>> Step 280 metrics: {'loss': 1.2803, 'grad_norm': 7.78125, 'learning_rate': 4.992645853515746e-05, 'epoch': 0.38095238095238093}\n",
            "\n",
            ">>> Step 281 metrics: {'loss': 0.9864, 'grad_norm': 10.25, 'learning_rate': 4.9925911012883065e-05, 'epoch': 0.38231292517006804}\n",
            "\n",
            ">>> Step 282 metrics: {'loss': 1.356, 'grad_norm': 7.34375, 'learning_rate': 4.992536146301102e-05, 'epoch': 0.3836734693877551}\n",
            "\n",
            ">>> Step 283 metrics: {'loss': 0.8658, 'grad_norm': 27.0, 'learning_rate': 4.992480988558602e-05, 'epoch': 0.38503401360544215}\n",
            "\n",
            ">>> Step 284 metrics: {'loss': 1.1151, 'grad_norm': 6.875, 'learning_rate': 4.992425628065295e-05, 'epoch': 0.38639455782312926}\n",
            "\n",
            ">>> Step 285 metrics: {'loss': 0.7821, 'grad_norm': 6.5, 'learning_rate': 4.992370064825683e-05, 'epoch': 0.3877551020408163}\n",
            "\n",
            ">>> Step 286 metrics: {'loss': 1.1292, 'grad_norm': 8.5625, 'learning_rate': 4.992314298844285e-05, 'epoch': 0.3891156462585034}\n",
            "\n",
            ">>> Step 287 metrics: {'loss': 1.4529, 'grad_norm': 7.0, 'learning_rate': 4.992258330125639e-05, 'epoch': 0.3904761904761905}\n",
            "\n",
            ">>> Step 288 metrics: {'loss': 0.8875, 'grad_norm': 6.59375, 'learning_rate': 4.992202158674297e-05, 'epoch': 0.39183673469387753}\n",
            "\n",
            ">>> Step 289 metrics: {'loss': 1.024, 'grad_norm': 7.34375, 'learning_rate': 4.9921457844948286e-05, 'epoch': 0.39319727891156464}\n",
            "\n",
            ">>> Step 290 metrics: {'loss': 0.8497, 'grad_norm': 8.875, 'learning_rate': 4.9920892075918184e-05, 'epoch': 0.3945578231292517}\n",
            "\n",
            ">>> Step 291 metrics: {'loss': 0.8538, 'grad_norm': 6.0625, 'learning_rate': 4.9920324279698705e-05, 'epoch': 0.39591836734693875}\n",
            "\n",
            ">>> Step 292 metrics: {'loss': 0.9999, 'grad_norm': 7.25, 'learning_rate': 4.991975445633602e-05, 'epoch': 0.39727891156462586}\n",
            "\n",
            ">>> Step 293 metrics: {'loss': 0.632, 'grad_norm': 8.3125, 'learning_rate': 4.991918260587649e-05, 'epoch': 0.3986394557823129}\n",
            "\n",
            ">>> Step 294 metrics: {'loss': 0.7308, 'grad_norm': 6.0625, 'learning_rate': 4.991860872836662e-05, 'epoch': 0.4}\n",
            "\n",
            ">>> Step 295 metrics: {'loss': 1.1319, 'grad_norm': 9.0625, 'learning_rate': 4.991803282385312e-05, 'epoch': 0.4013605442176871}\n",
            "\n",
            ">>> Step 296 metrics: {'loss': 0.9539, 'grad_norm': 7.6875, 'learning_rate': 4.991745489238281e-05, 'epoch': 0.40272108843537413}\n",
            "\n",
            ">>> Step 297 metrics: {'loss': 0.7774, 'grad_norm': 6.46875, 'learning_rate': 4.9916874934002704e-05, 'epoch': 0.40408163265306124}\n",
            "\n",
            ">>> Step 298 metrics: {'loss': 0.6026, 'grad_norm': 6.0, 'learning_rate': 4.991629294875999e-05, 'epoch': 0.4054421768707483}\n",
            "\n",
            ">>> Step 299 metrics: {'loss': 0.7786, 'grad_norm': 6.78125, 'learning_rate': 4.991570893670201e-05, 'epoch': 0.40680272108843535}\n",
            "\n",
            ">>> Step 300 metrics: {'loss': 0.7, 'grad_norm': 6.71875, 'learning_rate': 4.991512289787626e-05, 'epoch': 0.40816326530612246}\n",
            "\n",
            ">>> Step 301 metrics: {'loss': 0.5333, 'grad_norm': 6.3125, 'learning_rate': 4.991453483233042e-05, 'epoch': 0.4095238095238095}\n",
            "\n",
            ">>> Step 302 metrics: {'loss': 0.8035, 'grad_norm': 5.65625, 'learning_rate': 4.9913944740112314e-05, 'epoch': 0.4108843537414966}\n",
            "\n",
            ">>> Step 303 metrics: {'loss': 0.7028, 'grad_norm': 6.03125, 'learning_rate': 4.991335262126996e-05, 'epoch': 0.4122448979591837}\n",
            "\n",
            ">>> Step 304 metrics: {'loss': 1.2967, 'grad_norm': 6.71875, 'learning_rate': 4.9912758475851515e-05, 'epoch': 0.41360544217687073}\n",
            "\n",
            ">>> Step 305 metrics: {'loss': 1.171, 'grad_norm': 8.625, 'learning_rate': 4.9912162303905314e-05, 'epoch': 0.41496598639455784}\n",
            "\n",
            ">>> Step 306 metrics: {'loss': 1.2879, 'grad_norm': 8.3125, 'learning_rate': 4.991156410547985e-05, 'epoch': 0.4163265306122449}\n",
            "\n",
            ">>> Step 307 metrics: {'loss': 0.9983, 'grad_norm': 6.1875, 'learning_rate': 4.9910963880623776e-05, 'epoch': 0.41768707482993195}\n",
            "\n",
            ">>> Step 308 metrics: {'loss': 0.5663, 'grad_norm': 4.90625, 'learning_rate': 4.9910361629385916e-05, 'epoch': 0.41904761904761906}\n",
            "\n",
            ">>> Step 309 metrics: {'loss': 1.1071, 'grad_norm': 8.5625, 'learning_rate': 4.990975735181528e-05, 'epoch': 0.4204081632653061}\n",
            "\n",
            ">>> Step 310 metrics: {'loss': 0.7792, 'grad_norm': 7.125, 'learning_rate': 4.990915104796101e-05, 'epoch': 0.4217687074829932}\n",
            "\n",
            ">>> Step 311 metrics: {'loss': 0.8875, 'grad_norm': 6.6875, 'learning_rate': 4.990854271787242e-05, 'epoch': 0.4231292517006803}\n",
            "\n",
            ">>> Step 312 metrics: {'loss': 1.2879, 'grad_norm': 7.65625, 'learning_rate': 4.990793236159901e-05, 'epoch': 0.42448979591836733}\n",
            "\n",
            ">>> Step 313 metrics: {'loss': 1.1519, 'grad_norm': 8.125, 'learning_rate': 4.990731997919042e-05, 'epoch': 0.42585034013605444}\n",
            "\n",
            ">>> Step 314 metrics: {'loss': 1.1958, 'grad_norm': 7.40625, 'learning_rate': 4.990670557069645e-05, 'epoch': 0.4272108843537415}\n",
            "\n",
            ">>> Step 315 metrics: {'loss': 0.7455, 'grad_norm': 6.375, 'learning_rate': 4.990608913616711e-05, 'epoch': 0.42857142857142855}\n",
            "\n",
            ">>> Step 316 metrics: {'loss': 0.6919, 'grad_norm': 5.59375, 'learning_rate': 4.9905470675652523e-05, 'epoch': 0.42993197278911566}\n",
            "\n",
            ">>> Step 317 metrics: {'loss': 1.0042, 'grad_norm': 7.15625, 'learning_rate': 4.9904850189203003e-05, 'epoch': 0.4312925170068027}\n",
            "\n",
            ">>> Step 318 metrics: {'loss': 1.3323, 'grad_norm': 6.96875, 'learning_rate': 4.9904227676869025e-05, 'epoch': 0.4326530612244898}\n",
            "\n",
            ">>> Step 319 metrics: {'loss': 0.8667, 'grad_norm': 6.90625, 'learning_rate': 4.990360313870122e-05, 'epoch': 0.4340136054421769}\n",
            "\n",
            ">>> Step 320 metrics: {'loss': 0.5833, 'grad_norm': 5.75, 'learning_rate': 4.9902976574750406e-05, 'epoch': 0.43537414965986393}\n",
            "\n",
            ">>> Step 321 metrics: {'loss': 0.9104, 'grad_norm': 7.25, 'learning_rate': 4.990234798506753e-05, 'epoch': 0.43673469387755104}\n",
            "\n",
            ">>> Step 322 metrics: {'loss': 0.6432, 'grad_norm': 6.21875, 'learning_rate': 4.9901717369703735e-05, 'epoch': 0.4380952380952381}\n",
            "\n",
            ">>> Step 323 metrics: {'loss': 1.2415, 'grad_norm': 7.03125, 'learning_rate': 4.9901084728710324e-05, 'epoch': 0.43945578231292515}\n",
            "\n",
            ">>> Step 324 metrics: {'loss': 1.5544, 'grad_norm': 7.40625, 'learning_rate': 4.990045006213876e-05, 'epoch': 0.44081632653061226}\n",
            "\n",
            ">>> Step 325 metrics: {'loss': 0.7175, 'grad_norm': 7.25, 'learning_rate': 4.9899813370040656e-05, 'epoch': 0.4421768707482993}\n",
            "\n",
            ">>> Step 326 metrics: {'loss': 1.2804, 'grad_norm': 7.6875, 'learning_rate': 4.989917465246781e-05, 'epoch': 0.4435374149659864}\n",
            "\n",
            ">>> Step 327 metrics: {'loss': 0.5978, 'grad_norm': 8.5, 'learning_rate': 4.9898533909472185e-05, 'epoch': 0.4448979591836735}\n",
            "\n",
            ">>> Step 328 metrics: {'loss': 1.2647, 'grad_norm': 6.8125, 'learning_rate': 4.9897891141105895e-05, 'epoch': 0.44625850340136053}\n",
            "\n",
            ">>> Step 329 metrics: {'loss': 0.6044, 'grad_norm': 4.75, 'learning_rate': 4.9897246347421236e-05, 'epoch': 0.44761904761904764}\n",
            "\n",
            ">>> Step 330 metrics: {'loss': 0.638, 'grad_norm': 7.09375, 'learning_rate': 4.989659952847064e-05, 'epoch': 0.4489795918367347}\n",
            "\n",
            ">>> Step 331 metrics: {'loss': 1.4604, 'grad_norm': 9.0625, 'learning_rate': 4.989595068430674e-05, 'epoch': 0.45034013605442175}\n",
            "\n",
            ">>> Step 332 metrics: {'loss': 1.1871, 'grad_norm': 6.65625, 'learning_rate': 4.989529981498231e-05, 'epoch': 0.45170068027210886}\n",
            "\n",
            ">>> Step 333 metrics: {'loss': 1.1773, 'grad_norm': 7.75, 'learning_rate': 4.989464692055029e-05, 'epoch': 0.4530612244897959}\n",
            "\n",
            ">>> Step 334 metrics: {'loss': 1.192, 'grad_norm': 9.125, 'learning_rate': 4.98939920010638e-05, 'epoch': 0.454421768707483}\n",
            "\n",
            ">>> Step 335 metrics: {'loss': 0.8278, 'grad_norm': 7.0, 'learning_rate': 4.989333505657611e-05, 'epoch': 0.4557823129251701}\n",
            "\n",
            ">>> Step 336 metrics: {'loss': 0.8845, 'grad_norm': 7.40625, 'learning_rate': 4.989267608714065e-05, 'epoch': 0.45714285714285713}\n",
            "\n",
            ">>> Step 337 metrics: {'loss': 0.7423, 'grad_norm': 6.75, 'learning_rate': 4.989201509281104e-05, 'epoch': 0.45850340136054424}\n",
            "\n",
            ">>> Step 338 metrics: {'loss': 0.9963, 'grad_norm': 7.1875, 'learning_rate': 4.9891352073641034e-05, 'epoch': 0.4598639455782313}\n",
            "\n",
            ">>> Step 339 metrics: {'loss': 0.9473, 'grad_norm': 8.25, 'learning_rate': 4.989068702968458e-05, 'epoch': 0.46122448979591835}\n",
            "\n",
            ">>> Step 340 metrics: {'loss': 0.8044, 'grad_norm': 7.125, 'learning_rate': 4.989001996099576e-05, 'epoch': 0.46258503401360546}\n",
            "\n",
            ">>> Step 341 metrics: {'loss': 1.3046, 'grad_norm': 8.375, 'learning_rate': 4.9889350867628845e-05, 'epoch': 0.4639455782312925}\n",
            "\n",
            ">>> Step 342 metrics: {'loss': 1.0364, 'grad_norm': 9.0625, 'learning_rate': 4.9888679749638265e-05, 'epoch': 0.46530612244897956}\n",
            "\n",
            ">>> Step 343 metrics: {'loss': 1.1872, 'grad_norm': 7.28125, 'learning_rate': 4.988800660707861e-05, 'epoch': 0.4666666666666667}\n",
            "\n",
            ">>> Step 344 metrics: {'loss': 1.0659, 'grad_norm': 7.9375, 'learning_rate': 4.988733144000464e-05, 'epoch': 0.46802721088435373}\n",
            "\n",
            ">>> Step 345 metrics: {'loss': 0.8256, 'grad_norm': 6.96875, 'learning_rate': 4.988665424847126e-05, 'epoch': 0.46938775510204084}\n",
            "\n",
            ">>> Step 346 metrics: {'loss': 0.9697, 'grad_norm': 7.1875, 'learning_rate': 4.9885975032533575e-05, 'epoch': 0.4707482993197279}\n",
            "\n",
            ">>> Step 347 metrics: {'loss': 1.1684, 'grad_norm': 8.4375, 'learning_rate': 4.9885293792246825e-05, 'epoch': 0.47210884353741495}\n",
            "\n",
            ">>> Step 348 metrics: {'loss': 1.0073, 'grad_norm': 7.96875, 'learning_rate': 4.988461052766643e-05, 'epoch': 0.47346938775510206}\n",
            "\n",
            ">>> Step 349 metrics: {'loss': 0.9573, 'grad_norm': 7.9375, 'learning_rate': 4.988392523884798e-05, 'epoch': 0.4748299319727891}\n",
            "\n",
            ">>> Step 350 metrics: {'loss': 1.1664, 'grad_norm': 6.53125, 'learning_rate': 4.98832379258472e-05, 'epoch': 0.47619047619047616}\n",
            "\n",
            ">>> Step 351 metrics: {'loss': 1.0243, 'grad_norm': 7.1875, 'learning_rate': 4.988254858872001e-05, 'epoch': 0.4775510204081633}\n",
            "\n",
            ">>> Step 352 metrics: {'loss': 0.7494, 'grad_norm': 6.03125, 'learning_rate': 4.9881857227522486e-05, 'epoch': 0.47891156462585033}\n",
            "\n",
            ">>> Step 353 metrics: {'loss': 0.7749, 'grad_norm': 6.5625, 'learning_rate': 4.9881163842310865e-05, 'epoch': 0.48027210884353744}\n",
            "\n",
            ">>> Step 354 metrics: {'loss': 0.7084, 'grad_norm': 6.03125, 'learning_rate': 4.988046843314154e-05, 'epoch': 0.4816326530612245}\n",
            "\n",
            ">>> Step 355 metrics: {'loss': 1.1284, 'grad_norm': 7.375, 'learning_rate': 4.9879771000071094e-05, 'epoch': 0.48299319727891155}\n",
            "\n",
            ">>> Step 356 metrics: {'loss': 0.8084, 'grad_norm': 5.4375, 'learning_rate': 4.9879071543156254e-05, 'epoch': 0.48435374149659866}\n",
            "\n",
            ">>> Step 357 metrics: {'loss': 1.3177, 'grad_norm': 7.75, 'learning_rate': 4.987837006245392e-05, 'epoch': 0.4857142857142857}\n",
            "\n",
            ">>> Step 358 metrics: {'loss': 0.9101, 'grad_norm': 6.53125, 'learning_rate': 4.987766655802115e-05, 'epoch': 0.48707482993197276}\n",
            "\n",
            ">>> Step 359 metrics: {'loss': 1.1479, 'grad_norm': 8.75, 'learning_rate': 4.987696102991517e-05, 'epoch': 0.4884353741496599}\n",
            "\n",
            ">>> Step 360 metrics: {'loss': 1.5065, 'grad_norm': 7.0625, 'learning_rate': 4.9876253478193365e-05, 'epoch': 0.4897959183673469}\n",
            "\n",
            ">>> Step 361 metrics: {'loss': 0.6219, 'grad_norm': 5.9375, 'learning_rate': 4.9875543902913315e-05, 'epoch': 0.49115646258503404}\n",
            "\n",
            ">>> Step 362 metrics: {'loss': 0.8711, 'grad_norm': 6.96875, 'learning_rate': 4.9874832304132716e-05, 'epoch': 0.4925170068027211}\n",
            "\n",
            ">>> Step 363 metrics: {'loss': 0.8185, 'grad_norm': 5.78125, 'learning_rate': 4.987411868190946e-05, 'epoch': 0.49387755102040815}\n",
            "\n",
            ">>> Step 364 metrics: {'loss': 0.8881, 'grad_norm': 6.78125, 'learning_rate': 4.98734030363016e-05, 'epoch': 0.49523809523809526}\n",
            "\n",
            ">>> Step 365 metrics: {'loss': 1.3127, 'grad_norm': 7.75, 'learning_rate': 4.9872685367367344e-05, 'epoch': 0.4965986394557823}\n",
            "\n",
            ">>> Step 366 metrics: {'loss': 0.8157, 'grad_norm': 5.84375, 'learning_rate': 4.987196567516508e-05, 'epoch': 0.49795918367346936}\n",
            "\n",
            ">>> Step 367 metrics: {'loss': 1.2262, 'grad_norm': 8.5, 'learning_rate': 4.987124395975334e-05, 'epoch': 0.4993197278911565}\n",
            "\n",
            ">>> Step 368 metrics: {'loss': 0.7903, 'grad_norm': 6.90625, 'learning_rate': 4.987052022119084e-05, 'epoch': 0.5006802721088436}\n",
            "\n",
            ">>> Step 369 metrics: {'loss': 1.2952, 'grad_norm': 6.5625, 'learning_rate': 4.986979445953645e-05, 'epoch': 0.5020408163265306}\n",
            "\n",
            ">>> Step 370 metrics: {'loss': 0.9955, 'grad_norm': 7.90625, 'learning_rate': 4.9869066674849215e-05, 'epoch': 0.5034013605442177}\n",
            "\n",
            ">>> Step 371 metrics: {'loss': 1.1991, 'grad_norm': 6.96875, 'learning_rate': 4.986833686718832e-05, 'epoch': 0.5047619047619047}\n",
            "\n",
            ">>> Step 372 metrics: {'loss': 1.0033, 'grad_norm': 6.21875, 'learning_rate': 4.986760503661314e-05, 'epoch': 0.5061224489795918}\n",
            "\n",
            ">>> Step 373 metrics: {'loss': 0.7117, 'grad_norm': 7.125, 'learning_rate': 4.986687118318322e-05, 'epoch': 0.507482993197279}\n",
            "\n",
            ">>> Step 374 metrics: {'loss': 1.0753, 'grad_norm': 8.125, 'learning_rate': 4.9866135306958224e-05, 'epoch': 0.508843537414966}\n",
            "\n",
            ">>> Step 375 metrics: {'loss': 0.7589, 'grad_norm': 8.0625, 'learning_rate': 4.986539740799804e-05, 'epoch': 0.5102040816326531}\n",
            "\n",
            ">>> Step 376 metrics: {'loss': 0.5923, 'grad_norm': 5.78125, 'learning_rate': 4.986465748636267e-05, 'epoch': 0.5115646258503401}\n",
            "\n",
            ">>> Step 377 metrics: {'loss': 1.3005, 'grad_norm': 7.53125, 'learning_rate': 4.9863915542112324e-05, 'epoch': 0.5129251700680272}\n",
            "\n",
            ">>> Step 378 metrics: {'loss': 0.5481, 'grad_norm': 5.25, 'learning_rate': 4.986317157530734e-05, 'epoch': 0.5142857142857142}\n",
            "\n",
            ">>> Step 379 metrics: {'loss': 1.6295, 'grad_norm': 8.5625, 'learning_rate': 4.9862425586008246e-05, 'epoch': 0.5156462585034014}\n",
            "\n",
            ">>> Step 380 metrics: {'loss': 1.3239, 'grad_norm': 7.90625, 'learning_rate': 4.986167757427572e-05, 'epoch': 0.5170068027210885}\n",
            "\n",
            ">>> Step 381 metrics: {'loss': 0.9803, 'grad_norm': 7.75, 'learning_rate': 4.986092754017061e-05, 'epoch': 0.5183673469387755}\n",
            "\n",
            ">>> Step 382 metrics: {'loss': 0.9795, 'grad_norm': 6.3125, 'learning_rate': 4.9860175483753925e-05, 'epoch': 0.5197278911564626}\n",
            "\n",
            ">>> Step 383 metrics: {'loss': 0.9537, 'grad_norm': 6.3125, 'learning_rate': 4.9859421405086836e-05, 'epoch': 0.5210884353741496}\n",
            "\n",
            ">>> Step 384 metrics: {'loss': 0.955, 'grad_norm': 6.21875, 'learning_rate': 4.98586653042307e-05, 'epoch': 0.5224489795918368}\n",
            "\n",
            ">>> Step 385 metrics: {'loss': 0.8292, 'grad_norm': 6.375, 'learning_rate': 4.985790718124701e-05, 'epoch': 0.5238095238095238}\n",
            "\n",
            ">>> Step 386 metrics: {'loss': 1.2028, 'grad_norm': 7.59375, 'learning_rate': 4.985714703619744e-05, 'epoch': 0.5251700680272109}\n",
            "\n",
            ">>> Step 387 metrics: {'loss': 1.1896, 'grad_norm': 7.1875, 'learning_rate': 4.9856384869143816e-05, 'epoch': 0.5265306122448979}\n",
            "\n",
            ">>> Step 388 metrics: {'loss': 0.6878, 'grad_norm': 6.28125, 'learning_rate': 4.985562068014814e-05, 'epoch': 0.527891156462585}\n",
            "\n",
            ">>> Step 389 metrics: {'loss': 1.4988, 'grad_norm': 7.34375, 'learning_rate': 4.985485446927258e-05, 'epoch': 0.5292517006802722}\n",
            "\n",
            ">>> Step 390 metrics: {'loss': 0.976, 'grad_norm': 6.53125, 'learning_rate': 4.985408623657947e-05, 'epoch': 0.5306122448979592}\n",
            "\n",
            ">>> Step 391 metrics: {'loss': 1.2085, 'grad_norm': 6.96875, 'learning_rate': 4.985331598213129e-05, 'epoch': 0.5319727891156463}\n",
            "\n",
            ">>> Step 392 metrics: {'loss': 0.8673, 'grad_norm': 7.40625, 'learning_rate': 4.985254370599069e-05, 'epoch': 0.5333333333333333}\n",
            "\n",
            ">>> Step 393 metrics: {'loss': 0.9173, 'grad_norm': 7.46875, 'learning_rate': 4.9851769408220507e-05, 'epoch': 0.5346938775510204}\n",
            "\n",
            ">>> Step 394 metrics: {'loss': 0.9857, 'grad_norm': 6.25, 'learning_rate': 4.9850993088883716e-05, 'epoch': 0.5360544217687074}\n",
            "\n",
            ">>> Step 395 metrics: {'loss': 1.0758, 'grad_norm': 7.1875, 'learning_rate': 4.9850214748043464e-05, 'epoch': 0.5374149659863946}\n",
            "\n",
            ">>> Step 396 metrics: {'loss': 0.8916, 'grad_norm': 6.3125, 'learning_rate': 4.9849434385763074e-05, 'epoch': 0.5387755102040817}\n",
            "\n",
            ">>> Step 397 metrics: {'loss': 0.7878, 'grad_norm': 6.0, 'learning_rate': 4.984865200210602e-05, 'epoch': 0.5401360544217687}\n",
            "\n",
            ">>> Step 398 metrics: {'loss': 1.4786, 'grad_norm': 7.46875, 'learning_rate': 4.984786759713595e-05, 'epoch': 0.5414965986394558}\n",
            "\n",
            ">>> Step 399 metrics: {'loss': 0.7923, 'grad_norm': 6.25, 'learning_rate': 4.984708117091667e-05, 'epoch': 0.5428571428571428}\n",
            "\n",
            ">>> Step 400 metrics: {'loss': 0.6935, 'grad_norm': 5.84375, 'learning_rate': 4.984629272351215e-05, 'epoch': 0.54421768707483}\n",
            "\n",
            ">>> Step 401 metrics: {'loss': 0.9583, 'grad_norm': 7.0625, 'learning_rate': 4.984550225498651e-05, 'epoch': 0.545578231292517}\n",
            "\n",
            ">>> Step 402 metrics: {'loss': 0.8678, 'grad_norm': 6.3125, 'learning_rate': 4.984470976540408e-05, 'epoch': 0.5469387755102041}\n",
            "\n",
            ">>> Step 403 metrics: {'loss': 1.2969, 'grad_norm': 8.6875, 'learning_rate': 4.9843915254829307e-05, 'epoch': 0.5482993197278911}\n",
            "\n",
            ">>> Step 404 metrics: {'loss': 1.0077, 'grad_norm': 5.875, 'learning_rate': 4.9843118723326834e-05, 'epoch': 0.5496598639455782}\n",
            "\n",
            ">>> Step 405 metrics: {'loss': 1.0691, 'grad_norm': 7.25, 'learning_rate': 4.9842320170961434e-05, 'epoch': 0.5510204081632653}\n",
            "\n",
            ">>> Step 406 metrics: {'loss': 0.7496, 'grad_norm': 5.875, 'learning_rate': 4.984151959779808e-05, 'epoch': 0.5523809523809524}\n",
            "\n",
            ">>> Step 407 metrics: {'loss': 0.7371, 'grad_norm': 7.75, 'learning_rate': 4.984071700390189e-05, 'epoch': 0.5537414965986395}\n",
            "\n",
            ">>> Step 408 metrics: {'loss': 1.197, 'grad_norm': 7.34375, 'learning_rate': 4.983991238933815e-05, 'epoch': 0.5551020408163265}\n",
            "\n",
            ">>> Step 409 metrics: {'loss': 0.9181, 'grad_norm': 7.09375, 'learning_rate': 4.983910575417232e-05, 'epoch': 0.5564625850340136}\n",
            "\n",
            ">>> Step 410 metrics: {'loss': 0.898, 'grad_norm': 7.09375, 'learning_rate': 4.983829709847001e-05, 'epoch': 0.5578231292517006}\n",
            "\n",
            ">>> Step 411 metrics: {'loss': 0.6847, 'grad_norm': 6.78125, 'learning_rate': 4.9837486422297e-05, 'epoch': 0.5591836734693878}\n",
            "\n",
            ">>> Step 412 metrics: {'loss': 0.8779, 'grad_norm': 6.53125, 'learning_rate': 4.983667372571923e-05, 'epoch': 0.5605442176870749}\n",
            "\n",
            ">>> Step 413 metrics: {'loss': 1.0232, 'grad_norm': 7.46875, 'learning_rate': 4.983585900880281e-05, 'epoch': 0.5619047619047619}\n",
            "\n",
            ">>> Step 414 metrics: {'loss': 0.8751, 'grad_norm': 7.75, 'learning_rate': 4.983504227161402e-05, 'epoch': 0.563265306122449}\n",
            "\n",
            ">>> Step 415 metrics: {'loss': 0.7494, 'grad_norm': 6.90625, 'learning_rate': 4.983422351421929e-05, 'epoch': 0.564625850340136}\n",
            "\n",
            ">>> Step 416 metrics: {'loss': 1.4112, 'grad_norm': 7.28125, 'learning_rate': 4.983340273668523e-05, 'epoch': 0.5659863945578232}\n",
            "\n",
            ">>> Step 417 metrics: {'loss': 1.1223, 'grad_norm': 7.8125, 'learning_rate': 4.98325799390786e-05, 'epoch': 0.5673469387755102}\n",
            "\n",
            ">>> Step 418 metrics: {'loss': 1.8154, 'grad_norm': 23.5, 'learning_rate': 4.983175512146633e-05, 'epoch': 0.5687074829931973}\n",
            "\n",
            ">>> Step 419 metrics: {'loss': 1.1671, 'grad_norm': 9.5, 'learning_rate': 4.983092828391551e-05, 'epoch': 0.5700680272108843}\n",
            "\n",
            ">>> Step 420 metrics: {'loss': 0.7872, 'grad_norm': 6.4375, 'learning_rate': 4.9830099426493415e-05, 'epoch': 0.5714285714285714}\n",
            "\n",
            ">>> Step 421 metrics: {'loss': 1.0557, 'grad_norm': 6.5625, 'learning_rate': 4.982926854926746e-05, 'epoch': 0.5727891156462585}\n",
            "\n",
            ">>> Step 422 metrics: {'loss': 1.3913, 'grad_norm': 6.25, 'learning_rate': 4.982843565230523e-05, 'epoch': 0.5741496598639456}\n",
            "\n",
            ">>> Step 423 metrics: {'loss': 1.4342, 'grad_norm': 9.75, 'learning_rate': 4.982760073567447e-05, 'epoch': 0.5755102040816327}\n",
            "\n",
            ">>> Step 424 metrics: {'loss': 0.9805, 'grad_norm': 5.90625, 'learning_rate': 4.982676379944312e-05, 'epoch': 0.5768707482993197}\n",
            "\n",
            ">>> Step 425 metrics: {'loss': 0.8104, 'grad_norm': 7.96875, 'learning_rate': 4.982592484367924e-05, 'epoch': 0.5782312925170068}\n",
            "\n",
            ">>> Step 426 metrics: {'loss': 1.1175, 'grad_norm': 6.625, 'learning_rate': 4.9825083868451075e-05, 'epoch': 0.5795918367346938}\n",
            "\n",
            ">>> Step 427 metrics: {'loss': 1.1182, 'grad_norm': 6.78125, 'learning_rate': 4.982424087382704e-05, 'epoch': 0.580952380952381}\n",
            "\n",
            ">>> Step 428 metrics: {'loss': 1.4327, 'grad_norm': 7.28125, 'learning_rate': 4.982339585987571e-05, 'epoch': 0.582312925170068}\n",
            "\n",
            ">>> Step 429 metrics: {'loss': 0.5785, 'grad_norm': 5.46875, 'learning_rate': 4.9822548826665814e-05, 'epoch': 0.5836734693877551}\n",
            "\n",
            ">>> Step 430 metrics: {'loss': 0.5533, 'grad_norm': 5.75, 'learning_rate': 4.982169977426627e-05, 'epoch': 0.5850340136054422}\n",
            "\n",
            ">>> Step 431 metrics: {'loss': 0.8177, 'grad_norm': 7.5, 'learning_rate': 4.982084870274614e-05, 'epoch': 0.5863945578231292}\n",
            "\n",
            ">>> Step 432 metrics: {'loss': 0.836, 'grad_norm': 6.4375, 'learning_rate': 4.981999561217463e-05, 'epoch': 0.5877551020408164}\n",
            "\n",
            ">>> Step 433 metrics: {'loss': 0.7645, 'grad_norm': 6.96875, 'learning_rate': 4.981914050262117e-05, 'epoch': 0.5891156462585034}\n",
            "\n",
            ">>> Step 434 metrics: {'loss': 1.1064, 'grad_norm': 7.0625, 'learning_rate': 4.9818283374155294e-05, 'epoch': 0.5904761904761905}\n",
            "\n",
            ">>> Step 435 metrics: {'loss': 0.8923, 'grad_norm': 6.53125, 'learning_rate': 4.9817424226846734e-05, 'epoch': 0.5918367346938775}\n",
            "\n",
            ">>> Step 436 metrics: {'loss': 0.974, 'grad_norm': 12.1875, 'learning_rate': 4.981656306076538e-05, 'epoch': 0.5931972789115646}\n",
            "\n",
            ">>> Step 437 metrics: {'loss': 1.0807, 'grad_norm': 7.0, 'learning_rate': 4.9815699875981276e-05, 'epoch': 0.5945578231292517}\n",
            "\n",
            ">>> Step 438 metrics: {'loss': 1.2596, 'grad_norm': 8.375, 'learning_rate': 4.981483467256464e-05, 'epoch': 0.5959183673469388}\n",
            "\n",
            ">>> Step 439 metrics: {'loss': 0.9909, 'grad_norm': 7.25, 'learning_rate': 4.981396745058586e-05, 'epoch': 0.5972789115646259}\n",
            "\n",
            ">>> Step 440 metrics: {'loss': 0.6959, 'grad_norm': 8.125, 'learning_rate': 4.9813098210115474e-05, 'epoch': 0.5986394557823129}\n",
            "\n",
            ">>> Step 441 metrics: {'loss': 0.7805, 'grad_norm': 7.0625, 'learning_rate': 4.981222695122418e-05, 'epoch': 0.6}\n",
            "\n",
            ">>> Step 442 metrics: {'loss': 0.9558, 'grad_norm': 9.0625, 'learning_rate': 4.9811353673982876e-05, 'epoch': 0.601360544217687}\n",
            "\n",
            ">>> Step 443 metrics: {'loss': 0.9078, 'grad_norm': 7.1875, 'learning_rate': 4.9810478378462574e-05, 'epoch': 0.6027210884353742}\n",
            "\n",
            ">>> Step 444 metrics: {'loss': 1.2635, 'grad_norm': 6.875, 'learning_rate': 4.980960106473449e-05, 'epoch': 0.6040816326530613}\n",
            "\n",
            ">>> Step 445 metrics: {'loss': 0.746, 'grad_norm': 6.5625, 'learning_rate': 4.980872173286998e-05, 'epoch': 0.6054421768707483}\n",
            "\n",
            ">>> Step 446 metrics: {'loss': 0.5305, 'grad_norm': 6.59375, 'learning_rate': 4.9807840382940584e-05, 'epoch': 0.6068027210884354}\n",
            "\n",
            ">>> Step 447 metrics: {'loss': 0.5912, 'grad_norm': 6.78125, 'learning_rate': 4.9806957015017984e-05, 'epoch': 0.6081632653061224}\n",
            "\n",
            ">>> Step 448 metrics: {'loss': 0.8966, 'grad_norm': 6.4375, 'learning_rate': 4.980607162917404e-05, 'epoch': 0.6095238095238096}\n",
            "\n",
            ">>> Step 449 metrics: {'loss': 1.17, 'grad_norm': 6.6875, 'learning_rate': 4.980518422548077e-05, 'epoch': 0.6108843537414966}\n",
            "\n",
            ">>> Step 450 metrics: {'loss': 0.8303, 'grad_norm': 6.625, 'learning_rate': 4.980429480401038e-05, 'epoch': 0.6122448979591837}\n",
            "\n",
            ">>> Step 451 metrics: {'loss': 0.6417, 'grad_norm': 5.375, 'learning_rate': 4.9803403364835196e-05, 'epoch': 0.6136054421768707}\n",
            "\n",
            ">>> Step 452 metrics: {'loss': 0.8875, 'grad_norm': 6.125, 'learning_rate': 4.9802509908027737e-05, 'epoch': 0.6149659863945578}\n",
            "\n",
            ">>> Step 453 metrics: {'loss': 1.0357, 'grad_norm': 7.09375, 'learning_rate': 4.9801614433660695e-05, 'epoch': 0.6163265306122448}\n",
            "\n",
            ">>> Step 454 metrics: {'loss': 1.5913, 'grad_norm': 8.125, 'learning_rate': 4.9800716941806904e-05, 'epoch': 0.617687074829932}\n",
            "\n",
            ">>> Step 455 metrics: {'loss': 1.4518, 'grad_norm': 7.1875, 'learning_rate': 4.9799817432539364e-05, 'epoch': 0.6190476190476191}\n",
            "\n",
            ">>> Step 456 metrics: {'loss': 1.1579, 'grad_norm': 7.96875, 'learning_rate': 4.979891590593125e-05, 'epoch': 0.6204081632653061}\n",
            "\n",
            ">>> Step 457 metrics: {'loss': 1.2934, 'grad_norm': 6.78125, 'learning_rate': 4.9798012362055905e-05, 'epoch': 0.6217687074829932}\n",
            "\n",
            ">>> Step 458 metrics: {'loss': 1.0699, 'grad_norm': 6.40625, 'learning_rate': 4.979710680098681e-05, 'epoch': 0.6231292517006802}\n",
            "\n",
            ">>> Step 459 metrics: {'loss': 0.63, 'grad_norm': 5.78125, 'learning_rate': 4.9796199222797644e-05, 'epoch': 0.6244897959183674}\n",
            "\n",
            ">>> Step 460 metrics: {'loss': 1.2344, 'grad_norm': 6.71875, 'learning_rate': 4.979528962756222e-05, 'epoch': 0.6258503401360545}\n",
            "\n",
            ">>> Step 461 metrics: {'loss': 0.9988, 'grad_norm': 6.90625, 'learning_rate': 4.979437801535455e-05, 'epoch': 0.6272108843537415}\n",
            "\n",
            ">>> Step 462 metrics: {'loss': 0.7659, 'grad_norm': 5.9375, 'learning_rate': 4.979346438624877e-05, 'epoch': 0.6285714285714286}\n",
            "\n",
            ">>> Step 463 metrics: {'loss': 1.1058, 'grad_norm': 6.375, 'learning_rate': 4.979254874031921e-05, 'epoch': 0.6299319727891156}\n",
            "\n",
            ">>> Step 464 metrics: {'loss': 1.3077, 'grad_norm': 6.84375, 'learning_rate': 4.9791631077640345e-05, 'epoch': 0.6312925170068027}\n",
            "\n",
            ">>> Step 465 metrics: {'loss': 0.8877, 'grad_norm': 6.28125, 'learning_rate': 4.9790711398286826e-05, 'epoch': 0.6326530612244898}\n",
            "\n",
            ">>> Step 466 metrics: {'loss': 0.8767, 'grad_norm': 6.5, 'learning_rate': 4.978978970233347e-05, 'epoch': 0.6340136054421769}\n",
            "\n",
            ">>> Step 467 metrics: {'loss': 1.3235, 'grad_norm': 10.125, 'learning_rate': 4.9788865989855236e-05, 'epoch': 0.6353741496598639}\n",
            "\n",
            ">>> Step 468 metrics: {'loss': 0.4865, 'grad_norm': 6.15625, 'learning_rate': 4.978794026092728e-05, 'epoch': 0.636734693877551}\n",
            "\n",
            ">>> Step 469 metrics: {'loss': 0.6227, 'grad_norm': 6.1875, 'learning_rate': 4.97870125156249e-05, 'epoch': 0.638095238095238}\n",
            "\n",
            ">>> Step 470 metrics: {'loss': 0.958, 'grad_norm': 6.09375, 'learning_rate': 4.978608275402356e-05, 'epoch': 0.6394557823129252}\n",
            "\n",
            ">>> Step 471 metrics: {'loss': 0.8704, 'grad_norm': 6.28125, 'learning_rate': 4.9785150976198894e-05, 'epoch': 0.6408163265306123}\n",
            "\n",
            ">>> Step 472 metrics: {'loss': 0.9605, 'grad_norm': 6.28125, 'learning_rate': 4.9784217182226694e-05, 'epoch': 0.6421768707482993}\n",
            "\n",
            ">>> Step 473 metrics: {'loss': 0.7563, 'grad_norm': 6.1875, 'learning_rate': 4.978328137218293e-05, 'epoch': 0.6435374149659864}\n",
            "\n",
            ">>> Step 474 metrics: {'loss': 0.7639, 'grad_norm': 6.25, 'learning_rate': 4.9782343546143714e-05, 'epoch': 0.6448979591836734}\n",
            "\n",
            ">>> Step 475 metrics: {'loss': 0.8379, 'grad_norm': 6.34375, 'learning_rate': 4.978140370418534e-05, 'epoch': 0.6462585034013606}\n",
            "\n",
            ">>> Step 476 metrics: {'loss': 1.3019, 'grad_norm': 8.5625, 'learning_rate': 4.978046184638426e-05, 'epoch': 0.6476190476190476}\n",
            "\n",
            ">>> Step 477 metrics: {'loss': 0.8261, 'grad_norm': 6.3125, 'learning_rate': 4.977951797281708e-05, 'epoch': 0.6489795918367347}\n",
            "\n",
            ">>> Step 478 metrics: {'loss': 1.0123, 'grad_norm': 6.78125, 'learning_rate': 4.9778572083560595e-05, 'epoch': 0.6503401360544218}\n",
            "\n",
            ">>> Step 479 metrics: {'loss': 0.3991, 'grad_norm': 5.28125, 'learning_rate': 4.9777624178691726e-05, 'epoch': 0.6517006802721088}\n",
            "\n",
            ">>> Step 480 metrics: {'loss': 0.7167, 'grad_norm': 5.90625, 'learning_rate': 4.977667425828761e-05, 'epoch': 0.6530612244897959}\n",
            "\n",
            ">>> Step 481 metrics: {'loss': 0.6392, 'grad_norm': 6.0, 'learning_rate': 4.977572232242549e-05, 'epoch': 0.654421768707483}\n",
            "\n",
            ">>> Step 482 metrics: {'loss': 1.723, 'grad_norm': 8.0625, 'learning_rate': 4.977476837118282e-05, 'epoch': 0.6557823129251701}\n",
            "\n",
            ">>> Step 483 metrics: {'loss': 1.2441, 'grad_norm': 7.96875, 'learning_rate': 4.977381240463719e-05, 'epoch': 0.6571428571428571}\n",
            "\n",
            ">>> Step 484 metrics: {'loss': 0.8397, 'grad_norm': 6.28125, 'learning_rate': 4.9772854422866366e-05, 'epoch': 0.6585034013605442}\n",
            "\n",
            ">>> Step 485 metrics: {'loss': 0.6112, 'grad_norm': 5.84375, 'learning_rate': 4.977189442594827e-05, 'epoch': 0.6598639455782312}\n",
            "\n",
            ">>> Step 486 metrics: {'loss': 0.4219, 'grad_norm': 13.625, 'learning_rate': 4.9770932413961e-05, 'epoch': 0.6612244897959184}\n",
            "\n",
            ">>> Step 487 metrics: {'loss': 1.2014, 'grad_norm': 6.90625, 'learning_rate': 4.976996838698281e-05, 'epoch': 0.6625850340136055}\n",
            "\n",
            ">>> Step 488 metrics: {'loss': 0.7493, 'grad_norm': 6.0, 'learning_rate': 4.976900234509212e-05, 'epoch': 0.6639455782312925}\n",
            "\n",
            ">>> Step 489 metrics: {'loss': 0.656, 'grad_norm': 5.3125, 'learning_rate': 4.97680342883675e-05, 'epoch': 0.6653061224489796}\n",
            "\n",
            ">>> Step 490 metrics: {'loss': 1.0543, 'grad_norm': 6.3125, 'learning_rate': 4.9767064216887714e-05, 'epoch': 0.6666666666666666}\n",
            "\n",
            ">>> Step 491 metrics: {'loss': 0.6904, 'grad_norm': 5.96875, 'learning_rate': 4.976609213073167e-05, 'epoch': 0.6680272108843538}\n",
            "\n",
            ">>> Step 492 metrics: {'loss': 0.7972, 'grad_norm': 7.65625, 'learning_rate': 4.9765118029978425e-05, 'epoch': 0.6693877551020408}\n",
            "\n",
            ">>> Step 493 metrics: {'loss': 1.1758, 'grad_norm': 6.125, 'learning_rate': 4.976414191470723e-05, 'epoch': 0.6707482993197279}\n",
            "\n",
            ">>> Step 494 metrics: {'loss': 0.8815, 'grad_norm': 6.78125, 'learning_rate': 4.976316378499749e-05, 'epoch': 0.672108843537415}\n",
            "\n",
            ">>> Step 495 metrics: {'loss': 0.559, 'grad_norm': 5.8125, 'learning_rate': 4.9762183640928773e-05, 'epoch': 0.673469387755102}\n",
            "\n",
            ">>> Step 496 metrics: {'loss': 1.0104, 'grad_norm': 6.53125, 'learning_rate': 4.97612014825808e-05, 'epoch': 0.6748299319727891}\n",
            "\n",
            ">>> Step 497 metrics: {'loss': 0.8049, 'grad_norm': 6.59375, 'learning_rate': 4.9760217310033464e-05, 'epoch': 0.6761904761904762}\n",
            "\n",
            ">>> Step 498 metrics: {'loss': 0.6734, 'grad_norm': 6.34375, 'learning_rate': 4.9759231123366826e-05, 'epoch': 0.6775510204081633}\n",
            "\n",
            ">>> Step 499 metrics: {'loss': 0.9007, 'grad_norm': 5.84375, 'learning_rate': 4.97582429226611e-05, 'epoch': 0.6789115646258503}\n",
            "\n",
            ">>> Step 500 metrics: {'loss': 0.8031, 'grad_norm': 6.6875, 'learning_rate': 4.975725270799669e-05, 'epoch': 0.6802721088435374}\n",
            "\n",
            ">>> Step 501 metrics: {'loss': 0.9288, 'grad_norm': 5.9375, 'learning_rate': 4.975626047945413e-05, 'epoch': 0.6816326530612244}\n",
            "\n",
            ">>> Step 502 metrics: {'loss': 1.1165, 'grad_norm': 6.375, 'learning_rate': 4.975526623711414e-05, 'epoch': 0.6829931972789116}\n",
            "\n",
            ">>> Step 503 metrics: {'loss': 0.6025, 'grad_norm': 7.53125, 'learning_rate': 4.975426998105759e-05, 'epoch': 0.6843537414965987}\n",
            "\n",
            ">>> Step 504 metrics: {'loss': 1.2893, 'grad_norm': 7.25, 'learning_rate': 4.975327171136552e-05, 'epoch': 0.6857142857142857}\n",
            "\n",
            ">>> Step 505 metrics: {'loss': 1.2993, 'grad_norm': 8.125, 'learning_rate': 4.975227142811914e-05, 'epoch': 0.6870748299319728}\n",
            "\n",
            ">>> Step 506 metrics: {'loss': 1.0261, 'grad_norm': 7.3125, 'learning_rate': 4.975126913139982e-05, 'epoch': 0.6884353741496598}\n",
            "\n",
            ">>> Step 507 metrics: {'loss': 0.5836, 'grad_norm': 6.78125, 'learning_rate': 4.975026482128908e-05, 'epoch': 0.689795918367347}\n",
            "\n",
            ">>> Step 508 metrics: {'loss': 0.7443, 'grad_norm': 6.84375, 'learning_rate': 4.9749258497868626e-05, 'epoch': 0.691156462585034}\n",
            "\n",
            ">>> Step 509 metrics: {'loss': 1.0198, 'grad_norm': 7.15625, 'learning_rate': 4.974825016122032e-05, 'epoch': 0.6925170068027211}\n",
            "\n",
            ">>> Step 510 metrics: {'loss': 1.4065, 'grad_norm': 6.96875, 'learning_rate': 4.974723981142617e-05, 'epoch': 0.6938775510204082}\n",
            "\n",
            ">>> Step 511 metrics: {'loss': 0.6896, 'grad_norm': 5.75, 'learning_rate': 4.974622744856838e-05, 'epoch': 0.6952380952380952}\n",
            "\n",
            ">>> Step 512 metrics: {'loss': 0.9832, 'grad_norm': 6.6875, 'learning_rate': 4.974521307272929e-05, 'epoch': 0.6965986394557823}\n",
            "\n",
            ">>> Step 513 metrics: {'loss': 0.9116, 'grad_norm': 5.84375, 'learning_rate': 4.974419668399142e-05, 'epoch': 0.6979591836734694}\n",
            "\n",
            ">>> Step 514 metrics: {'loss': 1.1307, 'grad_norm': 6.8125, 'learning_rate': 4.974317828243744e-05, 'epoch': 0.6993197278911565}\n",
            "\n",
            ">>> Step 515 metrics: {'loss': 1.007, 'grad_norm': 7.03125, 'learning_rate': 4.97421578681502e-05, 'epoch': 0.7006802721088435}\n",
            "\n",
            ">>> Step 516 metrics: {'loss': 1.0325, 'grad_norm': 6.5, 'learning_rate': 4.9741135441212706e-05, 'epoch': 0.7020408163265306}\n",
            "\n",
            ">>> Step 517 metrics: {'loss': 1.4069, 'grad_norm': 7.375, 'learning_rate': 4.9740111001708125e-05, 'epoch': 0.7034013605442176}\n",
            "\n",
            ">>> Step 518 metrics: {'loss': 1.0045, 'grad_norm': 6.1875, 'learning_rate': 4.9739084549719784e-05, 'epoch': 0.7047619047619048}\n",
            "\n",
            ">>> Step 519 metrics: {'loss': 1.325, 'grad_norm': 8.375, 'learning_rate': 4.973805608533119e-05, 'epoch': 0.7061224489795919}\n",
            "\n",
            ">>> Step 520 metrics: {'loss': 1.0818, 'grad_norm': 6.96875, 'learning_rate': 4.973702560862599e-05, 'epoch': 0.7074829931972789}\n",
            "\n",
            ">>> Step 521 metrics: {'loss': 0.7808, 'grad_norm': 6.5, 'learning_rate': 4.973599311968802e-05, 'epoch': 0.708843537414966}\n",
            "\n",
            ">>> Step 522 metrics: {'loss': 1.2826, 'grad_norm': 7.34375, 'learning_rate': 4.973495861860127e-05, 'epoch': 0.710204081632653}\n",
            "\n",
            ">>> Step 523 metrics: {'loss': 0.7759, 'grad_norm': 6.3125, 'learning_rate': 4.9733922105449885e-05, 'epoch': 0.7115646258503401}\n",
            "\n",
            ">>> Step 524 metrics: {'loss': 1.0582, 'grad_norm': 6.59375, 'learning_rate': 4.9732883580318176e-05, 'epoch': 0.7129251700680272}\n",
            "\n",
            ">>> Step 525 metrics: {'loss': 0.7399, 'grad_norm': 6.53125, 'learning_rate': 4.973184304329063e-05, 'epoch': 0.7142857142857143}\n",
            "\n",
            ">>> Step 526 metrics: {'loss': 1.318, 'grad_norm': 7.15625, 'learning_rate': 4.973080049445188e-05, 'epoch': 0.7156462585034014}\n",
            "\n",
            ">>> Step 527 metrics: {'loss': 0.7138, 'grad_norm': 6.03125, 'learning_rate': 4.9729755933886747e-05, 'epoch': 0.7170068027210884}\n",
            "\n",
            ">>> Step 528 metrics: {'loss': 0.6068, 'grad_norm': 6.1875, 'learning_rate': 4.9728709361680184e-05, 'epoch': 0.7183673469387755}\n",
            "\n",
            ">>> Step 529 metrics: {'loss': 0.9527, 'grad_norm': 7.5, 'learning_rate': 4.972766077791734e-05, 'epoch': 0.7197278911564626}\n",
            "\n",
            ">>> Step 530 metrics: {'loss': 0.7006, 'grad_norm': 5.78125, 'learning_rate': 4.97266101826835e-05, 'epoch': 0.7210884353741497}\n",
            "\n",
            ">>> Step 531 metrics: {'loss': 0.8423, 'grad_norm': 6.5625, 'learning_rate': 4.972555757606413e-05, 'epoch': 0.7224489795918367}\n",
            "\n",
            ">>> Step 532 metrics: {'loss': 0.8958, 'grad_norm': 6.96875, 'learning_rate': 4.9724502958144846e-05, 'epoch': 0.7238095238095238}\n",
            "\n",
            ">>> Step 533 metrics: {'loss': 1.3569, 'grad_norm': 6.5625, 'learning_rate': 4.972344632901146e-05, 'epoch': 0.7251700680272108}\n",
            "\n",
            ">>> Step 534 metrics: {'loss': 0.9702, 'grad_norm': 7.125, 'learning_rate': 4.9722387688749894e-05, 'epoch': 0.726530612244898}\n",
            "\n",
            ">>> Step 535 metrics: {'loss': 0.8353, 'grad_norm': 7.21875, 'learning_rate': 4.972132703744628e-05, 'epoch': 0.7278911564625851}\n",
            "\n",
            ">>> Step 536 metrics: {'loss': 0.5929, 'grad_norm': 6.0625, 'learning_rate': 4.972026437518689e-05, 'epoch': 0.7292517006802721}\n",
            "\n",
            ">>> Step 537 metrics: {'loss': 1.1752, 'grad_norm': 6.59375, 'learning_rate': 4.971919970205817e-05, 'epoch': 0.7306122448979592}\n",
            "\n",
            ">>> Step 538 metrics: {'loss': 1.0673, 'grad_norm': 7.15625, 'learning_rate': 4.9718133018146716e-05, 'epoch': 0.7319727891156462}\n",
            "\n",
            ">>> Step 539 metrics: {'loss': 0.8442, 'grad_norm': 6.5, 'learning_rate': 4.971706432353932e-05, 'epoch': 0.7333333333333333}\n",
            "\n",
            ">>> Step 540 metrics: {'loss': 1.2823, 'grad_norm': 9.0, 'learning_rate': 4.9715993618322895e-05, 'epoch': 0.7346938775510204}\n",
            "\n",
            ">>> Step 541 metrics: {'loss': 0.9164, 'grad_norm': 6.59375, 'learning_rate': 4.971492090258454e-05, 'epoch': 0.7360544217687075}\n",
            "\n",
            ">>> Step 542 metrics: {'loss': 0.9689, 'grad_norm': 7.03125, 'learning_rate': 4.971384617641153e-05, 'epoch': 0.7374149659863946}\n",
            "\n",
            ">>> Step 543 metrics: {'loss': 1.2365, 'grad_norm': 6.125, 'learning_rate': 4.971276943989126e-05, 'epoch': 0.7387755102040816}\n",
            "\n",
            ">>> Step 544 metrics: {'loss': 0.9454, 'grad_norm': 6.40625, 'learning_rate': 4.9711690693111346e-05, 'epoch': 0.7401360544217687}\n",
            "\n",
            ">>> Step 545 metrics: {'loss': 0.8451, 'grad_norm': 6.40625, 'learning_rate': 4.9710609936159525e-05, 'epoch': 0.7414965986394558}\n",
            "\n",
            ">>> Step 546 metrics: {'loss': 0.8834, 'grad_norm': 6.40625, 'learning_rate': 4.9709527169123704e-05, 'epoch': 0.7428571428571429}\n",
            "\n",
            ">>> Step 547 metrics: {'loss': 1.0737, 'grad_norm': 6.59375, 'learning_rate': 4.970844239209198e-05, 'epoch': 0.7442176870748299}\n",
            "\n",
            ">>> Step 548 metrics: {'loss': 0.834, 'grad_norm': 5.9375, 'learning_rate': 4.970735560515258e-05, 'epoch': 0.745578231292517}\n",
            "\n",
            ">>> Step 549 metrics: {'loss': 0.852, 'grad_norm': 6.0, 'learning_rate': 4.970626680839391e-05, 'epoch': 0.746938775510204}\n",
            "\n",
            ">>> Step 550 metrics: {'loss': 0.752, 'grad_norm': 6.09375, 'learning_rate': 4.9705176001904544e-05, 'epoch': 0.7482993197278912}\n",
            "\n",
            ">>> Step 551 metrics: {'loss': 0.8353, 'grad_norm': 10.0625, 'learning_rate': 4.970408318577321e-05, 'epoch': 0.7496598639455783}\n",
            "\n",
            ">>> Step 552 metrics: {'loss': 1.0023, 'grad_norm': 5.8125, 'learning_rate': 4.97029883600888e-05, 'epoch': 0.7510204081632653}\n",
            "\n",
            ">>> Step 553 metrics: {'loss': 0.9925, 'grad_norm': 6.875, 'learning_rate': 4.9701891524940376e-05, 'epoch': 0.7523809523809524}\n",
            "\n",
            ">>> Step 554 metrics: {'loss': 1.0236, 'grad_norm': 7.8125, 'learning_rate': 4.9700792680417164e-05, 'epoch': 0.7537414965986394}\n",
            "\n",
            ">>> Step 555 metrics: {'loss': 1.0008, 'grad_norm': 7.25, 'learning_rate': 4.969969182660854e-05, 'epoch': 0.7551020408163265}\n",
            "\n",
            ">>> Step 556 metrics: {'loss': 1.5138, 'grad_norm': 7.71875, 'learning_rate': 4.969858896360406e-05, 'epoch': 0.7564625850340136}\n",
            "\n",
            ">>> Step 557 metrics: {'loss': 0.7686, 'grad_norm': 6.15625, 'learning_rate': 4.9697484091493435e-05, 'epoch': 0.7578231292517007}\n",
            "\n",
            ">>> Step 558 metrics: {'loss': 1.0459, 'grad_norm': 7.1875, 'learning_rate': 4.969637721036654e-05, 'epoch': 0.7591836734693878}\n",
            "\n",
            ">>> Step 559 metrics: {'loss': 1.3743, 'grad_norm': 7.75, 'learning_rate': 4.9695268320313414e-05, 'epoch': 0.7605442176870748}\n",
            "\n",
            ">>> Step 560 metrics: {'loss': 0.8512, 'grad_norm': 5.8125, 'learning_rate': 4.969415742142426e-05, 'epoch': 0.7619047619047619}\n",
            "\n",
            ">>> Step 561 metrics: {'loss': 0.8914, 'grad_norm': 6.25, 'learning_rate': 4.9693044513789444e-05, 'epoch': 0.763265306122449}\n",
            "\n",
            ">>> Step 562 metrics: {'loss': 0.9222, 'grad_norm': 6.625, 'learning_rate': 4.96919295974995e-05, 'epoch': 0.7646258503401361}\n",
            "\n",
            ">>> Step 563 metrics: {'loss': 1.1365, 'grad_norm': 7.03125, 'learning_rate': 4.969081267264511e-05, 'epoch': 0.7659863945578231}\n",
            "\n",
            ">>> Step 564 metrics: {'loss': 1.0122, 'grad_norm': 6.21875, 'learning_rate': 4.968969373931714e-05, 'epoch': 0.7673469387755102}\n",
            "\n",
            ">>> Step 565 metrics: {'loss': 0.9962, 'grad_norm': 6.0, 'learning_rate': 4.9688572797606604e-05, 'epoch': 0.7687074829931972}\n",
            "\n",
            ">>> Step 566 metrics: {'loss': 1.3814, 'grad_norm': 8.5, 'learning_rate': 4.9687449847604697e-05, 'epoch': 0.7700680272108843}\n",
            "\n",
            ">>> Step 567 metrics: {'loss': 1.3021, 'grad_norm': 6.9375, 'learning_rate': 4.968632488940274e-05, 'epoch': 0.7714285714285715}\n",
            "\n",
            ">>> Step 568 metrics: {'loss': 1.5982, 'grad_norm': 6.6875, 'learning_rate': 4.9685197923092266e-05, 'epoch': 0.7727891156462585}\n",
            "\n",
            ">>> Step 569 metrics: {'loss': 0.9696, 'grad_norm': 6.5, 'learning_rate': 4.968406894876494e-05, 'epoch': 0.7741496598639456}\n",
            "\n",
            ">>> Step 570 metrics: {'loss': 0.6521, 'grad_norm': 6.46875, 'learning_rate': 4.96829379665126e-05, 'epoch': 0.7755102040816326}\n",
            "\n",
            ">>> Step 571 metrics: {'loss': 1.486, 'grad_norm': 6.8125, 'learning_rate': 4.9681804976427245e-05, 'epoch': 0.7768707482993197}\n",
            "\n",
            ">>> Step 572 metrics: {'loss': 0.9888, 'grad_norm': 6.4375, 'learning_rate': 4.968066997860103e-05, 'epoch': 0.7782312925170068}\n",
            "\n",
            ">>> Step 573 metrics: {'loss': 1.0431, 'grad_norm': 6.875, 'learning_rate': 4.9679532973126295e-05, 'epoch': 0.7795918367346939}\n",
            "\n",
            ">>> Step 574 metrics: {'loss': 1.1439, 'grad_norm': 6.5, 'learning_rate': 4.9678393960095525e-05, 'epoch': 0.780952380952381}\n",
            "\n",
            ">>> Step 575 metrics: {'loss': 0.9787, 'grad_norm': 8.3125, 'learning_rate': 4.967725293960138e-05, 'epoch': 0.782312925170068}\n",
            "\n",
            ">>> Step 576 metrics: {'loss': 0.7315, 'grad_norm': 5.71875, 'learning_rate': 4.967610991173666e-05, 'epoch': 0.7836734693877551}\n",
            "\n",
            ">>> Step 577 metrics: {'loss': 0.8845, 'grad_norm': 7.4375, 'learning_rate': 4.9674964876594346e-05, 'epoch': 0.7850340136054422}\n",
            "\n",
            ">>> Step 578 metrics: {'loss': 1.0508, 'grad_norm': 6.75, 'learning_rate': 4.967381783426759e-05, 'epoch': 0.7863945578231293}\n",
            "\n",
            ">>> Step 579 metrics: {'loss': 1.4414, 'grad_norm': 8.5, 'learning_rate': 4.96726687848497e-05, 'epoch': 0.7877551020408163}\n",
            "\n",
            ">>> Step 580 metrics: {'loss': 0.8565, 'grad_norm': 6.6875, 'learning_rate': 4.967151772843414e-05, 'epoch': 0.7891156462585034}\n",
            "\n",
            ">>> Step 581 metrics: {'loss': 1.3692, 'grad_norm': 7.84375, 'learning_rate': 4.9670364665114535e-05, 'epoch': 0.7904761904761904}\n",
            "\n",
            ">>> Step 582 metrics: {'loss': 1.0177, 'grad_norm': 7.375, 'learning_rate': 4.96692095949847e-05, 'epoch': 0.7918367346938775}\n",
            "\n",
            ">>> Step 583 metrics: {'loss': 0.9625, 'grad_norm': 6.1875, 'learning_rate': 4.966805251813857e-05, 'epoch': 0.7931972789115647}\n",
            "\n",
            ">>> Step 584 metrics: {'loss': 1.211, 'grad_norm': 7.6875, 'learning_rate': 4.96668934346703e-05, 'epoch': 0.7945578231292517}\n",
            "\n",
            ">>> Step 585 metrics: {'loss': 1.3252, 'grad_norm': 7.25, 'learning_rate': 4.966573234467414e-05, 'epoch': 0.7959183673469388}\n",
            "\n",
            ">>> Step 586 metrics: {'loss': 1.0541, 'grad_norm': 6.8125, 'learning_rate': 4.966456924824457e-05, 'epoch': 0.7972789115646258}\n",
            "\n",
            ">>> Step 587 metrics: {'loss': 1.4297, 'grad_norm': 9.6875, 'learning_rate': 4.9663404145476174e-05, 'epoch': 0.7986394557823129}\n",
            "\n",
            ">>> Step 588 metrics: {'loss': 1.1032, 'grad_norm': 9.25, 'learning_rate': 4.9662237036463753e-05, 'epoch': 0.8}\n",
            "\n",
            ">>> Step 589 metrics: {'loss': 1.4318, 'grad_norm': 7.59375, 'learning_rate': 4.966106792130223e-05, 'epoch': 0.8013605442176871}\n",
            "\n",
            ">>> Step 590 metrics: {'loss': 1.2253, 'grad_norm': 6.75, 'learning_rate': 4.96598968000867e-05, 'epoch': 0.8027210884353742}\n",
            "\n",
            ">>> Step 591 metrics: {'loss': 0.8231, 'grad_norm': 7.90625, 'learning_rate': 4.9658723672912446e-05, 'epoch': 0.8040816326530612}\n",
            "\n",
            ">>> Step 592 metrics: {'loss': 0.8822, 'grad_norm': 6.5625, 'learning_rate': 4.965754853987489e-05, 'epoch': 0.8054421768707483}\n",
            "\n",
            ">>> Step 593 metrics: {'loss': 1.0939, 'grad_norm': 6.84375, 'learning_rate': 4.9656371401069614e-05, 'epoch': 0.8068027210884354}\n",
            "\n",
            ">>> Step 594 metrics: {'loss': 1.1376, 'grad_norm': 6.75, 'learning_rate': 4.965519225659238e-05, 'epoch': 0.8081632653061225}\n",
            "\n",
            ">>> Step 595 metrics: {'loss': 1.3755, 'grad_norm': 7.71875, 'learning_rate': 4.965401110653911e-05, 'epoch': 0.8095238095238095}\n",
            "\n",
            ">>> Step 596 metrics: {'loss': 0.69, 'grad_norm': 6.71875, 'learning_rate': 4.9652827951005876e-05, 'epoch': 0.8108843537414966}\n",
            "\n",
            ">>> Step 597 metrics: {'loss': 0.7154, 'grad_norm': 6.34375, 'learning_rate': 4.965164279008892e-05, 'epoch': 0.8122448979591836}\n",
            "\n",
            ">>> Step 598 metrics: {'loss': 1.4018, 'grad_norm': 7.9375, 'learning_rate': 4.9650455623884664e-05, 'epoch': 0.8136054421768707}\n",
            "\n",
            ">>> Step 599 metrics: {'loss': 1.0341, 'grad_norm': 5.90625, 'learning_rate': 4.964926645248966e-05, 'epoch': 0.8149659863945579}\n",
            "\n",
            ">>> Step 600 metrics: {'loss': 0.8945, 'grad_norm': 6.3125, 'learning_rate': 4.964807527600065e-05, 'epoch': 0.8163265306122449}\n",
            "\n",
            ">>> Step 601 metrics: {'loss': 1.0941, 'grad_norm': 6.625, 'learning_rate': 4.964688209451453e-05, 'epoch': 0.817687074829932}\n",
            "\n",
            ">>> Step 602 metrics: {'loss': 1.5657, 'grad_norm': 7.65625, 'learning_rate': 4.964568690812836e-05, 'epoch': 0.819047619047619}\n",
            "\n",
            ">>> Step 603 metrics: {'loss': 0.717, 'grad_norm': 6.96875, 'learning_rate': 4.964448971693936e-05, 'epoch': 0.8204081632653061}\n",
            "\n",
            ">>> Step 604 metrics: {'loss': 0.9588, 'grad_norm': 6.59375, 'learning_rate': 4.964329052104492e-05, 'epoch': 0.8217687074829932}\n",
            "\n",
            ">>> Step 605 metrics: {'loss': 0.7127, 'grad_norm': 7.96875, 'learning_rate': 4.964208932054258e-05, 'epoch': 0.8231292517006803}\n",
            "\n",
            ">>> Step 606 metrics: {'loss': 0.7931, 'grad_norm': 5.875, 'learning_rate': 4.964088611553006e-05, 'epoch': 0.8244897959183674}\n",
            "\n",
            ">>> Step 607 metrics: {'loss': 1.0965, 'grad_norm': 7.09375, 'learning_rate': 4.9639680906105225e-05, 'epoch': 0.8258503401360544}\n",
            "\n",
            ">>> Step 608 metrics: {'loss': 1.1034, 'grad_norm': 6.5, 'learning_rate': 4.963847369236613e-05, 'epoch': 0.8272108843537415}\n",
            "\n",
            ">>> Step 609 metrics: {'loss': 1.383, 'grad_norm': 8.0, 'learning_rate': 4.963726447441096e-05, 'epoch': 0.8285714285714286}\n",
            "\n",
            ">>> Step 610 metrics: {'loss': 0.5309, 'grad_norm': 5.8125, 'learning_rate': 4.9636053252338076e-05, 'epoch': 0.8299319727891157}\n",
            "\n",
            ">>> Step 611 metrics: {'loss': 0.708, 'grad_norm': 5.75, 'learning_rate': 4.963484002624602e-05, 'epoch': 0.8312925170068027}\n",
            "\n",
            ">>> Step 612 metrics: {'loss': 1.4263, 'grad_norm': 7.46875, 'learning_rate': 4.963362479623347e-05, 'epoch': 0.8326530612244898}\n",
            "\n",
            ">>> Step 613 metrics: {'loss': 0.5851, 'grad_norm': 5.03125, 'learning_rate': 4.9632407562399284e-05, 'epoch': 0.8340136054421768}\n",
            "\n",
            ">>> Step 614 metrics: {'loss': 1.2261, 'grad_norm': 6.4375, 'learning_rate': 4.963118832484248e-05, 'epoch': 0.8353741496598639}\n",
            "\n",
            ">>> Step 615 metrics: {'loss': 0.8328, 'grad_norm': 5.84375, 'learning_rate': 4.962996708366224e-05, 'epoch': 0.8367346938775511}\n",
            "\n",
            ">>> Step 616 metrics: {'loss': 0.9592, 'grad_norm': 6.34375, 'learning_rate': 4.962874383895789e-05, 'epoch': 0.8380952380952381}\n",
            "\n",
            ">>> Step 617 metrics: {'loss': 0.6934, 'grad_norm': 6.5, 'learning_rate': 4.962751859082895e-05, 'epoch': 0.8394557823129252}\n",
            "\n",
            ">>> Step 618 metrics: {'loss': 0.8365, 'grad_norm': 6.6875, 'learning_rate': 4.962629133937507e-05, 'epoch': 0.8408163265306122}\n",
            "\n",
            ">>> Step 619 metrics: {'loss': 0.5916, 'grad_norm': 5.65625, 'learning_rate': 4.962506208469611e-05, 'epoch': 0.8421768707482993}\n",
            "\n",
            ">>> Step 620 metrics: {'loss': 1.1144, 'grad_norm': 6.90625, 'learning_rate': 4.9623830826892035e-05, 'epoch': 0.8435374149659864}\n",
            "\n",
            ">>> Step 621 metrics: {'loss': 0.939, 'grad_norm': 6.46875, 'learning_rate': 4.9622597566063025e-05, 'epoch': 0.8448979591836735}\n",
            "\n",
            ">>> Step 622 metrics: {'loss': 0.9011, 'grad_norm': 6.46875, 'learning_rate': 4.9621362302309374e-05, 'epoch': 0.8462585034013606}\n",
            "\n",
            ">>> Step 623 metrics: {'loss': 1.6827, 'grad_norm': 8.0, 'learning_rate': 4.962012503573159e-05, 'epoch': 0.8476190476190476}\n",
            "\n",
            ">>> Step 624 metrics: {'loss': 1.3409, 'grad_norm': 7.3125, 'learning_rate': 4.96188857664303e-05, 'epoch': 0.8489795918367347}\n",
            "\n",
            ">>> Step 625 metrics: {'loss': 1.027, 'grad_norm': 6.25, 'learning_rate': 4.961764449450632e-05, 'epoch': 0.8503401360544217}\n",
            "\n",
            ">>> Step 626 metrics: {'loss': 0.7024, 'grad_norm': 6.40625, 'learning_rate': 4.961640122006063e-05, 'epoch': 0.8517006802721089}\n",
            "\n",
            ">>> Step 627 metrics: {'loss': 1.1744, 'grad_norm': 7.15625, 'learning_rate': 4.9615155943194344e-05, 'epoch': 0.8530612244897959}\n",
            "\n",
            ">>> Step 628 metrics: {'loss': 0.7413, 'grad_norm': 6.34375, 'learning_rate': 4.961390866400877e-05, 'epoch': 0.854421768707483}\n",
            "\n",
            ">>> Step 629 metrics: {'loss': 0.9171, 'grad_norm': 7.1875, 'learning_rate': 4.961265938260537e-05, 'epoch': 0.85578231292517}\n",
            "\n",
            ">>> Step 630 metrics: {'loss': 0.621, 'grad_norm': 5.71875, 'learning_rate': 4.9611408099085764e-05, 'epoch': 0.8571428571428571}\n",
            "\n",
            ">>> Step 631 metrics: {'loss': 0.7837, 'grad_norm': 6.0625, 'learning_rate': 4.9610154813551736e-05, 'epoch': 0.8585034013605443}\n",
            "\n",
            ">>> Step 632 metrics: {'loss': 1.1616, 'grad_norm': 6.5625, 'learning_rate': 4.960889952610524e-05, 'epoch': 0.8598639455782313}\n",
            "\n",
            ">>> Step 633 metrics: {'loss': 0.8909, 'grad_norm': 6.21875, 'learning_rate': 4.960764223684839e-05, 'epoch': 0.8612244897959184}\n",
            "\n",
            ">>> Step 634 metrics: {'loss': 0.8913, 'grad_norm': 6.34375, 'learning_rate': 4.960638294588343e-05, 'epoch': 0.8625850340136054}\n",
            "\n",
            ">>> Step 635 metrics: {'loss': 0.788, 'grad_norm': 6.15625, 'learning_rate': 4.960512165331284e-05, 'epoch': 0.8639455782312925}\n",
            "\n",
            ">>> Step 636 metrics: {'loss': 0.9478, 'grad_norm': 7.03125, 'learning_rate': 4.9603858359239195e-05, 'epoch': 0.8653061224489796}\n",
            "\n",
            ">>> Step 637 metrics: {'loss': 1.3592, 'grad_norm': 8.375, 'learning_rate': 4.960259306376527e-05, 'epoch': 0.8666666666666667}\n",
            "\n",
            ">>> Step 638 metrics: {'loss': 1.1155, 'grad_norm': 6.84375, 'learning_rate': 4.9601325766993976e-05, 'epoch': 0.8680272108843538}\n",
            "\n",
            ">>> Step 639 metrics: {'loss': 0.9225, 'grad_norm': 6.3125, 'learning_rate': 4.9600056469028414e-05, 'epoch': 0.8693877551020408}\n",
            "\n",
            ">>> Step 640 metrics: {'loss': 0.8824, 'grad_norm': 6.09375, 'learning_rate': 4.959878516997183e-05, 'epoch': 0.8707482993197279}\n",
            "\n",
            ">>> Step 641 metrics: {'loss': 1.1113, 'grad_norm': 6.21875, 'learning_rate': 4.9597511869927635e-05, 'epoch': 0.8721088435374149}\n",
            "\n",
            ">>> Step 642 metrics: {'loss': 0.8909, 'grad_norm': 6.375, 'learning_rate': 4.9596236568999413e-05, 'epoch': 0.8734693877551021}\n",
            "\n",
            ">>> Step 643 metrics: {'loss': 0.7229, 'grad_norm': 6.125, 'learning_rate': 4.959495926729089e-05, 'epoch': 0.8748299319727891}\n",
            "\n",
            ">>> Step 644 metrics: {'loss': 0.8807, 'grad_norm': 6.40625, 'learning_rate': 4.959367996490598e-05, 'epoch': 0.8761904761904762}\n",
            "\n",
            ">>> Step 645 metrics: {'loss': 0.7924, 'grad_norm': 5.65625, 'learning_rate': 4.9592398661948745e-05, 'epoch': 0.8775510204081632}\n",
            "\n",
            ">>> Step 646 metrics: {'loss': 1.3649, 'grad_norm': 6.0, 'learning_rate': 4.959111535852342e-05, 'epoch': 0.8789115646258503}\n",
            "\n",
            ">>> Step 647 metrics: {'loss': 0.9318, 'grad_norm': 6.5625, 'learning_rate': 4.9589830054734374e-05, 'epoch': 0.8802721088435375}\n",
            "\n",
            ">>> Step 648 metrics: {'loss': 0.7222, 'grad_norm': 8.875, 'learning_rate': 4.9588542750686185e-05, 'epoch': 0.8816326530612245}\n",
            "\n",
            ">>> Step 649 metrics: {'loss': 0.6683, 'grad_norm': 6.46875, 'learning_rate': 4.958725344648355e-05, 'epoch': 0.8829931972789116}\n",
            "\n",
            ">>> Step 650 metrics: {'loss': 0.9442, 'grad_norm': 6.25, 'learning_rate': 4.958596214223136e-05, 'epoch': 0.8843537414965986}\n",
            "\n",
            ">>> Step 651 metrics: {'loss': 0.8138, 'grad_norm': 6.6875, 'learning_rate': 4.9584668838034644e-05, 'epoch': 0.8857142857142857}\n",
            "\n",
            ">>> Step 652 metrics: {'loss': 1.1888, 'grad_norm': 6.5, 'learning_rate': 4.9583373533998616e-05, 'epoch': 0.8870748299319728}\n",
            "\n",
            ">>> Step 653 metrics: {'loss': 1.0973, 'grad_norm': 6.09375, 'learning_rate': 4.958207623022864e-05, 'epoch': 0.8884353741496599}\n",
            "\n",
            ">>> Step 654 metrics: {'loss': 1.2507, 'grad_norm': 7.3125, 'learning_rate': 4.958077692683024e-05, 'epoch': 0.889795918367347}\n",
            "\n",
            ">>> Step 655 metrics: {'loss': 1.0025, 'grad_norm': 6.4375, 'learning_rate': 4.9579475623909114e-05, 'epoch': 0.891156462585034}\n",
            "\n",
            ">>> Step 656 metrics: {'loss': 1.0629, 'grad_norm': 6.53125, 'learning_rate': 4.9578172321571114e-05, 'epoch': 0.8925170068027211}\n",
            "\n",
            ">>> Step 657 metrics: {'loss': 0.8253, 'grad_norm': 7.40625, 'learning_rate': 4.957686701992226e-05, 'epoch': 0.8938775510204081}\n",
            "\n",
            ">>> Step 658 metrics: {'loss': 1.4689, 'grad_norm': 10.1875, 'learning_rate': 4.9575559719068734e-05, 'epoch': 0.8952380952380953}\n",
            "\n",
            ">>> Step 659 metrics: {'loss': 0.7493, 'grad_norm': 6.71875, 'learning_rate': 4.957425041911686e-05, 'epoch': 0.8965986394557823}\n",
            "\n",
            ">>> Step 660 metrics: {'loss': 1.0595, 'grad_norm': 6.4375, 'learning_rate': 4.9572939120173166e-05, 'epoch': 0.8979591836734694}\n",
            "\n",
            ">>> Step 661 metrics: {'loss': 0.8764, 'grad_norm': 7.0625, 'learning_rate': 4.957162582234431e-05, 'epoch': 0.8993197278911564}\n",
            "\n",
            ">>> Step 662 metrics: {'loss': 1.0327, 'grad_norm': 6.0, 'learning_rate': 4.957031052573712e-05, 'epoch': 0.9006802721088435}\n",
            "\n",
            ">>> Step 663 metrics: {'loss': 1.2751, 'grad_norm': 7.0625, 'learning_rate': 4.95689932304586e-05, 'epoch': 0.9020408163265307}\n",
            "\n",
            ">>> Step 664 metrics: {'loss': 1.0046, 'grad_norm': 6.625, 'learning_rate': 4.956767393661588e-05, 'epoch': 0.9034013605442177}\n",
            "\n",
            ">>> Step 665 metrics: {'loss': 1.0294, 'grad_norm': 6.875, 'learning_rate': 4.95663526443163e-05, 'epoch': 0.9047619047619048}\n",
            "\n",
            ">>> Step 666 metrics: {'loss': 1.2262, 'grad_norm': 6.65625, 'learning_rate': 4.9565029353667335e-05, 'epoch': 0.9061224489795918}\n",
            "\n",
            ">>> Step 667 metrics: {'loss': 1.1986, 'grad_norm': 7.9375, 'learning_rate': 4.9563704064776627e-05, 'epoch': 0.9074829931972789}\n",
            "\n",
            ">>> Step 668 metrics: {'loss': 1.2737, 'grad_norm': 7.125, 'learning_rate': 4.956237677775199e-05, 'epoch': 0.908843537414966}\n",
            "\n",
            ">>> Step 669 metrics: {'loss': 0.8087, 'grad_norm': 6.15625, 'learning_rate': 4.9561047492701377e-05, 'epoch': 0.9102040816326531}\n",
            "\n",
            ">>> Step 670 metrics: {'loss': 0.8797, 'grad_norm': 6.0, 'learning_rate': 4.955971620973293e-05, 'epoch': 0.9115646258503401}\n",
            "\n",
            ">>> Step 671 metrics: {'loss': 0.8263, 'grad_norm': 6.09375, 'learning_rate': 4.955838292895494e-05, 'epoch': 0.9129251700680272}\n",
            "\n",
            ">>> Step 672 metrics: {'loss': 0.6637, 'grad_norm': 6.21875, 'learning_rate': 4.955704765047586e-05, 'epoch': 0.9142857142857143}\n",
            "\n",
            ">>> Step 673 metrics: {'loss': 0.8416, 'grad_norm': 6.3125, 'learning_rate': 4.955571037440431e-05, 'epoch': 0.9156462585034013}\n",
            "\n",
            ">>> Step 674 metrics: {'loss': 0.6073, 'grad_norm': 6.3125, 'learning_rate': 4.955437110084906e-05, 'epoch': 0.9170068027210885}\n",
            "\n",
            ">>> Step 675 metrics: {'loss': 0.922, 'grad_norm': 8.9375, 'learning_rate': 4.955302982991907e-05, 'epoch': 0.9183673469387755}\n",
            "\n",
            ">>> Step 676 metrics: {'loss': 1.0178, 'grad_norm': 6.8125, 'learning_rate': 4.955168656172344e-05, 'epoch': 0.9197278911564626}\n",
            "\n",
            ">>> Step 677 metrics: {'loss': 0.8228, 'grad_norm': 5.53125, 'learning_rate': 4.955034129637144e-05, 'epoch': 0.9210884353741496}\n",
            "\n",
            ">>> Step 678 metrics: {'loss': 0.5993, 'grad_norm': 5.3125, 'learning_rate': 4.95489940339725e-05, 'epoch': 0.9224489795918367}\n",
            "\n",
            ">>> Step 679 metrics: {'loss': 0.6311, 'grad_norm': 6.09375, 'learning_rate': 4.95476447746362e-05, 'epoch': 0.9238095238095239}\n",
            "\n",
            ">>> Step 680 metrics: {'loss': 0.7284, 'grad_norm': 6.375, 'learning_rate': 4.954629351847231e-05, 'epoch': 0.9251700680272109}\n",
            "\n",
            ">>> Step 681 metrics: {'loss': 0.6419, 'grad_norm': 5.25, 'learning_rate': 4.954494026559074e-05, 'epoch': 0.926530612244898}\n",
            "\n",
            ">>> Step 682 metrics: {'loss': 0.6718, 'grad_norm': 6.34375, 'learning_rate': 4.954358501610158e-05, 'epoch': 0.927891156462585}\n",
            "\n",
            ">>> Step 683 metrics: {'loss': 0.8321, 'grad_norm': 6.125, 'learning_rate': 4.9542227770115066e-05, 'epoch': 0.9292517006802721}\n",
            "\n",
            ">>> Step 684 metrics: {'loss': 0.96, 'grad_norm': 7.375, 'learning_rate': 4.9540868527741605e-05, 'epoch': 0.9306122448979591}\n",
            "\n",
            ">>> Step 685 metrics: {'loss': 0.6219, 'grad_norm': 5.90625, 'learning_rate': 4.953950728909177e-05, 'epoch': 0.9319727891156463}\n",
            "\n",
            ">>> Step 686 metrics: {'loss': 1.0598, 'grad_norm': 7.5625, 'learning_rate': 4.953814405427627e-05, 'epoch': 0.9333333333333333}\n",
            "\n",
            ">>> Step 687 metrics: {'loss': 1.3888, 'grad_norm': 8.8125, 'learning_rate': 4.953677882340602e-05, 'epoch': 0.9346938775510204}\n",
            "\n",
            ">>> Step 688 metrics: {'loss': 1.5702, 'grad_norm': 8.6875, 'learning_rate': 4.953541159659207e-05, 'epoch': 0.9360544217687075}\n",
            "\n",
            ">>> Step 689 metrics: {'loss': 1.0327, 'grad_norm': 6.125, 'learning_rate': 4.9534042373945624e-05, 'epoch': 0.9374149659863945}\n",
            "\n",
            ">>> Step 690 metrics: {'loss': 0.9491, 'grad_norm': 6.4375, 'learning_rate': 4.953267115557808e-05, 'epoch': 0.9387755102040817}\n",
            "\n",
            ">>> Step 691 metrics: {'loss': 1.1753, 'grad_norm': 7.125, 'learning_rate': 4.953129794160097e-05, 'epoch': 0.9401360544217687}\n",
            "\n",
            ">>> Step 692 metrics: {'loss': 1.4649, 'grad_norm': 6.875, 'learning_rate': 4.952992273212599e-05, 'epoch': 0.9414965986394558}\n",
            "\n",
            ">>> Step 693 metrics: {'loss': 1.1285, 'grad_norm': 6.46875, 'learning_rate': 4.952854552726502e-05, 'epoch': 0.9428571428571428}\n",
            "\n",
            ">>> Step 694 metrics: {'loss': 0.9461, 'grad_norm': 5.65625, 'learning_rate': 4.952716632713009e-05, 'epoch': 0.9442176870748299}\n",
            "\n",
            ">>> Step 695 metrics: {'loss': 0.9578, 'grad_norm': 5.84375, 'learning_rate': 4.952578513183338e-05, 'epoch': 0.9455782312925171}\n",
            "\n",
            ">>> Step 696 metrics: {'loss': 1.3907, 'grad_norm': 7.1875, 'learning_rate': 4.952440194148725e-05, 'epoch': 0.9469387755102041}\n",
            "\n",
            ">>> Step 697 metrics: {'loss': 1.1037, 'grad_norm': 6.5, 'learning_rate': 4.952301675620421e-05, 'epoch': 0.9482993197278912}\n",
            "\n",
            ">>> Step 698 metrics: {'loss': 1.2363, 'grad_norm': 7.0, 'learning_rate': 4.952162957609694e-05, 'epoch': 0.9496598639455782}\n",
            "\n",
            ">>> Step 699 metrics: {'loss': 1.288, 'grad_norm': 8.9375, 'learning_rate': 4.952024040127829e-05, 'epoch': 0.9510204081632653}\n",
            "\n",
            ">>> Step 700 metrics: {'loss': 0.9013, 'grad_norm': 6.0625, 'learning_rate': 4.951884923186125e-05, 'epoch': 0.9523809523809523}\n",
            "\n",
            ">>> Step 701 metrics: {'loss': 0.8833, 'grad_norm': 6.40625, 'learning_rate': 4.951745606795899e-05, 'epoch': 0.9537414965986395}\n",
            "\n",
            ">>> Step 702 metrics: {'loss': 1.115, 'grad_norm': 7.53125, 'learning_rate': 4.951606090968483e-05, 'epoch': 0.9551020408163265}\n",
            "\n",
            ">>> Step 703 metrics: {'loss': 0.8318, 'grad_norm': 7.3125, 'learning_rate': 4.951466375715227e-05, 'epoch': 0.9564625850340136}\n",
            "\n",
            ">>> Step 704 metrics: {'loss': 0.7195, 'grad_norm': 5.3125, 'learning_rate': 4.9513264610474955e-05, 'epoch': 0.9578231292517007}\n",
            "\n",
            ">>> Step 705 metrics: {'loss': 0.8374, 'grad_norm': 5.8125, 'learning_rate': 4.95118634697667e-05, 'epoch': 0.9591836734693877}\n",
            "\n",
            ">>> Step 706 metrics: {'loss': 0.8575, 'grad_norm': 6.0625, 'learning_rate': 4.951046033514148e-05, 'epoch': 0.9605442176870749}\n",
            "\n",
            ">>> Step 707 metrics: {'loss': 1.1996, 'grad_norm': 7.1875, 'learning_rate': 4.950905520671343e-05, 'epoch': 0.9619047619047619}\n",
            "\n",
            ">>> Step 708 metrics: {'loss': 1.1743, 'grad_norm': 8.5625, 'learning_rate': 4.950764808459686e-05, 'epoch': 0.963265306122449}\n",
            "\n",
            ">>> Step 709 metrics: {'loss': 1.7893, 'grad_norm': 8.75, 'learning_rate': 4.9506238968906224e-05, 'epoch': 0.964625850340136}\n",
            "\n",
            ">>> Step 710 metrics: {'loss': 1.0826, 'grad_norm': 7.375, 'learning_rate': 4.950482785975614e-05, 'epoch': 0.9659863945578231}\n",
            "\n",
            ">>> Step 711 metrics: {'loss': 0.9498, 'grad_norm': 6.125, 'learning_rate': 4.950341475726141e-05, 'epoch': 0.9673469387755103}\n",
            "\n",
            ">>> Step 712 metrics: {'loss': 1.278, 'grad_norm': 6.53125, 'learning_rate': 4.950199966153698e-05, 'epoch': 0.9687074829931973}\n",
            "\n",
            ">>> Step 713 metrics: {'loss': 1.1888, 'grad_norm': 7.6875, 'learning_rate': 4.950058257269795e-05, 'epoch': 0.9700680272108844}\n",
            "\n",
            ">>> Step 714 metrics: {'loss': 1.1573, 'grad_norm': 7.3125, 'learning_rate': 4.9499163490859605e-05, 'epoch': 0.9714285714285714}\n",
            "\n",
            ">>> Step 715 metrics: {'loss': 0.8455, 'grad_norm': 6.625, 'learning_rate': 4.9497742416137374e-05, 'epoch': 0.9727891156462585}\n",
            "\n",
            ">>> Step 716 metrics: {'loss': 0.8571, 'grad_norm': 6.65625, 'learning_rate': 4.949631934864685e-05, 'epoch': 0.9741496598639455}\n",
            "\n",
            ">>> Step 717 metrics: {'loss': 1.0973, 'grad_norm': 7.1875, 'learning_rate': 4.94948942885038e-05, 'epoch': 0.9755102040816327}\n",
            "\n",
            ">>> Step 718 metrics: {'loss': 0.8265, 'grad_norm': 7.15625, 'learning_rate': 4.949346723582415e-05, 'epoch': 0.9768707482993197}\n",
            "\n",
            ">>> Step 719 metrics: {'loss': 0.9247, 'grad_norm': 6.96875, 'learning_rate': 4.9492038190723966e-05, 'epoch': 0.9782312925170068}\n",
            "\n",
            ">>> Step 720 metrics: {'loss': 1.7062, 'grad_norm': 7.65625, 'learning_rate': 4.949060715331951e-05, 'epoch': 0.9795918367346939}\n",
            "\n",
            ">>> Step 721 metrics: {'loss': 0.4995, 'grad_norm': 5.375, 'learning_rate': 4.948917412372719e-05, 'epoch': 0.9809523809523809}\n",
            "\n",
            ">>> Step 722 metrics: {'loss': 1.1101, 'grad_norm': 7.3125, 'learning_rate': 4.948773910206356e-05, 'epoch': 0.9823129251700681}\n",
            "\n",
            ">>> Step 723 metrics: {'loss': 1.2104, 'grad_norm': 6.96875, 'learning_rate': 4.9486302088445364e-05, 'epoch': 0.9836734693877551}\n",
            "\n",
            ">>> Step 724 metrics: {'loss': 0.753, 'grad_norm': 5.78125, 'learning_rate': 4.94848630829895e-05, 'epoch': 0.9850340136054422}\n",
            "\n",
            ">>> Step 725 metrics: {'loss': 1.1376, 'grad_norm': 6.21875, 'learning_rate': 4.9483422085813004e-05, 'epoch': 0.9863945578231292}\n",
            "\n",
            ">>> Step 726 metrics: {'loss': 0.6613, 'grad_norm': 6.1875, 'learning_rate': 4.9481979097033114e-05, 'epoch': 0.9877551020408163}\n",
            "\n",
            ">>> Step 727 metrics: {'loss': 1.2128, 'grad_norm': 5.875, 'learning_rate': 4.9480534116767207e-05, 'epoch': 0.9891156462585035}\n",
            "\n",
            ">>> Step 728 metrics: {'loss': 0.6732, 'grad_norm': 6.4375, 'learning_rate': 4.947908714513282e-05, 'epoch': 0.9904761904761905}\n",
            "\n",
            ">>> Step 729 metrics: {'loss': 0.8062, 'grad_norm': 6.09375, 'learning_rate': 4.9477638182247656e-05, 'epoch': 0.9918367346938776}\n",
            "\n",
            ">>> Step 730 metrics: {'loss': 0.9652, 'grad_norm': 6.6875, 'learning_rate': 4.9476187228229575e-05, 'epoch': 0.9931972789115646}\n",
            "\n",
            ">>> Step 731 metrics: {'loss': 0.8005, 'grad_norm': 5.5, 'learning_rate': 4.9474734283196625e-05, 'epoch': 0.9945578231292517}\n",
            "\n",
            ">>> Step 732 metrics: {'loss': 1.0525, 'grad_norm': 6.4375, 'learning_rate': 4.9473279347266976e-05, 'epoch': 0.9959183673469387}\n",
            "\n",
            ">>> Step 733 metrics: {'loss': 0.9398, 'grad_norm': 5.78125, 'learning_rate': 4.947182242055899e-05, 'epoch': 0.9972789115646259}\n",
            "\n",
            ">>> Step 734 metrics: {'loss': 0.8791, 'grad_norm': 6.25, 'learning_rate': 4.9470363503191176e-05, 'epoch': 0.998639455782313}\n",
            "\n",
            ">>> Step 735 metrics: {'loss': 0.9482, 'grad_norm': 6.9375, 'learning_rate': 4.946890259528221e-05, 'epoch': 1.0}\n",
            "\n",
            ">>> Step 736 metrics: {'loss': 1.1022, 'grad_norm': 5.65625, 'learning_rate': 4.9467439696950933e-05, 'epoch': 1.0013605442176872}\n",
            "\n",
            ">>> Step 737 metrics: {'loss': 0.7651, 'grad_norm': 6.0625, 'learning_rate': 4.946597480831634e-05, 'epoch': 1.002721088435374}\n",
            "\n",
            ">>> Step 738 metrics: {'loss': 0.9166, 'grad_norm': 5.78125, 'learning_rate': 4.9464507929497595e-05, 'epoch': 1.0040816326530613}\n",
            "\n",
            ">>> Step 739 metrics: {'loss': 0.5799, 'grad_norm': 6.4375, 'learning_rate': 4.946303906061403e-05, 'epoch': 1.0054421768707482}\n",
            "\n",
            ">>> Step 740 metrics: {'loss': 0.6235, 'grad_norm': 5.46875, 'learning_rate': 4.946156820178511e-05, 'epoch': 1.0068027210884354}\n",
            "\n",
            ">>> Step 741 metrics: {'loss': 0.3432, 'grad_norm': 4.875, 'learning_rate': 4.946009535313049e-05, 'epoch': 1.0081632653061225}\n",
            "\n",
            ">>> Step 742 metrics: {'loss': 1.1229, 'grad_norm': 7.5, 'learning_rate': 4.945862051476999e-05, 'epoch': 1.0095238095238095}\n",
            "\n",
            ">>> Step 743 metrics: {'loss': 0.4185, 'grad_norm': 5.9375, 'learning_rate': 4.945714368682357e-05, 'epoch': 1.0108843537414967}\n",
            "\n",
            ">>> Step 744 metrics: {'loss': 0.3885, 'grad_norm': 6.5, 'learning_rate': 4.9455664869411364e-05, 'epoch': 1.0122448979591836}\n",
            "\n",
            ">>> Step 745 metrics: {'loss': 0.6046, 'grad_norm': 7.65625, 'learning_rate': 4.9454184062653666e-05, 'epoch': 1.0136054421768708}\n",
            "\n",
            ">>> Step 746 metrics: {'loss': 0.598, 'grad_norm': 7.34375, 'learning_rate': 4.945270126667094e-05, 'epoch': 1.014965986394558}\n",
            "\n",
            ">>> Step 747 metrics: {'loss': 0.5685, 'grad_norm': 8.125, 'learning_rate': 4.9451216481583796e-05, 'epoch': 1.0163265306122449}\n",
            "\n",
            ">>> Step 748 metrics: {'loss': 0.3621, 'grad_norm': 6.03125, 'learning_rate': 4.944972970751301e-05, 'epoch': 1.017687074829932}\n",
            "\n",
            ">>> Step 749 metrics: {'loss': 0.4654, 'grad_norm': 6.0, 'learning_rate': 4.9448240944579526e-05, 'epoch': 1.019047619047619}\n",
            "\n",
            ">>> Step 750 metrics: {'loss': 1.0432, 'grad_norm': 8.375, 'learning_rate': 4.9446750192904455e-05, 'epoch': 1.0204081632653061}\n",
            "\n",
            ">>> Step 751 metrics: {'loss': 0.5621, 'grad_norm': 6.78125, 'learning_rate': 4.944525745260906e-05, 'epoch': 1.021768707482993}\n",
            "\n",
            ">>> Step 752 metrics: {'loss': 0.6479, 'grad_norm': 7.53125, 'learning_rate': 4.944376272381477e-05, 'epoch': 1.0231292517006803}\n",
            "\n",
            ">>> Step 753 metrics: {'loss': 0.8053, 'grad_norm': 5.46875, 'learning_rate': 4.944226600664316e-05, 'epoch': 1.0244897959183674}\n",
            "\n",
            ">>> Step 754 metrics: {'loss': 0.6592, 'grad_norm': 5.96875, 'learning_rate': 4.944076730121599e-05, 'epoch': 1.0258503401360544}\n",
            "\n",
            ">>> Step 755 metrics: {'loss': 0.6846, 'grad_norm': 6.34375, 'learning_rate': 4.9439266607655175e-05, 'epoch': 1.0272108843537415}\n",
            "\n",
            ">>> Step 756 metrics: {'loss': 0.9039, 'grad_norm': 7.28125, 'learning_rate': 4.943776392608278e-05, 'epoch': 1.0285714285714285}\n",
            "\n",
            ">>> Step 757 metrics: {'loss': 0.5675, 'grad_norm': 6.375, 'learning_rate': 4.943625925662105e-05, 'epoch': 1.0299319727891156}\n",
            "\n",
            ">>> Step 758 metrics: {'loss': 0.7001, 'grad_norm': 6.0625, 'learning_rate': 4.943475259939238e-05, 'epoch': 1.0312925170068028}\n",
            "\n",
            ">>> Step 759 metrics: {'loss': 0.4305, 'grad_norm': 5.875, 'learning_rate': 4.9433243954519325e-05, 'epoch': 1.0326530612244897}\n",
            "\n",
            ">>> Step 760 metrics: {'loss': 0.3189, 'grad_norm': 5.15625, 'learning_rate': 4.9431733322124604e-05, 'epoch': 1.034013605442177}\n",
            "\n",
            ">>> Step 761 metrics: {'loss': 0.3718, 'grad_norm': 5.375, 'learning_rate': 4.943022070233111e-05, 'epoch': 1.0353741496598639}\n",
            "\n",
            ">>> Step 762 metrics: {'loss': 0.5787, 'grad_norm': 8.1875, 'learning_rate': 4.9428706095261885e-05, 'epoch': 1.036734693877551}\n",
            "\n",
            ">>> Step 763 metrics: {'loss': 0.8992, 'grad_norm': 6.78125, 'learning_rate': 4.9427189501040124e-05, 'epoch': 1.0380952380952382}\n",
            "\n",
            ">>> Step 764 metrics: {'loss': 0.636, 'grad_norm': 6.34375, 'learning_rate': 4.94256709197892e-05, 'epoch': 1.0394557823129251}\n",
            "\n",
            ">>> Step 765 metrics: {'loss': 0.5614, 'grad_norm': 6.25, 'learning_rate': 4.942415035163264e-05, 'epoch': 1.0408163265306123}\n",
            "\n",
            ">>> Step 766 metrics: {'loss': 0.4301, 'grad_norm': 6.375, 'learning_rate': 4.942262779669414e-05, 'epoch': 1.0421768707482992}\n",
            "\n",
            ">>> Step 767 metrics: {'loss': 0.9731, 'grad_norm': 9.3125, 'learning_rate': 4.9421103255097553e-05, 'epoch': 1.0435374149659864}\n",
            "\n",
            ">>> Step 768 metrics: {'loss': 0.3918, 'grad_norm': 7.28125, 'learning_rate': 4.9419576726966886e-05, 'epoch': 1.0448979591836736}\n",
            "\n",
            ">>> Step 769 metrics: {'loss': 0.6986, 'grad_norm': 6.59375, 'learning_rate': 4.9418048212426314e-05, 'epoch': 1.0462585034013605}\n",
            "\n",
            ">>> Step 770 metrics: {'loss': 0.7311, 'grad_norm': 7.9375, 'learning_rate': 4.941651771160018e-05, 'epoch': 1.0476190476190477}\n",
            "\n",
            ">>> Step 771 metrics: {'loss': 0.5029, 'grad_norm': 6.28125, 'learning_rate': 4.941498522461298e-05, 'epoch': 1.0489795918367346}\n",
            "\n",
            ">>> Step 772 metrics: {'loss': 0.5871, 'grad_norm': 6.375, 'learning_rate': 4.941345075158938e-05, 'epoch': 1.0503401360544218}\n",
            "\n",
            ">>> Step 773 metrics: {'loss': 0.9132, 'grad_norm': 7.9375, 'learning_rate': 4.9411914292654185e-05, 'epoch': 1.051700680272109}\n",
            "\n",
            ">>> Step 774 metrics: {'loss': 1.0174, 'grad_norm': 9.75, 'learning_rate': 4.941037584793239e-05, 'epoch': 1.0530612244897959}\n",
            "\n",
            ">>> Step 775 metrics: {'loss': 0.7104, 'grad_norm': 7.46875, 'learning_rate': 4.940883541754915e-05, 'epoch': 1.054421768707483}\n",
            "\n",
            ">>> Step 776 metrics: {'loss': 0.6573, 'grad_norm': 7.0625, 'learning_rate': 4.940729300162975e-05, 'epoch': 1.05578231292517}\n",
            "\n",
            ">>> Step 777 metrics: {'loss': 0.5736, 'grad_norm': 8.0, 'learning_rate': 4.940574860029967e-05, 'epoch': 1.0571428571428572}\n",
            "\n",
            ">>> Step 778 metrics: {'loss': 0.5445, 'grad_norm': 5.6875, 'learning_rate': 4.9404202213684535e-05, 'epoch': 1.0585034013605443}\n",
            "\n",
            ">>> Step 779 metrics: {'loss': 0.6584, 'grad_norm': 5.78125, 'learning_rate': 4.940265384191014e-05, 'epoch': 1.0598639455782313}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2495' max='11025' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 2495/11025 2:57:28 < 10:07:13, 0.23 it/s, Epoch 3.39/15]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.379900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.949100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.026500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.451100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.110200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.993700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.213700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.624200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.187800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.892500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.301200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.811600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.099800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.090800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.981500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.230100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.239900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.393000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.692700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.955900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.876200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.549800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.496600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.245700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.662900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.360500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.037300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.244500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.367400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.132000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>1.004100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.175900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.576600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.763900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.311400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.805200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>1.913800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.593500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.129600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.027600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.868400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>1.074400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.987500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>1.174300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.745300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>1.205800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>1.695700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.785700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.959700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.125600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>1.266100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.733400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>1.074200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.605200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.913000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.950200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>1.121600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>1.166700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>1.191800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.879100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>1.276300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.804600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.488200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.705400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.913700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.826900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>1.077200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>1.073100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.899000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.895200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>1.517700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.705000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>1.086000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>1.172700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.861900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.725600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.875800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>1.159400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>1.469800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.999700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>0.698000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>1.338800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>1.114500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>1.265200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.967300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>1.582300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.698100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>1.264600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>1.075900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.116300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>1.143800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.695800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.763600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>1.188800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>1.086700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.961800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>1.169800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.936100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>1.237200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.020800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>1.104100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>1.110800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>0.864100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>0.709800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>1.238700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>1.085400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>0.817400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>0.977500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>1.149700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.774800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>1.205400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>1.020900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>1.414100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>0.958900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.879900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>0.848800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>0.946300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>0.621600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>0.983900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.892400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>1.033500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>0.743700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>0.795100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>0.725900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>1.265400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>0.766700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>1.467100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>1.407800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>1.543300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.525300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>1.237600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>1.163700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>0.921600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>1.377400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>0.677000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>1.339700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>0.833100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>1.099300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>1.297400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.403800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>0.780600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>1.445300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143</td>\n",
              "      <td>0.716000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>1.012400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>1.464200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>0.759700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>0.453400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>0.940400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149</td>\n",
              "      <td>1.083000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.577400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151</td>\n",
              "      <td>0.781300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>0.452300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153</td>\n",
              "      <td>1.058100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>0.723300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>1.325400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>0.816900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157</td>\n",
              "      <td>1.287200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158</td>\n",
              "      <td>1.026700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159</td>\n",
              "      <td>0.840000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.911400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161</td>\n",
              "      <td>0.530900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162</td>\n",
              "      <td>1.577200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>163</td>\n",
              "      <td>1.200200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164</td>\n",
              "      <td>1.050300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>0.778900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166</td>\n",
              "      <td>1.752600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>167</td>\n",
              "      <td>0.832000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>0.657400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>169</td>\n",
              "      <td>0.696700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.920800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>1.132000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172</td>\n",
              "      <td>0.696500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>173</td>\n",
              "      <td>1.082100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>174</td>\n",
              "      <td>0.953500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.786300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>176</td>\n",
              "      <td>1.209400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>177</td>\n",
              "      <td>0.584700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>178</td>\n",
              "      <td>0.787600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>179</td>\n",
              "      <td>0.842900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.833300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>181</td>\n",
              "      <td>1.723800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>182</td>\n",
              "      <td>0.746800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>183</td>\n",
              "      <td>0.717500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>184</td>\n",
              "      <td>1.286600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>1.460100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>186</td>\n",
              "      <td>0.762000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>187</td>\n",
              "      <td>0.650300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>188</td>\n",
              "      <td>1.005200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>189</td>\n",
              "      <td>1.213500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>1.338300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>191</td>\n",
              "      <td>0.831100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>192</td>\n",
              "      <td>0.641500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>193</td>\n",
              "      <td>1.303900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>194</td>\n",
              "      <td>1.111200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>0.781800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>0.807400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>197</td>\n",
              "      <td>0.813600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>198</td>\n",
              "      <td>0.943500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>199</td>\n",
              "      <td>0.871500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.687400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>201</td>\n",
              "      <td>1.619300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>202</td>\n",
              "      <td>1.038800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>203</td>\n",
              "      <td>1.041700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>204</td>\n",
              "      <td>0.832700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>205</td>\n",
              "      <td>0.920700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>206</td>\n",
              "      <td>1.414400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>207</td>\n",
              "      <td>1.042900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>208</td>\n",
              "      <td>0.578700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>209</td>\n",
              "      <td>0.835700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.835200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>211</td>\n",
              "      <td>1.219900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>212</td>\n",
              "      <td>0.865400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>213</td>\n",
              "      <td>1.246200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>214</td>\n",
              "      <td>0.995400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>215</td>\n",
              "      <td>1.576700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>216</td>\n",
              "      <td>1.181800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>217</td>\n",
              "      <td>0.655500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>218</td>\n",
              "      <td>0.929000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>219</td>\n",
              "      <td>0.794400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>1.190300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>221</td>\n",
              "      <td>0.846700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>222</td>\n",
              "      <td>1.024300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>223</td>\n",
              "      <td>1.005300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>224</td>\n",
              "      <td>0.700300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>1.044200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>226</td>\n",
              "      <td>1.127700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>227</td>\n",
              "      <td>1.546800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>228</td>\n",
              "      <td>0.523700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>229</td>\n",
              "      <td>0.730500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>1.127500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>231</td>\n",
              "      <td>1.102900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>232</td>\n",
              "      <td>1.112500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>233</td>\n",
              "      <td>1.078800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>234</td>\n",
              "      <td>0.971400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>235</td>\n",
              "      <td>1.443000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>236</td>\n",
              "      <td>0.980700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>237</td>\n",
              "      <td>0.948400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>238</td>\n",
              "      <td>0.832600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>239</td>\n",
              "      <td>0.870900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>1.044100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>241</td>\n",
              "      <td>1.438500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>242</td>\n",
              "      <td>0.820900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>243</td>\n",
              "      <td>1.401600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>244</td>\n",
              "      <td>0.840700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>245</td>\n",
              "      <td>0.900900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>246</td>\n",
              "      <td>0.988700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>247</td>\n",
              "      <td>0.834500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>248</td>\n",
              "      <td>1.465000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>249</td>\n",
              "      <td>0.724000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.780100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>251</td>\n",
              "      <td>1.218500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>252</td>\n",
              "      <td>1.386300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>253</td>\n",
              "      <td>1.773100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>254</td>\n",
              "      <td>0.939300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>255</td>\n",
              "      <td>1.472700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>256</td>\n",
              "      <td>0.864700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>257</td>\n",
              "      <td>0.940400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>258</td>\n",
              "      <td>1.518400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>259</td>\n",
              "      <td>0.850800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.948600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>261</td>\n",
              "      <td>1.149900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>262</td>\n",
              "      <td>0.886300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>263</td>\n",
              "      <td>0.679400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>264</td>\n",
              "      <td>0.908900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>265</td>\n",
              "      <td>0.931100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>266</td>\n",
              "      <td>0.745700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>267</td>\n",
              "      <td>1.215700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>268</td>\n",
              "      <td>0.859700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>269</td>\n",
              "      <td>1.034400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>1.398800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>271</td>\n",
              "      <td>0.860800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>272</td>\n",
              "      <td>0.690800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>273</td>\n",
              "      <td>1.324400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>274</td>\n",
              "      <td>0.742100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>0.874100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>276</td>\n",
              "      <td>0.929200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>277</td>\n",
              "      <td>0.887100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>278</td>\n",
              "      <td>0.647400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>279</td>\n",
              "      <td>1.375100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>1.280300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>281</td>\n",
              "      <td>0.986400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>282</td>\n",
              "      <td>1.356000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>283</td>\n",
              "      <td>0.865800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>284</td>\n",
              "      <td>1.115100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>285</td>\n",
              "      <td>0.782100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>286</td>\n",
              "      <td>1.129200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>287</td>\n",
              "      <td>1.452900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>288</td>\n",
              "      <td>0.887500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>289</td>\n",
              "      <td>1.024000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.849700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>291</td>\n",
              "      <td>0.853800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>292</td>\n",
              "      <td>0.999900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>293</td>\n",
              "      <td>0.632000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>294</td>\n",
              "      <td>0.730800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>295</td>\n",
              "      <td>1.131900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>296</td>\n",
              "      <td>0.953900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>297</td>\n",
              "      <td>0.777400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>298</td>\n",
              "      <td>0.602600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>299</td>\n",
              "      <td>0.778600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>301</td>\n",
              "      <td>0.533300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>302</td>\n",
              "      <td>0.803500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>303</td>\n",
              "      <td>0.702800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>304</td>\n",
              "      <td>1.296700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>305</td>\n",
              "      <td>1.171000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>306</td>\n",
              "      <td>1.287900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>307</td>\n",
              "      <td>0.998300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>308</td>\n",
              "      <td>0.566300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>309</td>\n",
              "      <td>1.107100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.779200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>311</td>\n",
              "      <td>0.887500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>312</td>\n",
              "      <td>1.287900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>313</td>\n",
              "      <td>1.151900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>314</td>\n",
              "      <td>1.195800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>315</td>\n",
              "      <td>0.745500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>316</td>\n",
              "      <td>0.691900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>317</td>\n",
              "      <td>1.004200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>318</td>\n",
              "      <td>1.332300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>319</td>\n",
              "      <td>0.866700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.583300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>321</td>\n",
              "      <td>0.910400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>322</td>\n",
              "      <td>0.643200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>323</td>\n",
              "      <td>1.241500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>324</td>\n",
              "      <td>1.554400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>325</td>\n",
              "      <td>0.717500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>326</td>\n",
              "      <td>1.280400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>327</td>\n",
              "      <td>0.597800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>328</td>\n",
              "      <td>1.264700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>329</td>\n",
              "      <td>0.604400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.638000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>331</td>\n",
              "      <td>1.460400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>332</td>\n",
              "      <td>1.187100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>333</td>\n",
              "      <td>1.177300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>334</td>\n",
              "      <td>1.192000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>335</td>\n",
              "      <td>0.827800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>336</td>\n",
              "      <td>0.884500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>337</td>\n",
              "      <td>0.742300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>338</td>\n",
              "      <td>0.996300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>339</td>\n",
              "      <td>0.947300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.804400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>341</td>\n",
              "      <td>1.304600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>342</td>\n",
              "      <td>1.036400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>343</td>\n",
              "      <td>1.187200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>344</td>\n",
              "      <td>1.065900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>345</td>\n",
              "      <td>0.825600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>346</td>\n",
              "      <td>0.969700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>347</td>\n",
              "      <td>1.168400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>348</td>\n",
              "      <td>1.007300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>349</td>\n",
              "      <td>0.957300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.166400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>351</td>\n",
              "      <td>1.024300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>352</td>\n",
              "      <td>0.749400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>353</td>\n",
              "      <td>0.774900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>354</td>\n",
              "      <td>0.708400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>355</td>\n",
              "      <td>1.128400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>356</td>\n",
              "      <td>0.808400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>357</td>\n",
              "      <td>1.317700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>358</td>\n",
              "      <td>0.910100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>359</td>\n",
              "      <td>1.147900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>1.506500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>361</td>\n",
              "      <td>0.621900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>362</td>\n",
              "      <td>0.871100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>363</td>\n",
              "      <td>0.818500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>364</td>\n",
              "      <td>0.888100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>365</td>\n",
              "      <td>1.312700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>366</td>\n",
              "      <td>0.815700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>367</td>\n",
              "      <td>1.226200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>368</td>\n",
              "      <td>0.790300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>369</td>\n",
              "      <td>1.295200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.995500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>371</td>\n",
              "      <td>1.199100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>372</td>\n",
              "      <td>1.003300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>373</td>\n",
              "      <td>0.711700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>374</td>\n",
              "      <td>1.075300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>375</td>\n",
              "      <td>0.758900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>376</td>\n",
              "      <td>0.592300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>377</td>\n",
              "      <td>1.300500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>378</td>\n",
              "      <td>0.548100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>379</td>\n",
              "      <td>1.629500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>1.323900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>381</td>\n",
              "      <td>0.980300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>382</td>\n",
              "      <td>0.979500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>383</td>\n",
              "      <td>0.953700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>384</td>\n",
              "      <td>0.955000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>385</td>\n",
              "      <td>0.829200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>386</td>\n",
              "      <td>1.202800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>387</td>\n",
              "      <td>1.189600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>388</td>\n",
              "      <td>0.687800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>389</td>\n",
              "      <td>1.498800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.976000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>391</td>\n",
              "      <td>1.208500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>392</td>\n",
              "      <td>0.867300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>393</td>\n",
              "      <td>0.917300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>394</td>\n",
              "      <td>0.985700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>395</td>\n",
              "      <td>1.075800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>396</td>\n",
              "      <td>0.891600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>397</td>\n",
              "      <td>0.787800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>398</td>\n",
              "      <td>1.478600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>399</td>\n",
              "      <td>0.792300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.693500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>401</td>\n",
              "      <td>0.958300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>402</td>\n",
              "      <td>0.867800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>403</td>\n",
              "      <td>1.296900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>404</td>\n",
              "      <td>1.007700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>405</td>\n",
              "      <td>1.069100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>406</td>\n",
              "      <td>0.749600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>407</td>\n",
              "      <td>0.737100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>408</td>\n",
              "      <td>1.197000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>409</td>\n",
              "      <td>0.918100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.898000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>411</td>\n",
              "      <td>0.684700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>412</td>\n",
              "      <td>0.877900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>413</td>\n",
              "      <td>1.023200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>414</td>\n",
              "      <td>0.875100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>415</td>\n",
              "      <td>0.749400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>416</td>\n",
              "      <td>1.411200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>417</td>\n",
              "      <td>1.122300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>418</td>\n",
              "      <td>1.815400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>419</td>\n",
              "      <td>1.167100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.787200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>421</td>\n",
              "      <td>1.055700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>422</td>\n",
              "      <td>1.391300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>423</td>\n",
              "      <td>1.434200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>424</td>\n",
              "      <td>0.980500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>425</td>\n",
              "      <td>0.810400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>426</td>\n",
              "      <td>1.117500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>427</td>\n",
              "      <td>1.118200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>428</td>\n",
              "      <td>1.432700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>429</td>\n",
              "      <td>0.578500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.553300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>431</td>\n",
              "      <td>0.817700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>432</td>\n",
              "      <td>0.836000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>433</td>\n",
              "      <td>0.764500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>434</td>\n",
              "      <td>1.106400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>435</td>\n",
              "      <td>0.892300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>436</td>\n",
              "      <td>0.974000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>437</td>\n",
              "      <td>1.080700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>438</td>\n",
              "      <td>1.259600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>439</td>\n",
              "      <td>0.990900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.695900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>441</td>\n",
              "      <td>0.780500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>442</td>\n",
              "      <td>0.955800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>443</td>\n",
              "      <td>0.907800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>444</td>\n",
              "      <td>1.263500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>445</td>\n",
              "      <td>0.746000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>446</td>\n",
              "      <td>0.530500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>447</td>\n",
              "      <td>0.591200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>448</td>\n",
              "      <td>0.896600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>449</td>\n",
              "      <td>1.170000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.830300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>451</td>\n",
              "      <td>0.641700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>452</td>\n",
              "      <td>0.887500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>453</td>\n",
              "      <td>1.035700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>454</td>\n",
              "      <td>1.591300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>455</td>\n",
              "      <td>1.451800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>456</td>\n",
              "      <td>1.157900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>457</td>\n",
              "      <td>1.293400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>458</td>\n",
              "      <td>1.069900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>459</td>\n",
              "      <td>0.630000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>1.234400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>461</td>\n",
              "      <td>0.998800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>462</td>\n",
              "      <td>0.765900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>463</td>\n",
              "      <td>1.105800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>464</td>\n",
              "      <td>1.307700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>465</td>\n",
              "      <td>0.887700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>466</td>\n",
              "      <td>0.876700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>467</td>\n",
              "      <td>1.323500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>468</td>\n",
              "      <td>0.486500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>469</td>\n",
              "      <td>0.622700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>0.958000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>471</td>\n",
              "      <td>0.870400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>472</td>\n",
              "      <td>0.960500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>473</td>\n",
              "      <td>0.756300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>474</td>\n",
              "      <td>0.763900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>475</td>\n",
              "      <td>0.837900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>476</td>\n",
              "      <td>1.301900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>477</td>\n",
              "      <td>0.826100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>478</td>\n",
              "      <td>1.012300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>479</td>\n",
              "      <td>0.399100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.716700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>481</td>\n",
              "      <td>0.639200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>482</td>\n",
              "      <td>1.723000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>483</td>\n",
              "      <td>1.244100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>484</td>\n",
              "      <td>0.839700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>485</td>\n",
              "      <td>0.611200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>486</td>\n",
              "      <td>0.421900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>487</td>\n",
              "      <td>1.201400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>488</td>\n",
              "      <td>0.749300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>489</td>\n",
              "      <td>0.656000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>1.054300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>491</td>\n",
              "      <td>0.690400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>492</td>\n",
              "      <td>0.797200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>493</td>\n",
              "      <td>1.175800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>494</td>\n",
              "      <td>0.881500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>495</td>\n",
              "      <td>0.559000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>496</td>\n",
              "      <td>1.010400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>497</td>\n",
              "      <td>0.804900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>498</td>\n",
              "      <td>0.673400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>499</td>\n",
              "      <td>0.900700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.803100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>501</td>\n",
              "      <td>0.928800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>502</td>\n",
              "      <td>1.116500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>503</td>\n",
              "      <td>0.602500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>504</td>\n",
              "      <td>1.289300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>505</td>\n",
              "      <td>1.299300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>506</td>\n",
              "      <td>1.026100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>507</td>\n",
              "      <td>0.583600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>508</td>\n",
              "      <td>0.744300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>509</td>\n",
              "      <td>1.019800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>1.406500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>511</td>\n",
              "      <td>0.689600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>512</td>\n",
              "      <td>0.983200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>513</td>\n",
              "      <td>0.911600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>514</td>\n",
              "      <td>1.130700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>515</td>\n",
              "      <td>1.007000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>516</td>\n",
              "      <td>1.032500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>517</td>\n",
              "      <td>1.406900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>518</td>\n",
              "      <td>1.004500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>519</td>\n",
              "      <td>1.325000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>1.081800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>521</td>\n",
              "      <td>0.780800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>522</td>\n",
              "      <td>1.282600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>523</td>\n",
              "      <td>0.775900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>524</td>\n",
              "      <td>1.058200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>525</td>\n",
              "      <td>0.739900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>526</td>\n",
              "      <td>1.318000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>527</td>\n",
              "      <td>0.713800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>528</td>\n",
              "      <td>0.606800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>529</td>\n",
              "      <td>0.952700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>0.700600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>531</td>\n",
              "      <td>0.842300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>532</td>\n",
              "      <td>0.895800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>533</td>\n",
              "      <td>1.356900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>534</td>\n",
              "      <td>0.970200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>535</td>\n",
              "      <td>0.835300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>536</td>\n",
              "      <td>0.592900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>537</td>\n",
              "      <td>1.175200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>538</td>\n",
              "      <td>1.067300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>539</td>\n",
              "      <td>0.844200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>1.282300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>541</td>\n",
              "      <td>0.916400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>542</td>\n",
              "      <td>0.968900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>543</td>\n",
              "      <td>1.236500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>544</td>\n",
              "      <td>0.945400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>545</td>\n",
              "      <td>0.845100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>546</td>\n",
              "      <td>0.883400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>547</td>\n",
              "      <td>1.073700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>548</td>\n",
              "      <td>0.834000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>549</td>\n",
              "      <td>0.852000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.752000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>551</td>\n",
              "      <td>0.835300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>552</td>\n",
              "      <td>1.002300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>553</td>\n",
              "      <td>0.992500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>554</td>\n",
              "      <td>1.023600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>555</td>\n",
              "      <td>1.000800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>556</td>\n",
              "      <td>1.513800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>557</td>\n",
              "      <td>0.768600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>558</td>\n",
              "      <td>1.045900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>559</td>\n",
              "      <td>1.374300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.851200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>561</td>\n",
              "      <td>0.891400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>562</td>\n",
              "      <td>0.922200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>563</td>\n",
              "      <td>1.136500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>564</td>\n",
              "      <td>1.012200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>565</td>\n",
              "      <td>0.996200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>566</td>\n",
              "      <td>1.381400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>567</td>\n",
              "      <td>1.302100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>568</td>\n",
              "      <td>1.598200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>569</td>\n",
              "      <td>0.969600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.652100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>571</td>\n",
              "      <td>1.486000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>572</td>\n",
              "      <td>0.988800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>573</td>\n",
              "      <td>1.043100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>574</td>\n",
              "      <td>1.143900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>575</td>\n",
              "      <td>0.978700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>576</td>\n",
              "      <td>0.731500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>577</td>\n",
              "      <td>0.884500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>578</td>\n",
              "      <td>1.050800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>579</td>\n",
              "      <td>1.441400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>0.856500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>581</td>\n",
              "      <td>1.369200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>582</td>\n",
              "      <td>1.017700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>583</td>\n",
              "      <td>0.962500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>584</td>\n",
              "      <td>1.211000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>585</td>\n",
              "      <td>1.325200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>586</td>\n",
              "      <td>1.054100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>587</td>\n",
              "      <td>1.429700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>588</td>\n",
              "      <td>1.103200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>589</td>\n",
              "      <td>1.431800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>1.225300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>591</td>\n",
              "      <td>0.823100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>592</td>\n",
              "      <td>0.882200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>593</td>\n",
              "      <td>1.093900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>594</td>\n",
              "      <td>1.137600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>595</td>\n",
              "      <td>1.375500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>596</td>\n",
              "      <td>0.690000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>597</td>\n",
              "      <td>0.715400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>598</td>\n",
              "      <td>1.401800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>599</td>\n",
              "      <td>1.034100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.894500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>601</td>\n",
              "      <td>1.094100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>602</td>\n",
              "      <td>1.565700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>603</td>\n",
              "      <td>0.717000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>604</td>\n",
              "      <td>0.958800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>605</td>\n",
              "      <td>0.712700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>606</td>\n",
              "      <td>0.793100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>607</td>\n",
              "      <td>1.096500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>608</td>\n",
              "      <td>1.103400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>609</td>\n",
              "      <td>1.383000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>610</td>\n",
              "      <td>0.530900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>611</td>\n",
              "      <td>0.708000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>612</td>\n",
              "      <td>1.426300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>613</td>\n",
              "      <td>0.585100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>614</td>\n",
              "      <td>1.226100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>615</td>\n",
              "      <td>0.832800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>616</td>\n",
              "      <td>0.959200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>617</td>\n",
              "      <td>0.693400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>618</td>\n",
              "      <td>0.836500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>619</td>\n",
              "      <td>0.591600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>1.114400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>621</td>\n",
              "      <td>0.939000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>622</td>\n",
              "      <td>0.901100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>623</td>\n",
              "      <td>1.682700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>624</td>\n",
              "      <td>1.340900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>625</td>\n",
              "      <td>1.027000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>626</td>\n",
              "      <td>0.702400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>627</td>\n",
              "      <td>1.174400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>628</td>\n",
              "      <td>0.741300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>629</td>\n",
              "      <td>0.917100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>630</td>\n",
              "      <td>0.621000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>631</td>\n",
              "      <td>0.783700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>632</td>\n",
              "      <td>1.161600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>633</td>\n",
              "      <td>0.890900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>634</td>\n",
              "      <td>0.891300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>635</td>\n",
              "      <td>0.788000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>636</td>\n",
              "      <td>0.947800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>637</td>\n",
              "      <td>1.359200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>638</td>\n",
              "      <td>1.115500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>639</td>\n",
              "      <td>0.922500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>0.882400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>641</td>\n",
              "      <td>1.111300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>642</td>\n",
              "      <td>0.890900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>643</td>\n",
              "      <td>0.722900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>644</td>\n",
              "      <td>0.880700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>645</td>\n",
              "      <td>0.792400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>646</td>\n",
              "      <td>1.364900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>647</td>\n",
              "      <td>0.931800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>648</td>\n",
              "      <td>0.722200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>649</td>\n",
              "      <td>0.668300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.944200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>651</td>\n",
              "      <td>0.813800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>652</td>\n",
              "      <td>1.188800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>653</td>\n",
              "      <td>1.097300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>654</td>\n",
              "      <td>1.250700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>655</td>\n",
              "      <td>1.002500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>656</td>\n",
              "      <td>1.062900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>657</td>\n",
              "      <td>0.825300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>658</td>\n",
              "      <td>1.468900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>659</td>\n",
              "      <td>0.749300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>1.059500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>661</td>\n",
              "      <td>0.876400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>662</td>\n",
              "      <td>1.032700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>663</td>\n",
              "      <td>1.275100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>664</td>\n",
              "      <td>1.004600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>665</td>\n",
              "      <td>1.029400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>666</td>\n",
              "      <td>1.226200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>667</td>\n",
              "      <td>1.198600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>668</td>\n",
              "      <td>1.273700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>669</td>\n",
              "      <td>0.808700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>670</td>\n",
              "      <td>0.879700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>671</td>\n",
              "      <td>0.826300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>672</td>\n",
              "      <td>0.663700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>673</td>\n",
              "      <td>0.841600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>674</td>\n",
              "      <td>0.607300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>675</td>\n",
              "      <td>0.922000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>676</td>\n",
              "      <td>1.017800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>677</td>\n",
              "      <td>0.822800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>678</td>\n",
              "      <td>0.599300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>679</td>\n",
              "      <td>0.631100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>0.728400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>681</td>\n",
              "      <td>0.641900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>682</td>\n",
              "      <td>0.671800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>683</td>\n",
              "      <td>0.832100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>684</td>\n",
              "      <td>0.960000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>685</td>\n",
              "      <td>0.621900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>686</td>\n",
              "      <td>1.059800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>687</td>\n",
              "      <td>1.388800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>688</td>\n",
              "      <td>1.570200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>689</td>\n",
              "      <td>1.032700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>0.949100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>691</td>\n",
              "      <td>1.175300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>692</td>\n",
              "      <td>1.464900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>693</td>\n",
              "      <td>1.128500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>694</td>\n",
              "      <td>0.946100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>695</td>\n",
              "      <td>0.957800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>696</td>\n",
              "      <td>1.390700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>697</td>\n",
              "      <td>1.103700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>698</td>\n",
              "      <td>1.236300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>699</td>\n",
              "      <td>1.288000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.901300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>701</td>\n",
              "      <td>0.883300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>702</td>\n",
              "      <td>1.115000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>703</td>\n",
              "      <td>0.831800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>704</td>\n",
              "      <td>0.719500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>705</td>\n",
              "      <td>0.837400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>706</td>\n",
              "      <td>0.857500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>707</td>\n",
              "      <td>1.199600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>708</td>\n",
              "      <td>1.174300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>709</td>\n",
              "      <td>1.789300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>710</td>\n",
              "      <td>1.082600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>711</td>\n",
              "      <td>0.949800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>712</td>\n",
              "      <td>1.278000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>713</td>\n",
              "      <td>1.188800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>714</td>\n",
              "      <td>1.157300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>715</td>\n",
              "      <td>0.845500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>716</td>\n",
              "      <td>0.857100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>717</td>\n",
              "      <td>1.097300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>718</td>\n",
              "      <td>0.826500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>719</td>\n",
              "      <td>0.924700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>1.706200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>721</td>\n",
              "      <td>0.499500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>722</td>\n",
              "      <td>1.110100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>723</td>\n",
              "      <td>1.210400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>724</td>\n",
              "      <td>0.753000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>725</td>\n",
              "      <td>1.137600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>726</td>\n",
              "      <td>0.661300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>727</td>\n",
              "      <td>1.212800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>728</td>\n",
              "      <td>0.673200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>729</td>\n",
              "      <td>0.806200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>730</td>\n",
              "      <td>0.965200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>731</td>\n",
              "      <td>0.800500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>732</td>\n",
              "      <td>1.052500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>733</td>\n",
              "      <td>0.939800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>734</td>\n",
              "      <td>0.879100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>735</td>\n",
              "      <td>0.948200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>736</td>\n",
              "      <td>1.102200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>737</td>\n",
              "      <td>0.765100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>738</td>\n",
              "      <td>0.916600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>739</td>\n",
              "      <td>0.579900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>740</td>\n",
              "      <td>0.623500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>741</td>\n",
              "      <td>0.343200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>742</td>\n",
              "      <td>1.122900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>743</td>\n",
              "      <td>0.418500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>744</td>\n",
              "      <td>0.388500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>745</td>\n",
              "      <td>0.604600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>746</td>\n",
              "      <td>0.598000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>747</td>\n",
              "      <td>0.568500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>748</td>\n",
              "      <td>0.362100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>749</td>\n",
              "      <td>0.465400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>1.043200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>751</td>\n",
              "      <td>0.562100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>752</td>\n",
              "      <td>0.647900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>753</td>\n",
              "      <td>0.805300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>754</td>\n",
              "      <td>0.659200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>755</td>\n",
              "      <td>0.684600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>756</td>\n",
              "      <td>0.903900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>757</td>\n",
              "      <td>0.567500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>758</td>\n",
              "      <td>0.700100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>759</td>\n",
              "      <td>0.430500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>760</td>\n",
              "      <td>0.318900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>761</td>\n",
              "      <td>0.371800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>762</td>\n",
              "      <td>0.578700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>763</td>\n",
              "      <td>0.899200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>764</td>\n",
              "      <td>0.636000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>765</td>\n",
              "      <td>0.561400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>766</td>\n",
              "      <td>0.430100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>767</td>\n",
              "      <td>0.973100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>768</td>\n",
              "      <td>0.391800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>769</td>\n",
              "      <td>0.698600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>770</td>\n",
              "      <td>0.731100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>771</td>\n",
              "      <td>0.502900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>772</td>\n",
              "      <td>0.587100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>773</td>\n",
              "      <td>0.913200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>774</td>\n",
              "      <td>1.017400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>775</td>\n",
              "      <td>0.710400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>776</td>\n",
              "      <td>0.657300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>777</td>\n",
              "      <td>0.573600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>778</td>\n",
              "      <td>0.544500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>779</td>\n",
              "      <td>0.658400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>780</td>\n",
              "      <td>1.165000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>781</td>\n",
              "      <td>0.949900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>782</td>\n",
              "      <td>1.041500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>783</td>\n",
              "      <td>0.472500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>784</td>\n",
              "      <td>0.723400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>785</td>\n",
              "      <td>0.611900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>786</td>\n",
              "      <td>0.855300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>787</td>\n",
              "      <td>0.566300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>788</td>\n",
              "      <td>1.127500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>789</td>\n",
              "      <td>0.859600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>790</td>\n",
              "      <td>0.798800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>791</td>\n",
              "      <td>0.851600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>792</td>\n",
              "      <td>0.936800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>793</td>\n",
              "      <td>0.826900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>794</td>\n",
              "      <td>0.594400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>795</td>\n",
              "      <td>0.656700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>796</td>\n",
              "      <td>0.878700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>797</td>\n",
              "      <td>0.643200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>798</td>\n",
              "      <td>0.969100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>799</td>\n",
              "      <td>0.418500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.353900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>801</td>\n",
              "      <td>0.746900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>802</td>\n",
              "      <td>0.931500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>803</td>\n",
              "      <td>0.597900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>804</td>\n",
              "      <td>0.819400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>805</td>\n",
              "      <td>0.780400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>806</td>\n",
              "      <td>0.579900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>807</td>\n",
              "      <td>0.674700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>808</td>\n",
              "      <td>0.920100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>809</td>\n",
              "      <td>0.515200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>810</td>\n",
              "      <td>0.630700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>811</td>\n",
              "      <td>0.572500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>812</td>\n",
              "      <td>0.635200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>813</td>\n",
              "      <td>0.567100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>814</td>\n",
              "      <td>0.513500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>815</td>\n",
              "      <td>0.444100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>816</td>\n",
              "      <td>0.368500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>817</td>\n",
              "      <td>0.694200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>818</td>\n",
              "      <td>0.649900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>819</td>\n",
              "      <td>0.984800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>820</td>\n",
              "      <td>0.543700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>821</td>\n",
              "      <td>0.639100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>822</td>\n",
              "      <td>0.431500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>823</td>\n",
              "      <td>0.522900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>824</td>\n",
              "      <td>0.956000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>825</td>\n",
              "      <td>0.405200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>826</td>\n",
              "      <td>0.752900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>827</td>\n",
              "      <td>0.328300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>828</td>\n",
              "      <td>0.884700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>829</td>\n",
              "      <td>0.850900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>830</td>\n",
              "      <td>0.566900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>831</td>\n",
              "      <td>0.653000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>832</td>\n",
              "      <td>1.225300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>833</td>\n",
              "      <td>0.761100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>834</td>\n",
              "      <td>0.525200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>835</td>\n",
              "      <td>0.665000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>836</td>\n",
              "      <td>0.680500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>837</td>\n",
              "      <td>0.913400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>838</td>\n",
              "      <td>0.977700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>839</td>\n",
              "      <td>0.583600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>840</td>\n",
              "      <td>1.092300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>841</td>\n",
              "      <td>0.489200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>842</td>\n",
              "      <td>1.029700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>843</td>\n",
              "      <td>0.837200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>844</td>\n",
              "      <td>1.229800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>845</td>\n",
              "      <td>1.227200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>846</td>\n",
              "      <td>0.429600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>847</td>\n",
              "      <td>0.578800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>848</td>\n",
              "      <td>0.434100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>849</td>\n",
              "      <td>0.830900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.548500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>851</td>\n",
              "      <td>0.575800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>852</td>\n",
              "      <td>0.390700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>853</td>\n",
              "      <td>0.627200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>854</td>\n",
              "      <td>0.694900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>855</td>\n",
              "      <td>0.645500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>856</td>\n",
              "      <td>0.763300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>857</td>\n",
              "      <td>0.596400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>858</td>\n",
              "      <td>0.822300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>859</td>\n",
              "      <td>0.813700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>860</td>\n",
              "      <td>1.360800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>861</td>\n",
              "      <td>0.874800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>862</td>\n",
              "      <td>0.542600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>863</td>\n",
              "      <td>0.651100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>864</td>\n",
              "      <td>0.748000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>865</td>\n",
              "      <td>0.770800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>866</td>\n",
              "      <td>1.055700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>867</td>\n",
              "      <td>1.168200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>868</td>\n",
              "      <td>0.634000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>869</td>\n",
              "      <td>0.687100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>870</td>\n",
              "      <td>0.597800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>871</td>\n",
              "      <td>1.200900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>872</td>\n",
              "      <td>1.008100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>873</td>\n",
              "      <td>0.709000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>874</td>\n",
              "      <td>0.633400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>875</td>\n",
              "      <td>0.891900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>876</td>\n",
              "      <td>0.489900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>877</td>\n",
              "      <td>0.718500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>878</td>\n",
              "      <td>0.892000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>879</td>\n",
              "      <td>0.628400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>880</td>\n",
              "      <td>0.393600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>881</td>\n",
              "      <td>0.459100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>882</td>\n",
              "      <td>0.567000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>883</td>\n",
              "      <td>0.598400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>884</td>\n",
              "      <td>0.465500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>885</td>\n",
              "      <td>0.530400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>886</td>\n",
              "      <td>0.967700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>887</td>\n",
              "      <td>1.045600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>888</td>\n",
              "      <td>0.613700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>889</td>\n",
              "      <td>0.851900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>890</td>\n",
              "      <td>1.103900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>891</td>\n",
              "      <td>0.596600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>892</td>\n",
              "      <td>0.579500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>893</td>\n",
              "      <td>0.482100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>894</td>\n",
              "      <td>1.083800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>895</td>\n",
              "      <td>0.282000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>896</td>\n",
              "      <td>0.750500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>897</td>\n",
              "      <td>0.748000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>898</td>\n",
              "      <td>0.689800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>899</td>\n",
              "      <td>0.702100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.631100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>901</td>\n",
              "      <td>0.772700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>902</td>\n",
              "      <td>0.911500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>903</td>\n",
              "      <td>0.496900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>904</td>\n",
              "      <td>1.071900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>905</td>\n",
              "      <td>0.529600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>906</td>\n",
              "      <td>0.857300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>907</td>\n",
              "      <td>0.669200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>908</td>\n",
              "      <td>0.473300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>909</td>\n",
              "      <td>0.402000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>910</td>\n",
              "      <td>0.889000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>911</td>\n",
              "      <td>0.607700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>912</td>\n",
              "      <td>0.693700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>913</td>\n",
              "      <td>0.871500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>914</td>\n",
              "      <td>0.794300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>915</td>\n",
              "      <td>0.701300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>916</td>\n",
              "      <td>0.915100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>917</td>\n",
              "      <td>0.746600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>918</td>\n",
              "      <td>0.605100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>919</td>\n",
              "      <td>0.928100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>0.840800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>921</td>\n",
              "      <td>0.459100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>922</td>\n",
              "      <td>0.957200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>923</td>\n",
              "      <td>0.481500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>924</td>\n",
              "      <td>0.829600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>925</td>\n",
              "      <td>0.822600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>926</td>\n",
              "      <td>0.731200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>927</td>\n",
              "      <td>0.697600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>928</td>\n",
              "      <td>0.387900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>929</td>\n",
              "      <td>0.538400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>930</td>\n",
              "      <td>0.455500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>931</td>\n",
              "      <td>0.421400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>932</td>\n",
              "      <td>0.730700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>933</td>\n",
              "      <td>0.489400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>934</td>\n",
              "      <td>0.745000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>935</td>\n",
              "      <td>1.139100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>936</td>\n",
              "      <td>0.724400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>937</td>\n",
              "      <td>1.322000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>938</td>\n",
              "      <td>0.456800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>939</td>\n",
              "      <td>0.399100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>940</td>\n",
              "      <td>0.522300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>941</td>\n",
              "      <td>1.002900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>942</td>\n",
              "      <td>0.654600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>943</td>\n",
              "      <td>0.481500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>944</td>\n",
              "      <td>0.419000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>945</td>\n",
              "      <td>0.646100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>946</td>\n",
              "      <td>1.098700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>947</td>\n",
              "      <td>0.735000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>948</td>\n",
              "      <td>0.344300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>949</td>\n",
              "      <td>0.739200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.865900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>951</td>\n",
              "      <td>0.920200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>952</td>\n",
              "      <td>0.545500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>953</td>\n",
              "      <td>0.619700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>954</td>\n",
              "      <td>0.583400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>955</td>\n",
              "      <td>0.716600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>956</td>\n",
              "      <td>0.461300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>957</td>\n",
              "      <td>0.266500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>958</td>\n",
              "      <td>0.518000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>959</td>\n",
              "      <td>0.518900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>960</td>\n",
              "      <td>0.378800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>961</td>\n",
              "      <td>1.301500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>962</td>\n",
              "      <td>0.467800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>963</td>\n",
              "      <td>0.732200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>964</td>\n",
              "      <td>0.517500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>965</td>\n",
              "      <td>0.913300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>966</td>\n",
              "      <td>0.796100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>967</td>\n",
              "      <td>0.946100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>968</td>\n",
              "      <td>0.437100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>969</td>\n",
              "      <td>0.814700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>970</td>\n",
              "      <td>0.607500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>971</td>\n",
              "      <td>0.844300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>972</td>\n",
              "      <td>0.969300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>973</td>\n",
              "      <td>0.705700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>974</td>\n",
              "      <td>0.484900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>975</td>\n",
              "      <td>0.603600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>976</td>\n",
              "      <td>0.476300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>977</td>\n",
              "      <td>1.065600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>978</td>\n",
              "      <td>0.619700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>979</td>\n",
              "      <td>0.582500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>980</td>\n",
              "      <td>0.705900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>981</td>\n",
              "      <td>0.503600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>982</td>\n",
              "      <td>0.873700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>983</td>\n",
              "      <td>1.140800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>984</td>\n",
              "      <td>0.853000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>985</td>\n",
              "      <td>0.482000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>986</td>\n",
              "      <td>0.580900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>987</td>\n",
              "      <td>1.169100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>988</td>\n",
              "      <td>0.595300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>989</td>\n",
              "      <td>0.826900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>990</td>\n",
              "      <td>1.252200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>991</td>\n",
              "      <td>0.702600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>992</td>\n",
              "      <td>0.463800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>993</td>\n",
              "      <td>1.082400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>994</td>\n",
              "      <td>0.873900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>995</td>\n",
              "      <td>0.612300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>996</td>\n",
              "      <td>0.570900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>997</td>\n",
              "      <td>0.544300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>998</td>\n",
              "      <td>0.460000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>999</td>\n",
              "      <td>0.874100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.394800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1001</td>\n",
              "      <td>1.071300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1002</td>\n",
              "      <td>1.073900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1003</td>\n",
              "      <td>0.416200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1004</td>\n",
              "      <td>0.941400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1005</td>\n",
              "      <td>0.846800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1006</td>\n",
              "      <td>0.955100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1007</td>\n",
              "      <td>0.367300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1008</td>\n",
              "      <td>0.514700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1009</td>\n",
              "      <td>0.617800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1010</td>\n",
              "      <td>0.889400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1011</td>\n",
              "      <td>0.672500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1012</td>\n",
              "      <td>0.583100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1013</td>\n",
              "      <td>0.793900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1014</td>\n",
              "      <td>0.989000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1015</td>\n",
              "      <td>0.752900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1016</td>\n",
              "      <td>0.965200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1017</td>\n",
              "      <td>0.596600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1018</td>\n",
              "      <td>1.206100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1019</td>\n",
              "      <td>0.907200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1020</td>\n",
              "      <td>0.796000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1021</td>\n",
              "      <td>0.836900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1022</td>\n",
              "      <td>0.601200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1023</td>\n",
              "      <td>0.465100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1024</td>\n",
              "      <td>0.903900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1025</td>\n",
              "      <td>0.391900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1026</td>\n",
              "      <td>0.717800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1027</td>\n",
              "      <td>0.805200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1028</td>\n",
              "      <td>0.747900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1029</td>\n",
              "      <td>0.728300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1030</td>\n",
              "      <td>0.716900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1031</td>\n",
              "      <td>0.624400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1032</td>\n",
              "      <td>0.782000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1033</td>\n",
              "      <td>0.276700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1034</td>\n",
              "      <td>0.580900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1035</td>\n",
              "      <td>0.938300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1036</td>\n",
              "      <td>0.563800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1037</td>\n",
              "      <td>0.451900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1038</td>\n",
              "      <td>0.604300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1039</td>\n",
              "      <td>0.579600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1040</td>\n",
              "      <td>0.907000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1041</td>\n",
              "      <td>0.462000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1042</td>\n",
              "      <td>0.622400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1043</td>\n",
              "      <td>0.594600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1044</td>\n",
              "      <td>0.629700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1045</td>\n",
              "      <td>0.980800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1046</td>\n",
              "      <td>0.838300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1047</td>\n",
              "      <td>0.736800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1048</td>\n",
              "      <td>1.149700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1049</td>\n",
              "      <td>1.244200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>0.481700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1051</td>\n",
              "      <td>0.735200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1052</td>\n",
              "      <td>0.544900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1053</td>\n",
              "      <td>0.862500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1054</td>\n",
              "      <td>0.527400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1055</td>\n",
              "      <td>0.478900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1056</td>\n",
              "      <td>0.597900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1057</td>\n",
              "      <td>1.268900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1058</td>\n",
              "      <td>0.966700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1059</td>\n",
              "      <td>0.744100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1060</td>\n",
              "      <td>0.765400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1061</td>\n",
              "      <td>0.796000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1062</td>\n",
              "      <td>0.416200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1063</td>\n",
              "      <td>0.624600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1064</td>\n",
              "      <td>1.036300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1065</td>\n",
              "      <td>1.134600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1066</td>\n",
              "      <td>0.833900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1067</td>\n",
              "      <td>0.521500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1068</td>\n",
              "      <td>0.562500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1069</td>\n",
              "      <td>0.831700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1070</td>\n",
              "      <td>0.997300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1071</td>\n",
              "      <td>0.573900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1072</td>\n",
              "      <td>0.807300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1073</td>\n",
              "      <td>0.940000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1074</td>\n",
              "      <td>0.521900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1075</td>\n",
              "      <td>0.836700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1076</td>\n",
              "      <td>0.597100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1077</td>\n",
              "      <td>0.951200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1078</td>\n",
              "      <td>1.108800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1079</td>\n",
              "      <td>0.718000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1080</td>\n",
              "      <td>1.385900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1081</td>\n",
              "      <td>1.534400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1082</td>\n",
              "      <td>0.804300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1083</td>\n",
              "      <td>0.646200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1084</td>\n",
              "      <td>0.943300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1085</td>\n",
              "      <td>0.988500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1086</td>\n",
              "      <td>0.999000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1087</td>\n",
              "      <td>0.981000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1088</td>\n",
              "      <td>0.780200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1089</td>\n",
              "      <td>0.696100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1090</td>\n",
              "      <td>0.583700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1091</td>\n",
              "      <td>0.683600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1092</td>\n",
              "      <td>0.441300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1093</td>\n",
              "      <td>0.564300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1094</td>\n",
              "      <td>1.284100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1095</td>\n",
              "      <td>0.702600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1096</td>\n",
              "      <td>0.805100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1097</td>\n",
              "      <td>0.743900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1098</td>\n",
              "      <td>1.131500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1099</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.543400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1101</td>\n",
              "      <td>1.101400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1102</td>\n",
              "      <td>0.577800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1103</td>\n",
              "      <td>1.104500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1104</td>\n",
              "      <td>0.517500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1105</td>\n",
              "      <td>0.925900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1106</td>\n",
              "      <td>0.598000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1107</td>\n",
              "      <td>0.687200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1108</td>\n",
              "      <td>0.716400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1109</td>\n",
              "      <td>1.133100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1110</td>\n",
              "      <td>0.803100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1111</td>\n",
              "      <td>0.961300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1112</td>\n",
              "      <td>0.424500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1113</td>\n",
              "      <td>0.742600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1114</td>\n",
              "      <td>0.401600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1115</td>\n",
              "      <td>0.875700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1116</td>\n",
              "      <td>0.897800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1117</td>\n",
              "      <td>1.145800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1118</td>\n",
              "      <td>0.567700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1119</td>\n",
              "      <td>0.534800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1120</td>\n",
              "      <td>1.521700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1121</td>\n",
              "      <td>0.799300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1122</td>\n",
              "      <td>1.144900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1123</td>\n",
              "      <td>0.508200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1124</td>\n",
              "      <td>0.551700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1125</td>\n",
              "      <td>0.787200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1126</td>\n",
              "      <td>0.661600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1127</td>\n",
              "      <td>1.088000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1128</td>\n",
              "      <td>0.643700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1129</td>\n",
              "      <td>0.552500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1130</td>\n",
              "      <td>1.047100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1131</td>\n",
              "      <td>0.618700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1132</td>\n",
              "      <td>0.437300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1133</td>\n",
              "      <td>0.840000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1134</td>\n",
              "      <td>0.776600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1135</td>\n",
              "      <td>0.653900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1136</td>\n",
              "      <td>0.647900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1137</td>\n",
              "      <td>0.411700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1138</td>\n",
              "      <td>0.546400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1139</td>\n",
              "      <td>0.655800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1140</td>\n",
              "      <td>0.629100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1141</td>\n",
              "      <td>0.585700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1142</td>\n",
              "      <td>0.491000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1143</td>\n",
              "      <td>1.170000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1144</td>\n",
              "      <td>0.852500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1145</td>\n",
              "      <td>0.654300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1146</td>\n",
              "      <td>1.078800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1147</td>\n",
              "      <td>0.773600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1148</td>\n",
              "      <td>0.293000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1149</td>\n",
              "      <td>0.788100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>1.016900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1151</td>\n",
              "      <td>0.988000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1152</td>\n",
              "      <td>0.624700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1153</td>\n",
              "      <td>0.782400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1154</td>\n",
              "      <td>0.678800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1155</td>\n",
              "      <td>0.972100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1156</td>\n",
              "      <td>0.702800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1157</td>\n",
              "      <td>0.512100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1158</td>\n",
              "      <td>0.640900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1159</td>\n",
              "      <td>0.590600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1160</td>\n",
              "      <td>0.993100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1161</td>\n",
              "      <td>0.654700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1162</td>\n",
              "      <td>1.136500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1163</td>\n",
              "      <td>0.847000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1164</td>\n",
              "      <td>0.606800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1165</td>\n",
              "      <td>0.389600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1166</td>\n",
              "      <td>0.536100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1167</td>\n",
              "      <td>0.595900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1168</td>\n",
              "      <td>0.853000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1169</td>\n",
              "      <td>0.379400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1170</td>\n",
              "      <td>0.695100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1171</td>\n",
              "      <td>0.475200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1172</td>\n",
              "      <td>0.808300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1173</td>\n",
              "      <td>0.454500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1174</td>\n",
              "      <td>0.565500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1175</td>\n",
              "      <td>0.673100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1176</td>\n",
              "      <td>0.761400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1177</td>\n",
              "      <td>0.595500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1178</td>\n",
              "      <td>0.436400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1179</td>\n",
              "      <td>0.743000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1180</td>\n",
              "      <td>0.946100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1181</td>\n",
              "      <td>0.675400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1182</td>\n",
              "      <td>0.904900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1183</td>\n",
              "      <td>0.510900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1184</td>\n",
              "      <td>0.633800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1185</td>\n",
              "      <td>0.514400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1186</td>\n",
              "      <td>0.940300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1187</td>\n",
              "      <td>0.689100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1188</td>\n",
              "      <td>0.651900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1189</td>\n",
              "      <td>0.529400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1190</td>\n",
              "      <td>0.479100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1191</td>\n",
              "      <td>0.642100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1192</td>\n",
              "      <td>0.557400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1193</td>\n",
              "      <td>0.723500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1194</td>\n",
              "      <td>0.597600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1195</td>\n",
              "      <td>0.922800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1196</td>\n",
              "      <td>0.490200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1197</td>\n",
              "      <td>0.742800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1198</td>\n",
              "      <td>1.003000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1199</td>\n",
              "      <td>1.102900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.894800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1201</td>\n",
              "      <td>0.883400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1202</td>\n",
              "      <td>0.426700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1203</td>\n",
              "      <td>1.265000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1204</td>\n",
              "      <td>0.553900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1205</td>\n",
              "      <td>0.693500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1206</td>\n",
              "      <td>1.282500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1207</td>\n",
              "      <td>0.864200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1208</td>\n",
              "      <td>0.815100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1209</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1210</td>\n",
              "      <td>0.446100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1211</td>\n",
              "      <td>0.987400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1212</td>\n",
              "      <td>0.693300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1213</td>\n",
              "      <td>0.969900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1214</td>\n",
              "      <td>0.516500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1215</td>\n",
              "      <td>1.097000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1216</td>\n",
              "      <td>0.464500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1217</td>\n",
              "      <td>1.027700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1218</td>\n",
              "      <td>0.466900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1219</td>\n",
              "      <td>0.735400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1220</td>\n",
              "      <td>0.621000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1221</td>\n",
              "      <td>0.710700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1222</td>\n",
              "      <td>0.838900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1223</td>\n",
              "      <td>0.586400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1224</td>\n",
              "      <td>0.573200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1225</td>\n",
              "      <td>1.580100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1226</td>\n",
              "      <td>0.776200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1227</td>\n",
              "      <td>0.458900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1228</td>\n",
              "      <td>0.896000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1229</td>\n",
              "      <td>1.001900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1230</td>\n",
              "      <td>0.691300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1231</td>\n",
              "      <td>0.545200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1232</td>\n",
              "      <td>0.426700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1233</td>\n",
              "      <td>0.646500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1234</td>\n",
              "      <td>1.102800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1235</td>\n",
              "      <td>1.289300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1236</td>\n",
              "      <td>0.947300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1237</td>\n",
              "      <td>0.687500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1238</td>\n",
              "      <td>0.759700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1239</td>\n",
              "      <td>1.006100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1240</td>\n",
              "      <td>0.334000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1241</td>\n",
              "      <td>1.076000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1242</td>\n",
              "      <td>0.627400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1243</td>\n",
              "      <td>1.190700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1244</td>\n",
              "      <td>0.456300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1245</td>\n",
              "      <td>0.809000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1246</td>\n",
              "      <td>0.323600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1247</td>\n",
              "      <td>0.539300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1248</td>\n",
              "      <td>0.902900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1249</td>\n",
              "      <td>0.679800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.521100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1251</td>\n",
              "      <td>0.390900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1252</td>\n",
              "      <td>0.344400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1253</td>\n",
              "      <td>0.401900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1254</td>\n",
              "      <td>0.583200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1255</td>\n",
              "      <td>0.331500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1256</td>\n",
              "      <td>0.423500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1257</td>\n",
              "      <td>0.569400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1258</td>\n",
              "      <td>0.932000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1259</td>\n",
              "      <td>0.480300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1260</td>\n",
              "      <td>1.100100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1261</td>\n",
              "      <td>0.882000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1262</td>\n",
              "      <td>0.937900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1263</td>\n",
              "      <td>1.070400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1264</td>\n",
              "      <td>0.413200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1265</td>\n",
              "      <td>0.450700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1266</td>\n",
              "      <td>0.592700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1267</td>\n",
              "      <td>0.484500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1268</td>\n",
              "      <td>0.584500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1269</td>\n",
              "      <td>0.561500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1270</td>\n",
              "      <td>0.525800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1271</td>\n",
              "      <td>1.019100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1272</td>\n",
              "      <td>0.301200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1273</td>\n",
              "      <td>0.740100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1274</td>\n",
              "      <td>0.890400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1275</td>\n",
              "      <td>0.821800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1276</td>\n",
              "      <td>0.801300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1277</td>\n",
              "      <td>0.574400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1278</td>\n",
              "      <td>0.695000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1279</td>\n",
              "      <td>0.479000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1280</td>\n",
              "      <td>0.999000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1281</td>\n",
              "      <td>0.661000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1282</td>\n",
              "      <td>0.580100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1283</td>\n",
              "      <td>0.710900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1284</td>\n",
              "      <td>0.707900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1285</td>\n",
              "      <td>0.683200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1286</td>\n",
              "      <td>0.652200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1287</td>\n",
              "      <td>0.471800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1288</td>\n",
              "      <td>0.828100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1289</td>\n",
              "      <td>0.950700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1290</td>\n",
              "      <td>1.066600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1291</td>\n",
              "      <td>0.599900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1292</td>\n",
              "      <td>0.655500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1293</td>\n",
              "      <td>0.908200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1294</td>\n",
              "      <td>0.648600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1295</td>\n",
              "      <td>0.468100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1296</td>\n",
              "      <td>1.181400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1297</td>\n",
              "      <td>0.924200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1298</td>\n",
              "      <td>0.769800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1299</td>\n",
              "      <td>0.603400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>1.374400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1301</td>\n",
              "      <td>0.470100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1302</td>\n",
              "      <td>0.654100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1303</td>\n",
              "      <td>0.386500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1304</td>\n",
              "      <td>0.702000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1305</td>\n",
              "      <td>1.059000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1306</td>\n",
              "      <td>0.462700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1307</td>\n",
              "      <td>0.541100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1308</td>\n",
              "      <td>0.397900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1309</td>\n",
              "      <td>0.611200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1310</td>\n",
              "      <td>1.073200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1311</td>\n",
              "      <td>0.511700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1312</td>\n",
              "      <td>0.820300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1313</td>\n",
              "      <td>0.878100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1314</td>\n",
              "      <td>0.362300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1315</td>\n",
              "      <td>0.743100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1316</td>\n",
              "      <td>1.080100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1317</td>\n",
              "      <td>0.597200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1318</td>\n",
              "      <td>0.593500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1319</td>\n",
              "      <td>0.649900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1320</td>\n",
              "      <td>0.645000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1321</td>\n",
              "      <td>0.931800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1322</td>\n",
              "      <td>0.649400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1323</td>\n",
              "      <td>0.722200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1324</td>\n",
              "      <td>0.446200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1325</td>\n",
              "      <td>0.899200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1326</td>\n",
              "      <td>0.813300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1327</td>\n",
              "      <td>0.319700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1328</td>\n",
              "      <td>0.518500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1329</td>\n",
              "      <td>1.309200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1330</td>\n",
              "      <td>0.737900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1331</td>\n",
              "      <td>0.634200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1332</td>\n",
              "      <td>0.693900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1333</td>\n",
              "      <td>0.756600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1334</td>\n",
              "      <td>0.358600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1335</td>\n",
              "      <td>0.677700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1336</td>\n",
              "      <td>0.298800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1337</td>\n",
              "      <td>0.676700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1338</td>\n",
              "      <td>0.982900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1339</td>\n",
              "      <td>1.009200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1340</td>\n",
              "      <td>1.126400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1341</td>\n",
              "      <td>0.783900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1342</td>\n",
              "      <td>0.598600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1343</td>\n",
              "      <td>1.412900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1344</td>\n",
              "      <td>0.584100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1345</td>\n",
              "      <td>0.729400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1346</td>\n",
              "      <td>0.934300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1347</td>\n",
              "      <td>0.764900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1348</td>\n",
              "      <td>0.491000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1349</td>\n",
              "      <td>0.886700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>0.523900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1351</td>\n",
              "      <td>0.817800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1352</td>\n",
              "      <td>0.902400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1353</td>\n",
              "      <td>0.398500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1354</td>\n",
              "      <td>0.816000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1355</td>\n",
              "      <td>0.859400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1356</td>\n",
              "      <td>0.356700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1357</td>\n",
              "      <td>0.607400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1358</td>\n",
              "      <td>0.557500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1359</td>\n",
              "      <td>0.970200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1360</td>\n",
              "      <td>0.522800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1361</td>\n",
              "      <td>1.022100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1362</td>\n",
              "      <td>0.678800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1363</td>\n",
              "      <td>0.826700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1364</td>\n",
              "      <td>0.710500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1365</td>\n",
              "      <td>0.714300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1366</td>\n",
              "      <td>0.533000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1367</td>\n",
              "      <td>0.637500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1368</td>\n",
              "      <td>1.005200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1369</td>\n",
              "      <td>0.630300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1370</td>\n",
              "      <td>0.661400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1371</td>\n",
              "      <td>1.129800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1372</td>\n",
              "      <td>0.590800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1373</td>\n",
              "      <td>0.783400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1374</td>\n",
              "      <td>0.755400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1375</td>\n",
              "      <td>1.143600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1376</td>\n",
              "      <td>0.669100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1377</td>\n",
              "      <td>0.760300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1378</td>\n",
              "      <td>0.564000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1379</td>\n",
              "      <td>1.117700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1380</td>\n",
              "      <td>0.659700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1381</td>\n",
              "      <td>0.563000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1382</td>\n",
              "      <td>0.638900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1383</td>\n",
              "      <td>0.503600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1384</td>\n",
              "      <td>0.819200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1385</td>\n",
              "      <td>0.901400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1386</td>\n",
              "      <td>0.759900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1387</td>\n",
              "      <td>0.866500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1388</td>\n",
              "      <td>1.209300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1389</td>\n",
              "      <td>0.676600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1390</td>\n",
              "      <td>0.519800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1391</td>\n",
              "      <td>0.340300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1392</td>\n",
              "      <td>0.363100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1393</td>\n",
              "      <td>0.738400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1394</td>\n",
              "      <td>0.691000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1395</td>\n",
              "      <td>0.569000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1396</td>\n",
              "      <td>0.696800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1397</td>\n",
              "      <td>0.785600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1398</td>\n",
              "      <td>0.683400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1399</td>\n",
              "      <td>0.634900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.385300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1401</td>\n",
              "      <td>0.784900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1402</td>\n",
              "      <td>0.504500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1403</td>\n",
              "      <td>0.735200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1404</td>\n",
              "      <td>0.243700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1405</td>\n",
              "      <td>0.553900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1406</td>\n",
              "      <td>0.444100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1407</td>\n",
              "      <td>0.966100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1408</td>\n",
              "      <td>0.874400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1409</td>\n",
              "      <td>0.732000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1410</td>\n",
              "      <td>0.729200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1411</td>\n",
              "      <td>0.511700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1412</td>\n",
              "      <td>0.656300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1413</td>\n",
              "      <td>0.847300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1414</td>\n",
              "      <td>0.533000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1415</td>\n",
              "      <td>0.461100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1416</td>\n",
              "      <td>0.883800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1417</td>\n",
              "      <td>0.708100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1418</td>\n",
              "      <td>0.744100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1419</td>\n",
              "      <td>0.379000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1420</td>\n",
              "      <td>1.126900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1421</td>\n",
              "      <td>1.116600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1422</td>\n",
              "      <td>0.681500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1423</td>\n",
              "      <td>0.724100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1424</td>\n",
              "      <td>1.087100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1425</td>\n",
              "      <td>0.506200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1426</td>\n",
              "      <td>0.730600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1427</td>\n",
              "      <td>0.921900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1428</td>\n",
              "      <td>0.626100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1429</td>\n",
              "      <td>0.723900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1430</td>\n",
              "      <td>0.644800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1431</td>\n",
              "      <td>0.710100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1432</td>\n",
              "      <td>0.684400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1433</td>\n",
              "      <td>0.739300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1434</td>\n",
              "      <td>0.552200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1435</td>\n",
              "      <td>0.604000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1436</td>\n",
              "      <td>0.529500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1437</td>\n",
              "      <td>0.419300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1438</td>\n",
              "      <td>0.397600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1439</td>\n",
              "      <td>0.598000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1440</td>\n",
              "      <td>0.739600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1441</td>\n",
              "      <td>0.874600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1442</td>\n",
              "      <td>0.603800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1443</td>\n",
              "      <td>0.480000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1444</td>\n",
              "      <td>0.936000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1445</td>\n",
              "      <td>0.458700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1446</td>\n",
              "      <td>0.424400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1447</td>\n",
              "      <td>1.166500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1448</td>\n",
              "      <td>0.904000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1449</td>\n",
              "      <td>0.385700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>0.564200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1451</td>\n",
              "      <td>1.087800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1452</td>\n",
              "      <td>0.863900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1453</td>\n",
              "      <td>0.795600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1454</td>\n",
              "      <td>0.778000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1455</td>\n",
              "      <td>0.510500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1456</td>\n",
              "      <td>0.768500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1457</td>\n",
              "      <td>0.480800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1458</td>\n",
              "      <td>0.544500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1459</td>\n",
              "      <td>0.527000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1460</td>\n",
              "      <td>0.330800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1461</td>\n",
              "      <td>0.627700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1462</td>\n",
              "      <td>0.720000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1463</td>\n",
              "      <td>0.693900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1464</td>\n",
              "      <td>0.658400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1465</td>\n",
              "      <td>1.068300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1466</td>\n",
              "      <td>1.199000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1467</td>\n",
              "      <td>0.704600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1468</td>\n",
              "      <td>0.616700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1469</td>\n",
              "      <td>0.484800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1470</td>\n",
              "      <td>0.633300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1471</td>\n",
              "      <td>0.560600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1472</td>\n",
              "      <td>0.448500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1473</td>\n",
              "      <td>0.451100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1474</td>\n",
              "      <td>0.412200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1475</td>\n",
              "      <td>0.534700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1476</td>\n",
              "      <td>0.464600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1477</td>\n",
              "      <td>0.339400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1478</td>\n",
              "      <td>0.491000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1479</td>\n",
              "      <td>0.190900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1480</td>\n",
              "      <td>0.416000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1481</td>\n",
              "      <td>0.640000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1482</td>\n",
              "      <td>0.742300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1483</td>\n",
              "      <td>0.452300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1484</td>\n",
              "      <td>0.398500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1485</td>\n",
              "      <td>0.346000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1486</td>\n",
              "      <td>0.654300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1487</td>\n",
              "      <td>0.307300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1488</td>\n",
              "      <td>0.506500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1489</td>\n",
              "      <td>0.347500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1490</td>\n",
              "      <td>0.200700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1491</td>\n",
              "      <td>1.091900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1492</td>\n",
              "      <td>0.653300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1493</td>\n",
              "      <td>0.320900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1494</td>\n",
              "      <td>0.395000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1495</td>\n",
              "      <td>0.313100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1496</td>\n",
              "      <td>0.414000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1497</td>\n",
              "      <td>0.325400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1498</td>\n",
              "      <td>0.399400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1499</td>\n",
              "      <td>0.660400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.417100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1501</td>\n",
              "      <td>0.408100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1502</td>\n",
              "      <td>0.501100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1503</td>\n",
              "      <td>0.419100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1504</td>\n",
              "      <td>0.227400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1505</td>\n",
              "      <td>0.768500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1506</td>\n",
              "      <td>0.434400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1507</td>\n",
              "      <td>0.295100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1508</td>\n",
              "      <td>0.449300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1509</td>\n",
              "      <td>0.128300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1510</td>\n",
              "      <td>0.673600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1511</td>\n",
              "      <td>0.260700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1512</td>\n",
              "      <td>0.662900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1513</td>\n",
              "      <td>0.789400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1514</td>\n",
              "      <td>0.839700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1515</td>\n",
              "      <td>0.260900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1516</td>\n",
              "      <td>0.325800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1517</td>\n",
              "      <td>0.278700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1518</td>\n",
              "      <td>0.615800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1519</td>\n",
              "      <td>0.423300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1520</td>\n",
              "      <td>0.567100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1521</td>\n",
              "      <td>0.296500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1522</td>\n",
              "      <td>0.152700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1523</td>\n",
              "      <td>0.476200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1524</td>\n",
              "      <td>0.612500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1525</td>\n",
              "      <td>0.450700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1526</td>\n",
              "      <td>0.508500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1527</td>\n",
              "      <td>0.364700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1528</td>\n",
              "      <td>0.776800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1529</td>\n",
              "      <td>0.248100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1530</td>\n",
              "      <td>0.940400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1531</td>\n",
              "      <td>0.590600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1532</td>\n",
              "      <td>0.601600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1533</td>\n",
              "      <td>0.470600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1534</td>\n",
              "      <td>0.689000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1535</td>\n",
              "      <td>0.654500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1536</td>\n",
              "      <td>0.357000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1537</td>\n",
              "      <td>0.435900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1538</td>\n",
              "      <td>0.524800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1539</td>\n",
              "      <td>0.362600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1540</td>\n",
              "      <td>0.909300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1541</td>\n",
              "      <td>0.218000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1542</td>\n",
              "      <td>0.206500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1543</td>\n",
              "      <td>0.394400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1544</td>\n",
              "      <td>0.691300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1545</td>\n",
              "      <td>0.660600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1546</td>\n",
              "      <td>0.338800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1547</td>\n",
              "      <td>0.913700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1548</td>\n",
              "      <td>0.317800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1549</td>\n",
              "      <td>0.385500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1550</td>\n",
              "      <td>0.219000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1551</td>\n",
              "      <td>0.505200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1552</td>\n",
              "      <td>0.290200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1553</td>\n",
              "      <td>0.363300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1554</td>\n",
              "      <td>0.826900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1555</td>\n",
              "      <td>0.458300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1556</td>\n",
              "      <td>0.360600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1557</td>\n",
              "      <td>0.754300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1558</td>\n",
              "      <td>0.809100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1559</td>\n",
              "      <td>0.621600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1560</td>\n",
              "      <td>0.501500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1561</td>\n",
              "      <td>0.182500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1562</td>\n",
              "      <td>0.512800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1563</td>\n",
              "      <td>0.581000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1564</td>\n",
              "      <td>0.596700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1565</td>\n",
              "      <td>0.332300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1566</td>\n",
              "      <td>0.898400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1567</td>\n",
              "      <td>0.673700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1568</td>\n",
              "      <td>0.668000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1569</td>\n",
              "      <td>0.528300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1570</td>\n",
              "      <td>0.336200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1571</td>\n",
              "      <td>0.706300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1572</td>\n",
              "      <td>0.727700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1573</td>\n",
              "      <td>0.478800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1574</td>\n",
              "      <td>0.228700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1575</td>\n",
              "      <td>0.305100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1576</td>\n",
              "      <td>0.453500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1577</td>\n",
              "      <td>0.519300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1578</td>\n",
              "      <td>0.456800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1579</td>\n",
              "      <td>0.630500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1580</td>\n",
              "      <td>0.457100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1581</td>\n",
              "      <td>0.521500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1582</td>\n",
              "      <td>0.499900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1583</td>\n",
              "      <td>0.190200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1584</td>\n",
              "      <td>0.269300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1585</td>\n",
              "      <td>0.271600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1586</td>\n",
              "      <td>1.004100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1587</td>\n",
              "      <td>0.636900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1588</td>\n",
              "      <td>1.027300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1589</td>\n",
              "      <td>0.444500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1590</td>\n",
              "      <td>0.434300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1591</td>\n",
              "      <td>0.637900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1592</td>\n",
              "      <td>0.743300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1593</td>\n",
              "      <td>0.610900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1594</td>\n",
              "      <td>0.383700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1595</td>\n",
              "      <td>0.414600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1596</td>\n",
              "      <td>0.214000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1597</td>\n",
              "      <td>0.366300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1598</td>\n",
              "      <td>0.532100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1599</td>\n",
              "      <td>0.369500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.683600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1601</td>\n",
              "      <td>0.278900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1602</td>\n",
              "      <td>0.655500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1603</td>\n",
              "      <td>0.809500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1604</td>\n",
              "      <td>1.291100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1605</td>\n",
              "      <td>0.505200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1606</td>\n",
              "      <td>0.836400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1607</td>\n",
              "      <td>0.619300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1608</td>\n",
              "      <td>0.742000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1609</td>\n",
              "      <td>0.437900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1610</td>\n",
              "      <td>0.685400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1611</td>\n",
              "      <td>0.609900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1612</td>\n",
              "      <td>0.255400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1613</td>\n",
              "      <td>0.874700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1614</td>\n",
              "      <td>0.441200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1615</td>\n",
              "      <td>0.347500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1616</td>\n",
              "      <td>0.379700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1617</td>\n",
              "      <td>0.262000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1618</td>\n",
              "      <td>0.405000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1619</td>\n",
              "      <td>0.533600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1620</td>\n",
              "      <td>0.420800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1621</td>\n",
              "      <td>0.290100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1622</td>\n",
              "      <td>0.740100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1623</td>\n",
              "      <td>0.372500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1624</td>\n",
              "      <td>0.355800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1625</td>\n",
              "      <td>0.374300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1626</td>\n",
              "      <td>0.426600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1627</td>\n",
              "      <td>0.890200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1628</td>\n",
              "      <td>0.253200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1629</td>\n",
              "      <td>0.253500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1630</td>\n",
              "      <td>0.534500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1631</td>\n",
              "      <td>0.458400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1632</td>\n",
              "      <td>0.555200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1633</td>\n",
              "      <td>0.589700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1634</td>\n",
              "      <td>0.318200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1635</td>\n",
              "      <td>0.412300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1636</td>\n",
              "      <td>0.286100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1637</td>\n",
              "      <td>0.603300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1638</td>\n",
              "      <td>0.341200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1639</td>\n",
              "      <td>0.613700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1640</td>\n",
              "      <td>0.251500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1641</td>\n",
              "      <td>0.442900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1642</td>\n",
              "      <td>0.395300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1643</td>\n",
              "      <td>0.305500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1644</td>\n",
              "      <td>0.375500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1645</td>\n",
              "      <td>0.638900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1646</td>\n",
              "      <td>0.308400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1647</td>\n",
              "      <td>0.748100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1648</td>\n",
              "      <td>0.844800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1649</td>\n",
              "      <td>0.380100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>0.351900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1651</td>\n",
              "      <td>0.544900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1652</td>\n",
              "      <td>0.324100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1653</td>\n",
              "      <td>1.069700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1654</td>\n",
              "      <td>0.329800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1655</td>\n",
              "      <td>0.361400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1656</td>\n",
              "      <td>0.463000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1657</td>\n",
              "      <td>0.261300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1658</td>\n",
              "      <td>0.301000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1659</td>\n",
              "      <td>0.356600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1660</td>\n",
              "      <td>0.380500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1661</td>\n",
              "      <td>0.390700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1662</td>\n",
              "      <td>0.689700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1663</td>\n",
              "      <td>0.530100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1664</td>\n",
              "      <td>0.492000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1665</td>\n",
              "      <td>0.824300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1666</td>\n",
              "      <td>0.671500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1667</td>\n",
              "      <td>0.378300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1668</td>\n",
              "      <td>0.201900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1669</td>\n",
              "      <td>0.692200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1670</td>\n",
              "      <td>0.503600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1671</td>\n",
              "      <td>0.384000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1672</td>\n",
              "      <td>0.443100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1673</td>\n",
              "      <td>0.461200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1674</td>\n",
              "      <td>0.653600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1675</td>\n",
              "      <td>0.493800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1676</td>\n",
              "      <td>0.393500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1677</td>\n",
              "      <td>0.185100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1678</td>\n",
              "      <td>0.708600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1679</td>\n",
              "      <td>0.622400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1680</td>\n",
              "      <td>0.475600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1681</td>\n",
              "      <td>0.564900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1682</td>\n",
              "      <td>0.633200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1683</td>\n",
              "      <td>0.604200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1684</td>\n",
              "      <td>0.449200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1685</td>\n",
              "      <td>0.516700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1686</td>\n",
              "      <td>0.705200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1687</td>\n",
              "      <td>0.672000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1688</td>\n",
              "      <td>0.564300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1689</td>\n",
              "      <td>0.445500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1690</td>\n",
              "      <td>0.379300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1691</td>\n",
              "      <td>0.534500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1692</td>\n",
              "      <td>0.296500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1693</td>\n",
              "      <td>0.290000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1694</td>\n",
              "      <td>0.309500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1695</td>\n",
              "      <td>0.186200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1696</td>\n",
              "      <td>0.316100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1697</td>\n",
              "      <td>0.395500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1698</td>\n",
              "      <td>0.405200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1699</td>\n",
              "      <td>0.385900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.326200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1701</td>\n",
              "      <td>0.222800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1702</td>\n",
              "      <td>0.367700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1703</td>\n",
              "      <td>0.184000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1704</td>\n",
              "      <td>0.761400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1705</td>\n",
              "      <td>0.709400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1706</td>\n",
              "      <td>0.289200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1707</td>\n",
              "      <td>0.596000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1708</td>\n",
              "      <td>0.410200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1709</td>\n",
              "      <td>0.346200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1710</td>\n",
              "      <td>0.998300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1711</td>\n",
              "      <td>0.211600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1712</td>\n",
              "      <td>0.647300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1713</td>\n",
              "      <td>0.273200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1714</td>\n",
              "      <td>0.454000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1715</td>\n",
              "      <td>0.247000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1716</td>\n",
              "      <td>0.434000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1717</td>\n",
              "      <td>0.543400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1718</td>\n",
              "      <td>0.326800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1719</td>\n",
              "      <td>0.427100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1720</td>\n",
              "      <td>0.330600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1721</td>\n",
              "      <td>0.398300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1722</td>\n",
              "      <td>0.311500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1723</td>\n",
              "      <td>0.762500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1724</td>\n",
              "      <td>0.651700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1725</td>\n",
              "      <td>0.398800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1726</td>\n",
              "      <td>0.204800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1727</td>\n",
              "      <td>0.889400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1728</td>\n",
              "      <td>0.602200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1729</td>\n",
              "      <td>0.495200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1730</td>\n",
              "      <td>0.418400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1731</td>\n",
              "      <td>0.693400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1732</td>\n",
              "      <td>0.387400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1733</td>\n",
              "      <td>0.156100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1734</td>\n",
              "      <td>0.380500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1735</td>\n",
              "      <td>0.166100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1736</td>\n",
              "      <td>0.334800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1737</td>\n",
              "      <td>0.321500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1738</td>\n",
              "      <td>0.271200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1739</td>\n",
              "      <td>0.462500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1740</td>\n",
              "      <td>0.488600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1741</td>\n",
              "      <td>0.431200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1742</td>\n",
              "      <td>0.523900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1743</td>\n",
              "      <td>0.608600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1744</td>\n",
              "      <td>0.593300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1745</td>\n",
              "      <td>0.415900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1746</td>\n",
              "      <td>0.344400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1747</td>\n",
              "      <td>0.200400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1748</td>\n",
              "      <td>1.115200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1749</td>\n",
              "      <td>0.391200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>0.364900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1751</td>\n",
              "      <td>0.512300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1752</td>\n",
              "      <td>0.598400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1753</td>\n",
              "      <td>0.509600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1754</td>\n",
              "      <td>0.349400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1755</td>\n",
              "      <td>0.422200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1756</td>\n",
              "      <td>0.726700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1757</td>\n",
              "      <td>0.260400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1758</td>\n",
              "      <td>0.371100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1759</td>\n",
              "      <td>0.447600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1760</td>\n",
              "      <td>0.447000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1761</td>\n",
              "      <td>0.268000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1762</td>\n",
              "      <td>0.255600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1763</td>\n",
              "      <td>0.586500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1764</td>\n",
              "      <td>0.813600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1765</td>\n",
              "      <td>0.504500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1766</td>\n",
              "      <td>0.389300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1767</td>\n",
              "      <td>0.612900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1768</td>\n",
              "      <td>0.524600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1769</td>\n",
              "      <td>0.197200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1770</td>\n",
              "      <td>0.660100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1771</td>\n",
              "      <td>0.446000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1772</td>\n",
              "      <td>0.318200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1773</td>\n",
              "      <td>0.287800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1774</td>\n",
              "      <td>0.475900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1775</td>\n",
              "      <td>0.488500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1776</td>\n",
              "      <td>0.520900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1777</td>\n",
              "      <td>0.495200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1778</td>\n",
              "      <td>0.383900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1779</td>\n",
              "      <td>0.420000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1780</td>\n",
              "      <td>0.422500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1781</td>\n",
              "      <td>0.421000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1782</td>\n",
              "      <td>0.405700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1783</td>\n",
              "      <td>0.275800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1784</td>\n",
              "      <td>0.279800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1785</td>\n",
              "      <td>0.419200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1786</td>\n",
              "      <td>0.290600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1787</td>\n",
              "      <td>0.395500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1788</td>\n",
              "      <td>0.456700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1789</td>\n",
              "      <td>0.209000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1790</td>\n",
              "      <td>0.335800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1791</td>\n",
              "      <td>0.336600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1792</td>\n",
              "      <td>0.696300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1793</td>\n",
              "      <td>0.498200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1794</td>\n",
              "      <td>0.269900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1795</td>\n",
              "      <td>0.594000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1796</td>\n",
              "      <td>0.380300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1797</td>\n",
              "      <td>0.293200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1798</td>\n",
              "      <td>0.599100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1799</td>\n",
              "      <td>0.525000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.810600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1801</td>\n",
              "      <td>0.382900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1802</td>\n",
              "      <td>0.410700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1803</td>\n",
              "      <td>0.671100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1804</td>\n",
              "      <td>0.287500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1805</td>\n",
              "      <td>0.313300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1806</td>\n",
              "      <td>0.420300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1807</td>\n",
              "      <td>0.482700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1808</td>\n",
              "      <td>0.523600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1809</td>\n",
              "      <td>0.823500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1810</td>\n",
              "      <td>0.771900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1811</td>\n",
              "      <td>0.297900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1812</td>\n",
              "      <td>0.330300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1813</td>\n",
              "      <td>0.648700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1814</td>\n",
              "      <td>0.200700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1815</td>\n",
              "      <td>0.424600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1816</td>\n",
              "      <td>0.719200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1817</td>\n",
              "      <td>0.324900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1818</td>\n",
              "      <td>0.704500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1819</td>\n",
              "      <td>0.498400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1820</td>\n",
              "      <td>0.704500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1821</td>\n",
              "      <td>0.259400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1822</td>\n",
              "      <td>0.476900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1823</td>\n",
              "      <td>0.274000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1824</td>\n",
              "      <td>0.364600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1825</td>\n",
              "      <td>0.457400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1826</td>\n",
              "      <td>0.371000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1827</td>\n",
              "      <td>0.413700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1828</td>\n",
              "      <td>0.328700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1829</td>\n",
              "      <td>0.592500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1830</td>\n",
              "      <td>0.356100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1831</td>\n",
              "      <td>0.478300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1832</td>\n",
              "      <td>0.364000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1833</td>\n",
              "      <td>0.553700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1834</td>\n",
              "      <td>0.321700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1835</td>\n",
              "      <td>0.372800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1836</td>\n",
              "      <td>0.494300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1837</td>\n",
              "      <td>0.316100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1838</td>\n",
              "      <td>0.278700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1839</td>\n",
              "      <td>0.384400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1840</td>\n",
              "      <td>0.838900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1841</td>\n",
              "      <td>0.652500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1842</td>\n",
              "      <td>0.665000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1843</td>\n",
              "      <td>1.177900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1844</td>\n",
              "      <td>0.280000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1845</td>\n",
              "      <td>0.444700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1846</td>\n",
              "      <td>0.525200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1847</td>\n",
              "      <td>0.238200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1848</td>\n",
              "      <td>0.574500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1849</td>\n",
              "      <td>0.316900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1850</td>\n",
              "      <td>0.283000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1851</td>\n",
              "      <td>0.453200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1852</td>\n",
              "      <td>0.306200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1853</td>\n",
              "      <td>0.484000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1854</td>\n",
              "      <td>0.554300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1855</td>\n",
              "      <td>0.475600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1856</td>\n",
              "      <td>0.242800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1857</td>\n",
              "      <td>0.759400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1858</td>\n",
              "      <td>0.243800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1859</td>\n",
              "      <td>0.752300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1860</td>\n",
              "      <td>0.371800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1861</td>\n",
              "      <td>0.346400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1862</td>\n",
              "      <td>0.311900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1863</td>\n",
              "      <td>0.248800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1864</td>\n",
              "      <td>0.242300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1865</td>\n",
              "      <td>0.449900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1866</td>\n",
              "      <td>0.215400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1867</td>\n",
              "      <td>0.542600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1868</td>\n",
              "      <td>0.396300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1869</td>\n",
              "      <td>0.652700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1870</td>\n",
              "      <td>0.730700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1871</td>\n",
              "      <td>0.584200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1872</td>\n",
              "      <td>0.379300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1873</td>\n",
              "      <td>0.466200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1874</td>\n",
              "      <td>0.911000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1875</td>\n",
              "      <td>1.073300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1876</td>\n",
              "      <td>0.860700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1877</td>\n",
              "      <td>0.353400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1878</td>\n",
              "      <td>0.749400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1879</td>\n",
              "      <td>0.785100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1880</td>\n",
              "      <td>0.638400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1881</td>\n",
              "      <td>0.673200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1882</td>\n",
              "      <td>0.232700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1883</td>\n",
              "      <td>0.274600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1884</td>\n",
              "      <td>0.321900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1885</td>\n",
              "      <td>0.375300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1886</td>\n",
              "      <td>0.945500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1887</td>\n",
              "      <td>0.296900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1888</td>\n",
              "      <td>0.245200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1889</td>\n",
              "      <td>0.598200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1890</td>\n",
              "      <td>0.216800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1891</td>\n",
              "      <td>0.550800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1892</td>\n",
              "      <td>0.810800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1893</td>\n",
              "      <td>0.588600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1894</td>\n",
              "      <td>0.445500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1895</td>\n",
              "      <td>0.540200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1896</td>\n",
              "      <td>0.440400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1897</td>\n",
              "      <td>0.465600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1898</td>\n",
              "      <td>0.677400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1899</td>\n",
              "      <td>0.727000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.606500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1901</td>\n",
              "      <td>0.611400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1902</td>\n",
              "      <td>0.274400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1903</td>\n",
              "      <td>0.875200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1904</td>\n",
              "      <td>0.489200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1905</td>\n",
              "      <td>0.434100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1906</td>\n",
              "      <td>0.408600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1907</td>\n",
              "      <td>0.584400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1908</td>\n",
              "      <td>0.439800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1909</td>\n",
              "      <td>0.585700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1910</td>\n",
              "      <td>0.489900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1911</td>\n",
              "      <td>0.217600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1912</td>\n",
              "      <td>0.619400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1913</td>\n",
              "      <td>0.257700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1914</td>\n",
              "      <td>0.224000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1915</td>\n",
              "      <td>0.238000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1916</td>\n",
              "      <td>0.606100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1917</td>\n",
              "      <td>0.413000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1918</td>\n",
              "      <td>0.836200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1919</td>\n",
              "      <td>0.276800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1920</td>\n",
              "      <td>0.470300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1921</td>\n",
              "      <td>0.337600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1922</td>\n",
              "      <td>0.281800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1923</td>\n",
              "      <td>0.994100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1924</td>\n",
              "      <td>0.636600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1925</td>\n",
              "      <td>0.739500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1926</td>\n",
              "      <td>0.410700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1927</td>\n",
              "      <td>0.266700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1928</td>\n",
              "      <td>0.515900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1929</td>\n",
              "      <td>0.294800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1930</td>\n",
              "      <td>0.729300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1931</td>\n",
              "      <td>0.516500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1932</td>\n",
              "      <td>0.653700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1933</td>\n",
              "      <td>0.713700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1934</td>\n",
              "      <td>0.665900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1935</td>\n",
              "      <td>0.844600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1936</td>\n",
              "      <td>0.472000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1937</td>\n",
              "      <td>0.406700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1938</td>\n",
              "      <td>0.416600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1939</td>\n",
              "      <td>0.319800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1940</td>\n",
              "      <td>0.516100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1941</td>\n",
              "      <td>0.828000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1942</td>\n",
              "      <td>0.354500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1943</td>\n",
              "      <td>0.931700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1944</td>\n",
              "      <td>0.542300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1945</td>\n",
              "      <td>0.485000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1946</td>\n",
              "      <td>0.383700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1947</td>\n",
              "      <td>0.388800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1948</td>\n",
              "      <td>0.452000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1949</td>\n",
              "      <td>0.689600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>0.505100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1951</td>\n",
              "      <td>0.332200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1952</td>\n",
              "      <td>0.325400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1953</td>\n",
              "      <td>0.524300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1954</td>\n",
              "      <td>0.464900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1955</td>\n",
              "      <td>0.587100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1956</td>\n",
              "      <td>0.526900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1957</td>\n",
              "      <td>0.425700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1958</td>\n",
              "      <td>0.650900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1959</td>\n",
              "      <td>0.412300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1960</td>\n",
              "      <td>0.853300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1961</td>\n",
              "      <td>0.253500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1962</td>\n",
              "      <td>0.592000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1963</td>\n",
              "      <td>0.436300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1964</td>\n",
              "      <td>0.438800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1965</td>\n",
              "      <td>0.531500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1966</td>\n",
              "      <td>0.719800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1967</td>\n",
              "      <td>0.833100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1968</td>\n",
              "      <td>0.330400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1969</td>\n",
              "      <td>0.420700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1970</td>\n",
              "      <td>0.727400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1971</td>\n",
              "      <td>0.341900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1972</td>\n",
              "      <td>0.535800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1973</td>\n",
              "      <td>0.325600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1974</td>\n",
              "      <td>0.332900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1975</td>\n",
              "      <td>0.466600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1976</td>\n",
              "      <td>0.668300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1977</td>\n",
              "      <td>0.589800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1978</td>\n",
              "      <td>0.364700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1979</td>\n",
              "      <td>0.765600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1980</td>\n",
              "      <td>0.220300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1981</td>\n",
              "      <td>0.413800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1982</td>\n",
              "      <td>0.233300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1983</td>\n",
              "      <td>0.399700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1984</td>\n",
              "      <td>0.556900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1985</td>\n",
              "      <td>0.557400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1986</td>\n",
              "      <td>0.666400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1987</td>\n",
              "      <td>0.401900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1988</td>\n",
              "      <td>0.822000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1989</td>\n",
              "      <td>0.577500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1990</td>\n",
              "      <td>0.461500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1991</td>\n",
              "      <td>0.377200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1992</td>\n",
              "      <td>0.725400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1993</td>\n",
              "      <td>0.335500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1994</td>\n",
              "      <td>1.019100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1995</td>\n",
              "      <td>0.206900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1996</td>\n",
              "      <td>0.442500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1997</td>\n",
              "      <td>0.607700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1998</td>\n",
              "      <td>0.541200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1999</td>\n",
              "      <td>0.476500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.331500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2001</td>\n",
              "      <td>0.389200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2002</td>\n",
              "      <td>0.734200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2003</td>\n",
              "      <td>1.022000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2004</td>\n",
              "      <td>0.227100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2005</td>\n",
              "      <td>0.109900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2006</td>\n",
              "      <td>0.344800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2007</td>\n",
              "      <td>0.382900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2008</td>\n",
              "      <td>0.434000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2009</td>\n",
              "      <td>0.272700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2010</td>\n",
              "      <td>0.313400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2011</td>\n",
              "      <td>0.885000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2012</td>\n",
              "      <td>0.467300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2013</td>\n",
              "      <td>0.268400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2014</td>\n",
              "      <td>0.647300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2015</td>\n",
              "      <td>0.298300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2016</td>\n",
              "      <td>0.411900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2017</td>\n",
              "      <td>0.809500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2018</td>\n",
              "      <td>0.491800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2019</td>\n",
              "      <td>0.605400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2020</td>\n",
              "      <td>0.674900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2021</td>\n",
              "      <td>0.391900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2022</td>\n",
              "      <td>0.417600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2023</td>\n",
              "      <td>0.398500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2024</td>\n",
              "      <td>0.436200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2025</td>\n",
              "      <td>0.397200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2026</td>\n",
              "      <td>0.470700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2027</td>\n",
              "      <td>0.566600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2028</td>\n",
              "      <td>0.181400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2029</td>\n",
              "      <td>0.780400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2030</td>\n",
              "      <td>0.416400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2031</td>\n",
              "      <td>0.714800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2032</td>\n",
              "      <td>0.760900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2033</td>\n",
              "      <td>0.409600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2034</td>\n",
              "      <td>0.464400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2035</td>\n",
              "      <td>0.884200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2036</td>\n",
              "      <td>0.236100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2037</td>\n",
              "      <td>0.619500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2038</td>\n",
              "      <td>0.364800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2039</td>\n",
              "      <td>0.418900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2040</td>\n",
              "      <td>0.498600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2041</td>\n",
              "      <td>0.995000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2042</td>\n",
              "      <td>0.356700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2043</td>\n",
              "      <td>0.555100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2044</td>\n",
              "      <td>0.456700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2045</td>\n",
              "      <td>0.158400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2046</td>\n",
              "      <td>0.357900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2047</td>\n",
              "      <td>0.618000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2048</td>\n",
              "      <td>0.347500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2049</td>\n",
              "      <td>0.551100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2050</td>\n",
              "      <td>0.354600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2051</td>\n",
              "      <td>0.431300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2052</td>\n",
              "      <td>0.224500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2053</td>\n",
              "      <td>0.479100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2054</td>\n",
              "      <td>0.448400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2055</td>\n",
              "      <td>0.872700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2056</td>\n",
              "      <td>1.017600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2057</td>\n",
              "      <td>0.256700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2058</td>\n",
              "      <td>0.208900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2059</td>\n",
              "      <td>0.421500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2060</td>\n",
              "      <td>0.702000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2061</td>\n",
              "      <td>0.378200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2062</td>\n",
              "      <td>0.606700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2063</td>\n",
              "      <td>0.390300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2064</td>\n",
              "      <td>0.339300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2065</td>\n",
              "      <td>0.673400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2066</td>\n",
              "      <td>0.726500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2067</td>\n",
              "      <td>0.297900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2068</td>\n",
              "      <td>0.304400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2069</td>\n",
              "      <td>0.338100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2070</td>\n",
              "      <td>0.531300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2071</td>\n",
              "      <td>0.661000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2072</td>\n",
              "      <td>0.670100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2073</td>\n",
              "      <td>0.480900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2074</td>\n",
              "      <td>0.868300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2075</td>\n",
              "      <td>0.200500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2076</td>\n",
              "      <td>0.350900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2077</td>\n",
              "      <td>0.952400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2078</td>\n",
              "      <td>0.434100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2079</td>\n",
              "      <td>0.528200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2080</td>\n",
              "      <td>0.836800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2081</td>\n",
              "      <td>0.624500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2082</td>\n",
              "      <td>0.398800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2083</td>\n",
              "      <td>0.553900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2084</td>\n",
              "      <td>0.476100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2085</td>\n",
              "      <td>0.430000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2086</td>\n",
              "      <td>0.346200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2087</td>\n",
              "      <td>0.159600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2088</td>\n",
              "      <td>0.876600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2089</td>\n",
              "      <td>0.803800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2090</td>\n",
              "      <td>0.300500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2091</td>\n",
              "      <td>0.299800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2092</td>\n",
              "      <td>0.293200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2093</td>\n",
              "      <td>0.503300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2094</td>\n",
              "      <td>0.731100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2095</td>\n",
              "      <td>0.384300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2096</td>\n",
              "      <td>0.398600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2097</td>\n",
              "      <td>0.218800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2098</td>\n",
              "      <td>0.450900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2099</td>\n",
              "      <td>0.222900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.318600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2101</td>\n",
              "      <td>0.609800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2102</td>\n",
              "      <td>0.387400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2103</td>\n",
              "      <td>0.788000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2104</td>\n",
              "      <td>0.183400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2105</td>\n",
              "      <td>0.357000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2106</td>\n",
              "      <td>0.457500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2107</td>\n",
              "      <td>0.569300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2108</td>\n",
              "      <td>0.442500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2109</td>\n",
              "      <td>0.446200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2110</td>\n",
              "      <td>0.262200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2111</td>\n",
              "      <td>0.509500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2112</td>\n",
              "      <td>0.428500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2113</td>\n",
              "      <td>0.451300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2114</td>\n",
              "      <td>0.413300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2115</td>\n",
              "      <td>0.790700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2116</td>\n",
              "      <td>0.310700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2117</td>\n",
              "      <td>0.759500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2118</td>\n",
              "      <td>0.913900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2119</td>\n",
              "      <td>0.423900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2120</td>\n",
              "      <td>0.571600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2121</td>\n",
              "      <td>0.861900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2122</td>\n",
              "      <td>0.226400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2123</td>\n",
              "      <td>0.795400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2124</td>\n",
              "      <td>0.484900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2125</td>\n",
              "      <td>0.407100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2126</td>\n",
              "      <td>0.266200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2127</td>\n",
              "      <td>0.314400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2128</td>\n",
              "      <td>0.337200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2129</td>\n",
              "      <td>0.553600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2130</td>\n",
              "      <td>0.407200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2131</td>\n",
              "      <td>0.445400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2132</td>\n",
              "      <td>0.454800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2133</td>\n",
              "      <td>0.460900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2134</td>\n",
              "      <td>0.447100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2135</td>\n",
              "      <td>0.280000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2136</td>\n",
              "      <td>0.551000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2137</td>\n",
              "      <td>0.852000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2138</td>\n",
              "      <td>0.516500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2139</td>\n",
              "      <td>0.469700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2140</td>\n",
              "      <td>0.578000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2141</td>\n",
              "      <td>0.499200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2142</td>\n",
              "      <td>0.645800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2143</td>\n",
              "      <td>0.308800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2144</td>\n",
              "      <td>0.733400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2145</td>\n",
              "      <td>0.374900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2146</td>\n",
              "      <td>0.424600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2147</td>\n",
              "      <td>0.350900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2148</td>\n",
              "      <td>0.799700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2149</td>\n",
              "      <td>0.693900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2150</td>\n",
              "      <td>0.436300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2151</td>\n",
              "      <td>0.273000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2152</td>\n",
              "      <td>0.164300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2153</td>\n",
              "      <td>0.840500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2154</td>\n",
              "      <td>0.511900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2155</td>\n",
              "      <td>0.163200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2156</td>\n",
              "      <td>0.709000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2157</td>\n",
              "      <td>0.319600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2158</td>\n",
              "      <td>0.671300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2159</td>\n",
              "      <td>0.451600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2160</td>\n",
              "      <td>0.474500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2161</td>\n",
              "      <td>0.466700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2162</td>\n",
              "      <td>0.476700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2163</td>\n",
              "      <td>0.641100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2164</td>\n",
              "      <td>0.325700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2165</td>\n",
              "      <td>0.303800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2166</td>\n",
              "      <td>0.355000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2167</td>\n",
              "      <td>0.371300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2168</td>\n",
              "      <td>0.515700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2169</td>\n",
              "      <td>0.410100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2170</td>\n",
              "      <td>0.466300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2171</td>\n",
              "      <td>0.647000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2172</td>\n",
              "      <td>0.565500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2173</td>\n",
              "      <td>0.633100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2174</td>\n",
              "      <td>0.667600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2175</td>\n",
              "      <td>0.471000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2176</td>\n",
              "      <td>0.423700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2177</td>\n",
              "      <td>0.406900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2178</td>\n",
              "      <td>0.834400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2179</td>\n",
              "      <td>0.648400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2180</td>\n",
              "      <td>0.630900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2181</td>\n",
              "      <td>0.181300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2182</td>\n",
              "      <td>0.736100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2183</td>\n",
              "      <td>0.730200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2184</td>\n",
              "      <td>0.175500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2185</td>\n",
              "      <td>0.419000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2186</td>\n",
              "      <td>0.393400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2187</td>\n",
              "      <td>0.255600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2188</td>\n",
              "      <td>0.301300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2189</td>\n",
              "      <td>0.351200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2190</td>\n",
              "      <td>0.403900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2191</td>\n",
              "      <td>0.605300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2192</td>\n",
              "      <td>0.950800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2193</td>\n",
              "      <td>0.461300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2194</td>\n",
              "      <td>0.463300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2195</td>\n",
              "      <td>0.231900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2196</td>\n",
              "      <td>0.602200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2197</td>\n",
              "      <td>0.767800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2198</td>\n",
              "      <td>0.447700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2199</td>\n",
              "      <td>0.689300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.437900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2201</td>\n",
              "      <td>0.641400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2202</td>\n",
              "      <td>0.864200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2203</td>\n",
              "      <td>1.443300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2204</td>\n",
              "      <td>0.268700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2205</td>\n",
              "      <td>0.389600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2206</td>\n",
              "      <td>0.391000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2207</td>\n",
              "      <td>0.233900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2208</td>\n",
              "      <td>0.105400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2209</td>\n",
              "      <td>0.221400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2210</td>\n",
              "      <td>0.225500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2211</td>\n",
              "      <td>0.330000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2212</td>\n",
              "      <td>0.379900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2213</td>\n",
              "      <td>0.116200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2214</td>\n",
              "      <td>0.420700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2215</td>\n",
              "      <td>0.215100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2216</td>\n",
              "      <td>0.255500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2217</td>\n",
              "      <td>0.441600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2218</td>\n",
              "      <td>0.167900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2219</td>\n",
              "      <td>0.211300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2220</td>\n",
              "      <td>0.206900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2221</td>\n",
              "      <td>0.602900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2222</td>\n",
              "      <td>0.430100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2223</td>\n",
              "      <td>0.390400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2224</td>\n",
              "      <td>0.131300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2225</td>\n",
              "      <td>0.594100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2226</td>\n",
              "      <td>0.256800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2227</td>\n",
              "      <td>0.310600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2228</td>\n",
              "      <td>0.244000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2229</td>\n",
              "      <td>0.188800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2230</td>\n",
              "      <td>0.236100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2231</td>\n",
              "      <td>0.222500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2232</td>\n",
              "      <td>0.370100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2233</td>\n",
              "      <td>0.385700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2234</td>\n",
              "      <td>0.203800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2235</td>\n",
              "      <td>0.236800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2236</td>\n",
              "      <td>0.098800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2237</td>\n",
              "      <td>0.085900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2238</td>\n",
              "      <td>0.164100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2239</td>\n",
              "      <td>0.094400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2240</td>\n",
              "      <td>0.429900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2241</td>\n",
              "      <td>0.281300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2242</td>\n",
              "      <td>0.145700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2243</td>\n",
              "      <td>0.142200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2244</td>\n",
              "      <td>0.233900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2245</td>\n",
              "      <td>0.162000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2246</td>\n",
              "      <td>0.848100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2247</td>\n",
              "      <td>0.292400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2248</td>\n",
              "      <td>0.056900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2249</td>\n",
              "      <td>0.115100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>0.055300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2251</td>\n",
              "      <td>0.304900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2252</td>\n",
              "      <td>0.160500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2253</td>\n",
              "      <td>0.250700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2254</td>\n",
              "      <td>0.483800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2255</td>\n",
              "      <td>0.524600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2256</td>\n",
              "      <td>0.464600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2257</td>\n",
              "      <td>0.176000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2258</td>\n",
              "      <td>0.428900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2259</td>\n",
              "      <td>0.247000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2260</td>\n",
              "      <td>0.138100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2261</td>\n",
              "      <td>0.826500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2262</td>\n",
              "      <td>0.118600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2263</td>\n",
              "      <td>0.625700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2264</td>\n",
              "      <td>0.356700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2265</td>\n",
              "      <td>0.079500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2266</td>\n",
              "      <td>0.161200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2267</td>\n",
              "      <td>0.555900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2268</td>\n",
              "      <td>0.420700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2269</td>\n",
              "      <td>0.202900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2270</td>\n",
              "      <td>0.364500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2271</td>\n",
              "      <td>0.328800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2272</td>\n",
              "      <td>0.448400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2273</td>\n",
              "      <td>0.152000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2274</td>\n",
              "      <td>0.289300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2275</td>\n",
              "      <td>0.303400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2276</td>\n",
              "      <td>0.410600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2277</td>\n",
              "      <td>0.103200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2278</td>\n",
              "      <td>0.250300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2279</td>\n",
              "      <td>0.292900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2280</td>\n",
              "      <td>0.694100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2281</td>\n",
              "      <td>0.075700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2282</td>\n",
              "      <td>0.274700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2283</td>\n",
              "      <td>0.116500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2284</td>\n",
              "      <td>0.211400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2285</td>\n",
              "      <td>0.169700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2286</td>\n",
              "      <td>0.387200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2287</td>\n",
              "      <td>0.144100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2288</td>\n",
              "      <td>0.166700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2289</td>\n",
              "      <td>0.389100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2290</td>\n",
              "      <td>0.358700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2291</td>\n",
              "      <td>0.114400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2292</td>\n",
              "      <td>0.298600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2293</td>\n",
              "      <td>0.203200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2294</td>\n",
              "      <td>0.284100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2295</td>\n",
              "      <td>0.262000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2296</td>\n",
              "      <td>0.297800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2297</td>\n",
              "      <td>0.158900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2298</td>\n",
              "      <td>0.111200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2299</td>\n",
              "      <td>0.097700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.239500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2301</td>\n",
              "      <td>0.214700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2302</td>\n",
              "      <td>0.414800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2303</td>\n",
              "      <td>0.192000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2304</td>\n",
              "      <td>0.315900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2305</td>\n",
              "      <td>0.248600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2306</td>\n",
              "      <td>0.539600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2307</td>\n",
              "      <td>0.452500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2308</td>\n",
              "      <td>0.531400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2309</td>\n",
              "      <td>0.194300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2310</td>\n",
              "      <td>0.162300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2311</td>\n",
              "      <td>0.188800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2312</td>\n",
              "      <td>0.239200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2313</td>\n",
              "      <td>0.179100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2314</td>\n",
              "      <td>0.269400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2315</td>\n",
              "      <td>0.147700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2316</td>\n",
              "      <td>0.307200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2317</td>\n",
              "      <td>0.206700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2318</td>\n",
              "      <td>0.281500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2319</td>\n",
              "      <td>0.146200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2320</td>\n",
              "      <td>0.201200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2321</td>\n",
              "      <td>0.323900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2322</td>\n",
              "      <td>0.835000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2323</td>\n",
              "      <td>0.402200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2324</td>\n",
              "      <td>0.394400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2325</td>\n",
              "      <td>0.153300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2326</td>\n",
              "      <td>0.351100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2327</td>\n",
              "      <td>0.188600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2328</td>\n",
              "      <td>0.374000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2329</td>\n",
              "      <td>0.487200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2330</td>\n",
              "      <td>0.492000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2331</td>\n",
              "      <td>0.506700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2332</td>\n",
              "      <td>0.352100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2333</td>\n",
              "      <td>0.580800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2334</td>\n",
              "      <td>0.231600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2335</td>\n",
              "      <td>0.288400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2336</td>\n",
              "      <td>0.340300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2337</td>\n",
              "      <td>0.238800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2338</td>\n",
              "      <td>0.258300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2339</td>\n",
              "      <td>0.663300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2340</td>\n",
              "      <td>0.219700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2341</td>\n",
              "      <td>0.159900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2342</td>\n",
              "      <td>0.240200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2343</td>\n",
              "      <td>0.508800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2344</td>\n",
              "      <td>0.279000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2345</td>\n",
              "      <td>0.216600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2346</td>\n",
              "      <td>0.226500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2347</td>\n",
              "      <td>0.200700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2348</td>\n",
              "      <td>0.152200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2349</td>\n",
              "      <td>0.275200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2350</td>\n",
              "      <td>0.224000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2351</td>\n",
              "      <td>0.379900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2352</td>\n",
              "      <td>0.384600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2353</td>\n",
              "      <td>0.290200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2354</td>\n",
              "      <td>0.442800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2355</td>\n",
              "      <td>0.379700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2356</td>\n",
              "      <td>0.474500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2357</td>\n",
              "      <td>0.092300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2358</td>\n",
              "      <td>0.306300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2359</td>\n",
              "      <td>0.280400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2360</td>\n",
              "      <td>0.384400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2361</td>\n",
              "      <td>0.175900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2362</td>\n",
              "      <td>0.500500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2363</td>\n",
              "      <td>0.152200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2364</td>\n",
              "      <td>0.231800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2365</td>\n",
              "      <td>0.454000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2366</td>\n",
              "      <td>0.189000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2367</td>\n",
              "      <td>0.278200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2368</td>\n",
              "      <td>0.270000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2369</td>\n",
              "      <td>0.169600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2370</td>\n",
              "      <td>0.141600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2371</td>\n",
              "      <td>0.428200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2372</td>\n",
              "      <td>0.329500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2373</td>\n",
              "      <td>0.153300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2374</td>\n",
              "      <td>0.219700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2375</td>\n",
              "      <td>0.514300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2376</td>\n",
              "      <td>0.162200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2377</td>\n",
              "      <td>0.390800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2378</td>\n",
              "      <td>0.497300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2379</td>\n",
              "      <td>0.148800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2380</td>\n",
              "      <td>0.255100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2381</td>\n",
              "      <td>0.432200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2382</td>\n",
              "      <td>0.306400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2383</td>\n",
              "      <td>0.138200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2384</td>\n",
              "      <td>0.566100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2385</td>\n",
              "      <td>0.272400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2386</td>\n",
              "      <td>0.197000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2387</td>\n",
              "      <td>0.202300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2388</td>\n",
              "      <td>0.181800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2389</td>\n",
              "      <td>0.206200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2390</td>\n",
              "      <td>0.466100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2391</td>\n",
              "      <td>0.159200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2392</td>\n",
              "      <td>0.399600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2393</td>\n",
              "      <td>0.220800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2394</td>\n",
              "      <td>0.256900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2395</td>\n",
              "      <td>0.339900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2396</td>\n",
              "      <td>0.224000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2397</td>\n",
              "      <td>0.167900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2398</td>\n",
              "      <td>0.206600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2399</td>\n",
              "      <td>0.295500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.252200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2401</td>\n",
              "      <td>0.212800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2402</td>\n",
              "      <td>0.461500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2403</td>\n",
              "      <td>0.267700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2404</td>\n",
              "      <td>0.418200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2405</td>\n",
              "      <td>0.187900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2406</td>\n",
              "      <td>0.217600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2407</td>\n",
              "      <td>0.177500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2408</td>\n",
              "      <td>0.220000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2409</td>\n",
              "      <td>0.366300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2410</td>\n",
              "      <td>0.431500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2411</td>\n",
              "      <td>0.117600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2412</td>\n",
              "      <td>0.192300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2413</td>\n",
              "      <td>0.247000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2414</td>\n",
              "      <td>0.586500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2415</td>\n",
              "      <td>0.299600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2416</td>\n",
              "      <td>0.197500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2417</td>\n",
              "      <td>0.143200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2418</td>\n",
              "      <td>0.516800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2419</td>\n",
              "      <td>0.280800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2420</td>\n",
              "      <td>0.361100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2421</td>\n",
              "      <td>0.456700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2422</td>\n",
              "      <td>0.133900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2423</td>\n",
              "      <td>0.393400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2424</td>\n",
              "      <td>0.293600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2425</td>\n",
              "      <td>0.359500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2426</td>\n",
              "      <td>0.240100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2427</td>\n",
              "      <td>0.251100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2428</td>\n",
              "      <td>0.272000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2429</td>\n",
              "      <td>0.563700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2430</td>\n",
              "      <td>0.230000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2431</td>\n",
              "      <td>0.908600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2432</td>\n",
              "      <td>0.365400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2433</td>\n",
              "      <td>0.229500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2434</td>\n",
              "      <td>0.089200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2435</td>\n",
              "      <td>0.149100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2436</td>\n",
              "      <td>0.158900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2437</td>\n",
              "      <td>0.280300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2438</td>\n",
              "      <td>0.283000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2439</td>\n",
              "      <td>0.204500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2440</td>\n",
              "      <td>0.149900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2441</td>\n",
              "      <td>0.170100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2442</td>\n",
              "      <td>0.076500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2443</td>\n",
              "      <td>0.647400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2444</td>\n",
              "      <td>0.302000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2445</td>\n",
              "      <td>0.198500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2446</td>\n",
              "      <td>0.388600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2447</td>\n",
              "      <td>0.192100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2448</td>\n",
              "      <td>0.220600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2449</td>\n",
              "      <td>0.295200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2450</td>\n",
              "      <td>0.561800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2451</td>\n",
              "      <td>0.178100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2452</td>\n",
              "      <td>0.224700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2453</td>\n",
              "      <td>0.241800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2454</td>\n",
              "      <td>0.388900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2455</td>\n",
              "      <td>0.208100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2456</td>\n",
              "      <td>0.247500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2457</td>\n",
              "      <td>0.179200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2458</td>\n",
              "      <td>0.229700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2459</td>\n",
              "      <td>0.181900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2460</td>\n",
              "      <td>0.087800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2461</td>\n",
              "      <td>0.320200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2462</td>\n",
              "      <td>0.198300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2463</td>\n",
              "      <td>0.474000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2464</td>\n",
              "      <td>0.567400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2465</td>\n",
              "      <td>0.224700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2466</td>\n",
              "      <td>0.260100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2467</td>\n",
              "      <td>0.348300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2468</td>\n",
              "      <td>0.363400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2469</td>\n",
              "      <td>0.589900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2470</td>\n",
              "      <td>0.446600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2471</td>\n",
              "      <td>0.585500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2472</td>\n",
              "      <td>0.309900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2473</td>\n",
              "      <td>0.231500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2474</td>\n",
              "      <td>0.550700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2475</td>\n",
              "      <td>0.383700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2476</td>\n",
              "      <td>0.609700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2477</td>\n",
              "      <td>0.666900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2478</td>\n",
              "      <td>0.524900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2479</td>\n",
              "      <td>0.158700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2480</td>\n",
              "      <td>0.282000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2481</td>\n",
              "      <td>0.142900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2482</td>\n",
              "      <td>0.164600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2483</td>\n",
              "      <td>0.206800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2484</td>\n",
              "      <td>0.244500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2485</td>\n",
              "      <td>0.379700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2486</td>\n",
              "      <td>0.178600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2487</td>\n",
              "      <td>0.274800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2488</td>\n",
              "      <td>0.365000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2489</td>\n",
              "      <td>0.266700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2490</td>\n",
              "      <td>0.123300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2491</td>\n",
              "      <td>0.112200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2492</td>\n",
              "      <td>0.222500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2493</td>\n",
              "      <td>0.238800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> Step 780 metrics: {'loss': 1.165, 'grad_norm': 7.8125, 'learning_rate': 4.9401103485102435e-05, 'epoch': 1.0612244897959184}\n",
            "\n",
            ">>> Step 781 metrics: {'loss': 0.9499, 'grad_norm': 6.53125, 'learning_rate': 4.939955114338753e-05, 'epoch': 1.0625850340136054}\n",
            "\n",
            ">>> Step 782 metrics: {'loss': 1.0415, 'grad_norm': 9.4375, 'learning_rate': 4.939799681689171e-05, 'epoch': 1.0639455782312925}\n",
            "\n",
            ">>> Step 783 metrics: {'loss': 0.4725, 'grad_norm': 4.875, 'learning_rate': 4.93964405057414e-05, 'epoch': 1.0653061224489795}\n",
            "\n",
            ">>> Step 784 metrics: {'loss': 0.7234, 'grad_norm': 5.96875, 'learning_rate': 4.939488221006321e-05, 'epoch': 1.0666666666666667}\n",
            "\n",
            ">>> Step 785 metrics: {'loss': 0.6119, 'grad_norm': 6.1875, 'learning_rate': 4.939332192998389e-05, 'epoch': 1.0680272108843538}\n",
            "\n",
            ">>> Step 786 metrics: {'loss': 0.8553, 'grad_norm': 6.75, 'learning_rate': 4.9391759665630364e-05, 'epoch': 1.0693877551020408}\n",
            "\n",
            ">>> Step 787 metrics: {'loss': 0.5663, 'grad_norm': 11.8125, 'learning_rate': 4.9390195417129716e-05, 'epoch': 1.070748299319728}\n",
            "\n",
            ">>> Step 788 metrics: {'loss': 1.1275, 'grad_norm': 7.1875, 'learning_rate': 4.9388629184609194e-05, 'epoch': 1.0721088435374149}\n",
            "\n",
            ">>> Step 789 metrics: {'loss': 0.8596, 'grad_norm': 8.9375, 'learning_rate': 4.938706096819618e-05, 'epoch': 1.073469387755102}\n",
            "\n",
            ">>> Step 790 metrics: {'loss': 0.7988, 'grad_norm': 7.28125, 'learning_rate': 4.9385490768018274e-05, 'epoch': 1.0748299319727892}\n",
            "\n",
            ">>> Step 791 metrics: {'loss': 0.8516, 'grad_norm': 5.84375, 'learning_rate': 4.938391858420319e-05, 'epoch': 1.0761904761904761}\n",
            "\n",
            ">>> Step 792 metrics: {'loss': 0.9368, 'grad_norm': 8.5, 'learning_rate': 4.93823444168788e-05, 'epoch': 1.0775510204081633}\n",
            "\n",
            ">>> Step 793 metrics: {'loss': 0.8269, 'grad_norm': 5.21875, 'learning_rate': 4.9380768266173184e-05, 'epoch': 1.0789115646258503}\n",
            "\n",
            ">>> Step 794 metrics: {'loss': 0.5944, 'grad_norm': 5.65625, 'learning_rate': 4.9379190132214534e-05, 'epoch': 1.0802721088435374}\n",
            "\n",
            ">>> Step 795 metrics: {'loss': 0.6567, 'grad_norm': 6.0625, 'learning_rate': 4.937761001513123e-05, 'epoch': 1.0816326530612246}\n",
            "\n",
            ">>> Step 796 metrics: {'loss': 0.8787, 'grad_norm': 9.625, 'learning_rate': 4.9376027915051804e-05, 'epoch': 1.0829931972789115}\n",
            "\n",
            ">>> Step 797 metrics: {'loss': 0.6432, 'grad_norm': 6.3125, 'learning_rate': 4.937444383210496e-05, 'epoch': 1.0843537414965987}\n",
            "\n",
            ">>> Step 798 metrics: {'loss': 0.9691, 'grad_norm': 7.0, 'learning_rate': 4.937285776641953e-05, 'epoch': 1.0857142857142856}\n",
            "\n",
            ">>> Step 799 metrics: {'loss': 0.4185, 'grad_norm': 6.0625, 'learning_rate': 4.937126971812457e-05, 'epoch': 1.0870748299319728}\n",
            "\n",
            ">>> Step 800 metrics: {'loss': 0.3539, 'grad_norm': 5.40625, 'learning_rate': 4.936967968734924e-05, 'epoch': 1.08843537414966}\n",
            "\n",
            ">>> Step 801 metrics: {'loss': 0.7469, 'grad_norm': 6.65625, 'learning_rate': 4.9368087674222875e-05, 'epoch': 1.089795918367347}\n",
            "\n",
            ">>> Step 802 metrics: {'loss': 0.9315, 'grad_norm': 8.625, 'learning_rate': 4.936649367887498e-05, 'epoch': 1.091156462585034}\n",
            "\n",
            ">>> Step 803 metrics: {'loss': 0.5979, 'grad_norm': 6.34375, 'learning_rate': 4.936489770143523e-05, 'epoch': 1.092517006802721}\n",
            "\n",
            ">>> Step 804 metrics: {'loss': 0.8194, 'grad_norm': 8.25, 'learning_rate': 4.936329974203344e-05, 'epoch': 1.0938775510204082}\n",
            "\n",
            ">>> Step 805 metrics: {'loss': 0.7804, 'grad_norm': 7.28125, 'learning_rate': 4.93616998007996e-05, 'epoch': 1.0952380952380953}\n",
            "\n",
            ">>> Step 806 metrics: {'loss': 0.5799, 'grad_norm': 7.25, 'learning_rate': 4.9360097877863856e-05, 'epoch': 1.0965986394557823}\n",
            "\n",
            ">>> Step 807 metrics: {'loss': 0.6747, 'grad_norm': 6.0, 'learning_rate': 4.935849397335651e-05, 'epoch': 1.0979591836734695}\n",
            "\n",
            ">>> Step 808 metrics: {'loss': 0.9201, 'grad_norm': 6.71875, 'learning_rate': 4.935688808740805e-05, 'epoch': 1.0993197278911564}\n",
            "\n",
            ">>> Step 809 metrics: {'loss': 0.5152, 'grad_norm': 5.84375, 'learning_rate': 4.9355280220149084e-05, 'epoch': 1.1006802721088436}\n",
            "\n",
            ">>> Step 810 metrics: {'loss': 0.6307, 'grad_norm': 6.03125, 'learning_rate': 4.935367037171042e-05, 'epoch': 1.1020408163265305}\n",
            "\n",
            ">>> Step 811 metrics: {'loss': 0.5725, 'grad_norm': 5.90625, 'learning_rate': 4.9352058542223e-05, 'epoch': 1.1034013605442177}\n",
            "\n",
            ">>> Step 812 metrics: {'loss': 0.6352, 'grad_norm': 5.59375, 'learning_rate': 4.935044473181795e-05, 'epoch': 1.1047619047619048}\n",
            "\n",
            ">>> Step 813 metrics: {'loss': 0.5671, 'grad_norm': 6.09375, 'learning_rate': 4.934882894062654e-05, 'epoch': 1.1061224489795918}\n",
            "\n",
            ">>> Step 814 metrics: {'loss': 0.5135, 'grad_norm': 5.5, 'learning_rate': 4.9347211168780196e-05, 'epoch': 1.107482993197279}\n",
            "\n",
            ">>> Step 815 metrics: {'loss': 0.4441, 'grad_norm': 6.0625, 'learning_rate': 4.934559141641053e-05, 'epoch': 1.1088435374149659}\n",
            "\n",
            ">>> Step 816 metrics: {'loss': 0.3685, 'grad_norm': 5.34375, 'learning_rate': 4.934396968364929e-05, 'epoch': 1.110204081632653}\n",
            "\n",
            ">>> Step 817 metrics: {'loss': 0.6942, 'grad_norm': 7.84375, 'learning_rate': 4.934234597062841e-05, 'epoch': 1.1115646258503402}\n",
            "\n",
            ">>> Step 818 metrics: {'loss': 0.6499, 'grad_norm': 6.4375, 'learning_rate': 4.934072027747996e-05, 'epoch': 1.1129251700680272}\n",
            "\n",
            ">>> Step 819 metrics: {'loss': 0.9848, 'grad_norm': 7.90625, 'learning_rate': 4.933909260433618e-05, 'epoch': 1.1142857142857143}\n",
            "\n",
            ">>> Step 820 metrics: {'loss': 0.5437, 'grad_norm': 7.1875, 'learning_rate': 4.9337462951329485e-05, 'epoch': 1.1156462585034013}\n",
            "\n",
            ">>> Step 821 metrics: {'loss': 0.6391, 'grad_norm': 6.4375, 'learning_rate': 4.933583131859243e-05, 'epoch': 1.1170068027210884}\n",
            "\n",
            ">>> Step 822 metrics: {'loss': 0.4315, 'grad_norm': 6.3125, 'learning_rate': 4.9334197706257736e-05, 'epoch': 1.1183673469387756}\n",
            "\n",
            ">>> Step 823 metrics: {'loss': 0.5229, 'grad_norm': 6.0625, 'learning_rate': 4.93325621144583e-05, 'epoch': 1.1197278911564625}\n",
            "\n",
            ">>> Step 824 metrics: {'loss': 0.956, 'grad_norm': 8.4375, 'learning_rate': 4.933092454332716e-05, 'epoch': 1.1210884353741497}\n",
            "\n",
            ">>> Step 825 metrics: {'loss': 0.4052, 'grad_norm': 5.65625, 'learning_rate': 4.932928499299754e-05, 'epoch': 1.1224489795918366}\n",
            "\n",
            ">>> Step 826 metrics: {'loss': 0.7529, 'grad_norm': 6.0, 'learning_rate': 4.932764346360278e-05, 'epoch': 1.1238095238095238}\n",
            "\n",
            ">>> Step 827 metrics: {'loss': 0.3283, 'grad_norm': 5.28125, 'learning_rate': 4.932599995527644e-05, 'epoch': 1.125170068027211}\n",
            "\n",
            ">>> Step 828 metrics: {'loss': 0.8847, 'grad_norm': 10.3125, 'learning_rate': 4.932435446815219e-05, 'epoch': 1.126530612244898}\n",
            "\n",
            ">>> Step 829 metrics: {'loss': 0.8509, 'grad_norm': 6.75, 'learning_rate': 4.93227070023639e-05, 'epoch': 1.127891156462585}\n",
            "\n",
            ">>> Step 830 metrics: {'loss': 0.5669, 'grad_norm': 6.0625, 'learning_rate': 4.932105755804557e-05, 'epoch': 1.129251700680272}\n",
            "\n",
            ">>> Step 831 metrics: {'loss': 0.653, 'grad_norm': 7.46875, 'learning_rate': 4.931940613533137e-05, 'epoch': 1.1306122448979592}\n",
            "\n",
            ">>> Step 832 metrics: {'loss': 1.2253, 'grad_norm': 7.3125, 'learning_rate': 4.931775273435566e-05, 'epoch': 1.1319727891156464}\n",
            "\n",
            ">>> Step 833 metrics: {'loss': 0.7611, 'grad_norm': 6.34375, 'learning_rate': 4.931609735525291e-05, 'epoch': 1.1333333333333333}\n",
            "\n",
            ">>> Step 834 metrics: {'loss': 0.5252, 'grad_norm': 6.03125, 'learning_rate': 4.931443999815778e-05, 'epoch': 1.1346938775510205}\n",
            "\n",
            ">>> Step 835 metrics: {'loss': 0.665, 'grad_norm': 6.71875, 'learning_rate': 4.931278066320509e-05, 'epoch': 1.1360544217687074}\n",
            "\n",
            ">>> Step 836 metrics: {'loss': 0.6805, 'grad_norm': 6.375, 'learning_rate': 4.9311119350529835e-05, 'epoch': 1.1374149659863946}\n",
            "\n",
            ">>> Step 837 metrics: {'loss': 0.9134, 'grad_norm': 10.125, 'learning_rate': 4.930945606026713e-05, 'epoch': 1.1387755102040815}\n",
            "\n",
            ">>> Step 838 metrics: {'loss': 0.9777, 'grad_norm': 7.34375, 'learning_rate': 4.93077907925523e-05, 'epoch': 1.1401360544217687}\n",
            "\n",
            ">>> Step 839 metrics: {'loss': 0.5836, 'grad_norm': 5.78125, 'learning_rate': 4.930612354752078e-05, 'epoch': 1.1414965986394559}\n",
            "\n",
            ">>> Step 840 metrics: {'loss': 1.0923, 'grad_norm': 7.4375, 'learning_rate': 4.930445432530821e-05, 'epoch': 1.1428571428571428}\n",
            "\n",
            ">>> Step 841 metrics: {'loss': 0.4892, 'grad_norm': 6.0625, 'learning_rate': 4.9302783126050366e-05, 'epoch': 1.14421768707483}\n",
            "\n",
            ">>> Step 842 metrics: {'loss': 1.0297, 'grad_norm': 6.4375, 'learning_rate': 4.93011099498832e-05, 'epoch': 1.1455782312925171}\n",
            "\n",
            ">>> Step 843 metrics: {'loss': 0.8372, 'grad_norm': 7.40625, 'learning_rate': 4.92994347969428e-05, 'epoch': 1.146938775510204}\n",
            "\n",
            ">>> Step 844 metrics: {'loss': 1.2298, 'grad_norm': 6.8125, 'learning_rate': 4.929775766736544e-05, 'epoch': 1.1482993197278912}\n",
            "\n",
            ">>> Step 845 metrics: {'loss': 1.2272, 'grad_norm': 6.78125, 'learning_rate': 4.929607856128755e-05, 'epoch': 1.1496598639455782}\n",
            "\n",
            ">>> Step 846 metrics: {'loss': 0.4296, 'grad_norm': 5.40625, 'learning_rate': 4.929439747884572e-05, 'epoch': 1.1510204081632653}\n",
            "\n",
            ">>> Step 847 metrics: {'loss': 0.5788, 'grad_norm': 6.40625, 'learning_rate': 4.9292714420176685e-05, 'epoch': 1.1523809523809523}\n",
            "\n",
            ">>> Step 848 metrics: {'loss': 0.4341, 'grad_norm': 5.90625, 'learning_rate': 4.9291029385417364e-05, 'epoch': 1.1537414965986394}\n",
            "\n",
            ">>> Step 849 metrics: {'loss': 0.8309, 'grad_norm': 7.25, 'learning_rate': 4.9289342374704815e-05, 'epoch': 1.1551020408163266}\n",
            "\n",
            ">>> Step 850 metrics: {'loss': 0.5485, 'grad_norm': 7.03125, 'learning_rate': 4.928765338817628e-05, 'epoch': 1.1564625850340136}\n",
            "\n",
            ">>> Step 851 metrics: {'loss': 0.5758, 'grad_norm': 6.28125, 'learning_rate': 4.928596242596915e-05, 'epoch': 1.1578231292517007}\n",
            "\n",
            ">>> Step 852 metrics: {'loss': 0.3907, 'grad_norm': 5.125, 'learning_rate': 4.9284269488220955e-05, 'epoch': 1.1591836734693877}\n",
            "\n",
            ">>> Step 853 metrics: {'loss': 0.6272, 'grad_norm': 7.03125, 'learning_rate': 4.9282574575069435e-05, 'epoch': 1.1605442176870748}\n",
            "\n",
            ">>> Step 854 metrics: {'loss': 0.6949, 'grad_norm': 6.59375, 'learning_rate': 4.928087768665245e-05, 'epoch': 1.161904761904762}\n",
            "\n",
            ">>> Step 855 metrics: {'loss': 0.6455, 'grad_norm': 6.09375, 'learning_rate': 4.927917882310802e-05, 'epoch': 1.163265306122449}\n",
            "\n",
            ">>> Step 856 metrics: {'loss': 0.7633, 'grad_norm': 10.6875, 'learning_rate': 4.9277477984574375e-05, 'epoch': 1.164625850340136}\n",
            "\n",
            ">>> Step 857 metrics: {'loss': 0.5964, 'grad_norm': 6.3125, 'learning_rate': 4.927577517118983e-05, 'epoch': 1.165986394557823}\n",
            "\n",
            ">>> Step 858 metrics: {'loss': 0.8223, 'grad_norm': 6.0, 'learning_rate': 4.927407038309292e-05, 'epoch': 1.1673469387755102}\n",
            "\n",
            ">>> Step 859 metrics: {'loss': 0.8137, 'grad_norm': 6.25, 'learning_rate': 4.9272363620422325e-05, 'epoch': 1.1687074829931974}\n",
            "\n",
            ">>> Step 860 metrics: {'loss': 1.3608, 'grad_norm': 12.125, 'learning_rate': 4.927065488331687e-05, 'epoch': 1.1700680272108843}\n",
            "\n",
            ">>> Step 861 metrics: {'loss': 0.8748, 'grad_norm': 5.78125, 'learning_rate': 4.9268944171915554e-05, 'epoch': 1.1714285714285715}\n",
            "\n",
            ">>> Step 862 metrics: {'loss': 0.5426, 'grad_norm': 6.3125, 'learning_rate': 4.926723148635754e-05, 'epoch': 1.1727891156462584}\n",
            "\n",
            ">>> Step 863 metrics: {'loss': 0.6511, 'grad_norm': 6.40625, 'learning_rate': 4.926551682678215e-05, 'epoch': 1.1741496598639456}\n",
            "\n",
            ">>> Step 864 metrics: {'loss': 0.748, 'grad_norm': 6.90625, 'learning_rate': 4.9263800193328854e-05, 'epoch': 1.1755102040816325}\n",
            "\n",
            ">>> Step 865 metrics: {'loss': 0.7708, 'grad_norm': 7.0, 'learning_rate': 4.92620815861373e-05, 'epoch': 1.1768707482993197}\n",
            "\n",
            ">>> Step 866 metrics: {'loss': 1.0557, 'grad_norm': 7.1875, 'learning_rate': 4.926036100534728e-05, 'epoch': 1.1782312925170069}\n",
            "\n",
            ">>> Step 867 metrics: {'loss': 1.1682, 'grad_norm': 8.4375, 'learning_rate': 4.925863845109876e-05, 'epoch': 1.1795918367346938}\n",
            "\n",
            ">>> Step 868 metrics: {'loss': 0.634, 'grad_norm': 6.5625, 'learning_rate': 4.925691392353185e-05, 'epoch': 1.180952380952381}\n",
            "\n",
            ">>> Step 869 metrics: {'loss': 0.6871, 'grad_norm': 6.9375, 'learning_rate': 4.9255187422786846e-05, 'epoch': 1.1823129251700681}\n",
            "\n",
            ">>> Step 870 metrics: {'loss': 0.5978, 'grad_norm': 6.5, 'learning_rate': 4.9253458949004185e-05, 'epoch': 1.183673469387755}\n",
            "\n",
            ">>> Step 871 metrics: {'loss': 1.2009, 'grad_norm': 7.4375, 'learning_rate': 4.925172850232447e-05, 'epoch': 1.1850340136054422}\n",
            "\n",
            ">>> Step 872 metrics: {'loss': 1.0081, 'grad_norm': 6.875, 'learning_rate': 4.924999608288846e-05, 'epoch': 1.1863945578231292}\n",
            "\n",
            ">>> Step 873 metrics: {'loss': 0.709, 'grad_norm': 5.78125, 'learning_rate': 4.9248261690837084e-05, 'epoch': 1.1877551020408164}\n",
            "\n",
            ">>> Step 874 metrics: {'loss': 0.6334, 'grad_norm': 5.96875, 'learning_rate': 4.924652532631142e-05, 'epoch': 1.1891156462585033}\n",
            "\n",
            ">>> Step 875 metrics: {'loss': 0.8919, 'grad_norm': 6.46875, 'learning_rate': 4.9244786989452716e-05, 'epoch': 1.1904761904761905}\n",
            "\n",
            ">>> Step 876 metrics: {'loss': 0.4899, 'grad_norm': 5.875, 'learning_rate': 4.9243046680402385e-05, 'epoch': 1.1918367346938776}\n",
            "\n",
            ">>> Step 877 metrics: {'loss': 0.7185, 'grad_norm': 6.84375, 'learning_rate': 4.924130439930198e-05, 'epoch': 1.1931972789115646}\n",
            "\n",
            ">>> Step 878 metrics: {'loss': 0.892, 'grad_norm': 6.8125, 'learning_rate': 4.923956014629324e-05, 'epoch': 1.1945578231292517}\n",
            "\n",
            ">>> Step 879 metrics: {'loss': 0.6284, 'grad_norm': 7.5625, 'learning_rate': 4.923781392151804e-05, 'epoch': 1.1959183673469387}\n",
            "\n",
            ">>> Step 880 metrics: {'loss': 0.3936, 'grad_norm': 5.40625, 'learning_rate': 4.9236065725118416e-05, 'epoch': 1.1972789115646258}\n",
            "\n",
            ">>> Step 881 metrics: {'loss': 0.4591, 'grad_norm': 6.28125, 'learning_rate': 4.92343155572366e-05, 'epoch': 1.198639455782313}\n",
            "\n",
            ">>> Step 882 metrics: {'loss': 0.567, 'grad_norm': 6.0625, 'learning_rate': 4.9232563418014946e-05, 'epoch': 1.2}\n",
            "\n",
            ">>> Step 883 metrics: {'loss': 0.5984, 'grad_norm': 6.0, 'learning_rate': 4.9230809307595984e-05, 'epoch': 1.2013605442176871}\n",
            "\n",
            ">>> Step 884 metrics: {'loss': 0.4655, 'grad_norm': 5.65625, 'learning_rate': 4.92290532261224e-05, 'epoch': 1.202721088435374}\n",
            "\n",
            ">>> Step 885 metrics: {'loss': 0.5304, 'grad_norm': 6.375, 'learning_rate': 4.922729517373705e-05, 'epoch': 1.2040816326530612}\n",
            "\n",
            ">>> Step 886 metrics: {'loss': 0.9677, 'grad_norm': 9.5, 'learning_rate': 4.922553515058293e-05, 'epoch': 1.2054421768707484}\n",
            "\n",
            ">>> Step 887 metrics: {'loss': 1.0456, 'grad_norm': 7.8125, 'learning_rate': 4.922377315680322e-05, 'epoch': 1.2068027210884353}\n",
            "\n",
            ">>> Step 888 metrics: {'loss': 0.6137, 'grad_norm': 6.96875, 'learning_rate': 4.922200919254124e-05, 'epoch': 1.2081632653061225}\n",
            "\n",
            ">>> Step 889 metrics: {'loss': 0.8519, 'grad_norm': 6.84375, 'learning_rate': 4.922024325794049e-05, 'epoch': 1.2095238095238094}\n",
            "\n",
            ">>> Step 890 metrics: {'loss': 1.1039, 'grad_norm': 6.3125, 'learning_rate': 4.921847535314462e-05, 'epoch': 1.2108843537414966}\n",
            "\n",
            ">>> Step 891 metrics: {'loss': 0.5966, 'grad_norm': 7.0, 'learning_rate': 4.921670547829742e-05, 'epoch': 1.2122448979591836}\n",
            "\n",
            ">>> Step 892 metrics: {'loss': 0.5795, 'grad_norm': 6.15625, 'learning_rate': 4.9214933633542886e-05, 'epoch': 1.2136054421768707}\n",
            "\n",
            ">>> Step 893 metrics: {'loss': 0.4821, 'grad_norm': 5.84375, 'learning_rate': 4.921315981902513e-05, 'epoch': 1.2149659863945579}\n",
            "\n",
            ">>> Step 894 metrics: {'loss': 1.0838, 'grad_norm': 10.0, 'learning_rate': 4.9211384034888465e-05, 'epoch': 1.2163265306122448}\n",
            "\n",
            ">>> Step 895 metrics: {'loss': 0.282, 'grad_norm': 5.21875, 'learning_rate': 4.920960628127732e-05, 'epoch': 1.217687074829932}\n",
            "\n",
            ">>> Step 896 metrics: {'loss': 0.7505, 'grad_norm': 7.375, 'learning_rate': 4.9207826558336314e-05, 'epoch': 1.2190476190476192}\n",
            "\n",
            ">>> Step 897 metrics: {'loss': 0.748, 'grad_norm': 12.8125, 'learning_rate': 4.9206044866210224e-05, 'epoch': 1.220408163265306}\n",
            "\n",
            ">>> Step 898 metrics: {'loss': 0.6898, 'grad_norm': 6.65625, 'learning_rate': 4.920426120504397e-05, 'epoch': 1.2217687074829933}\n",
            "\n",
            ">>> Step 899 metrics: {'loss': 0.7021, 'grad_norm': 7.40625, 'learning_rate': 4.9202475574982655e-05, 'epoch': 1.2231292517006802}\n",
            "\n",
            ">>> Step 900 metrics: {'loss': 0.6311, 'grad_norm': 7.125, 'learning_rate': 4.9200687976171525e-05, 'epoch': 1.2244897959183674}\n",
            "\n",
            ">>> Step 901 metrics: {'loss': 0.7727, 'grad_norm': 6.6875, 'learning_rate': 4.919889840875599e-05, 'epoch': 1.2258503401360543}\n",
            "\n",
            ">>> Step 902 metrics: {'loss': 0.9115, 'grad_norm': 7.03125, 'learning_rate': 4.919710687288163e-05, 'epoch': 1.2272108843537415}\n",
            "\n",
            ">>> Step 903 metrics: {'loss': 0.4969, 'grad_norm': 5.46875, 'learning_rate': 4.919531336869417e-05, 'epoch': 1.2285714285714286}\n",
            "\n",
            ">>> Step 904 metrics: {'loss': 1.0719, 'grad_norm': 7.5, 'learning_rate': 4.919351789633951e-05, 'epoch': 1.2299319727891156}\n",
            "\n",
            ">>> Step 905 metrics: {'loss': 0.5296, 'grad_norm': 5.53125, 'learning_rate': 4.91917204559637e-05, 'epoch': 1.2312925170068028}\n",
            "\n",
            ">>> Step 906 metrics: {'loss': 0.8573, 'grad_norm': 7.15625, 'learning_rate': 4.9189921047712945e-05, 'epoch': 1.2326530612244897}\n",
            "\n",
            ">>> Step 907 metrics: {'loss': 0.6692, 'grad_norm': 8.375, 'learning_rate': 4.918811967173363e-05, 'epoch': 1.2340136054421769}\n",
            "\n",
            ">>> Step 908 metrics: {'loss': 0.4733, 'grad_norm': 6.71875, 'learning_rate': 4.9186316328172274e-05, 'epoch': 1.235374149659864}\n",
            "\n",
            ">>> Step 909 metrics: {'loss': 0.402, 'grad_norm': 5.15625, 'learning_rate': 4.918451101717559e-05, 'epoch': 1.236734693877551}\n",
            "\n",
            ">>> Step 910 metrics: {'loss': 0.889, 'grad_norm': 6.53125, 'learning_rate': 4.9182703738890415e-05, 'epoch': 1.2380952380952381}\n",
            "\n",
            ">>> Step 911 metrics: {'loss': 0.6077, 'grad_norm': 7.71875, 'learning_rate': 4.918089449346376e-05, 'epoch': 1.239455782312925}\n",
            "\n",
            ">>> Step 912 metrics: {'loss': 0.6937, 'grad_norm': 6.875, 'learning_rate': 4.917908328104281e-05, 'epoch': 1.2408163265306122}\n",
            "\n",
            ">>> Step 913 metrics: {'loss': 0.8715, 'grad_norm': 7.09375, 'learning_rate': 4.917727010177489e-05, 'epoch': 1.2421768707482994}\n",
            "\n",
            ">>> Step 914 metrics: {'loss': 0.7943, 'grad_norm': 7.28125, 'learning_rate': 4.91754549558075e-05, 'epoch': 1.2435374149659864}\n",
            "\n",
            ">>> Step 915 metrics: {'loss': 0.7013, 'grad_norm': 7.375, 'learning_rate': 4.917363784328828e-05, 'epoch': 1.2448979591836735}\n",
            "\n",
            ">>> Step 916 metrics: {'loss': 0.9151, 'grad_norm': 6.21875, 'learning_rate': 4.917181876436506e-05, 'epoch': 1.2462585034013605}\n",
            "\n",
            ">>> Step 917 metrics: {'loss': 0.7466, 'grad_norm': 6.5625, 'learning_rate': 4.91699977191858e-05, 'epoch': 1.2476190476190476}\n",
            "\n",
            ">>> Step 918 metrics: {'loss': 0.6051, 'grad_norm': 7.78125, 'learning_rate': 4.9168174707898634e-05, 'epoch': 1.2489795918367346}\n",
            "\n",
            ">>> Step 919 metrics: {'loss': 0.9281, 'grad_norm': 7.46875, 'learning_rate': 4.9166349730651865e-05, 'epoch': 1.2503401360544217}\n",
            "\n",
            ">>> Step 920 metrics: {'loss': 0.8408, 'grad_norm': 7.125, 'learning_rate': 4.916452278759393e-05, 'epoch': 1.251700680272109}\n",
            "\n",
            ">>> Step 921 metrics: {'loss': 0.4591, 'grad_norm': 5.59375, 'learning_rate': 4.9162693878873455e-05, 'epoch': 1.2530612244897958}\n",
            "\n",
            ">>> Step 922 metrics: {'loss': 0.9572, 'grad_norm': 6.375, 'learning_rate': 4.9160863004639204e-05, 'epoch': 1.254421768707483}\n",
            "\n",
            ">>> Step 923 metrics: {'loss': 0.4815, 'grad_norm': 5.75, 'learning_rate': 4.915903016504012e-05, 'epoch': 1.2557823129251702}\n",
            "\n",
            ">>> Step 924 metrics: {'loss': 0.8296, 'grad_norm': 6.03125, 'learning_rate': 4.915719536022528e-05, 'epoch': 1.2571428571428571}\n",
            "\n",
            ">>> Step 925 metrics: {'loss': 0.8226, 'grad_norm': 6.71875, 'learning_rate': 4.9155358590343955e-05, 'epoch': 1.2585034013605443}\n",
            "\n",
            ">>> Step 926 metrics: {'loss': 0.7312, 'grad_norm': 7.0, 'learning_rate': 4.9153519855545535e-05, 'epoch': 1.2598639455782312}\n",
            "\n",
            ">>> Step 927 metrics: {'loss': 0.6976, 'grad_norm': 6.40625, 'learning_rate': 4.9151679155979615e-05, 'epoch': 1.2612244897959184}\n",
            "\n",
            ">>> Step 928 metrics: {'loss': 0.3879, 'grad_norm': 5.40625, 'learning_rate': 4.914983649179591e-05, 'epoch': 1.2625850340136053}\n",
            "\n",
            ">>> Step 929 metrics: {'loss': 0.5384, 'grad_norm': 6.03125, 'learning_rate': 4.914799186314431e-05, 'epoch': 1.2639455782312925}\n",
            "\n",
            ">>> Step 930 metrics: {'loss': 0.4555, 'grad_norm': 6.3125, 'learning_rate': 4.9146145270174884e-05, 'epoch': 1.2653061224489797}\n",
            "\n",
            ">>> Step 931 metrics: {'loss': 0.4214, 'grad_norm': 5.78125, 'learning_rate': 4.9144296713037816e-05, 'epoch': 1.2666666666666666}\n",
            "\n",
            ">>> Step 932 metrics: {'loss': 0.7307, 'grad_norm': 6.34375, 'learning_rate': 4.914244619188351e-05, 'epoch': 1.2680272108843538}\n",
            "\n",
            ">>> Step 933 metrics: {'loss': 0.4894, 'grad_norm': 6.25, 'learning_rate': 4.914059370686247e-05, 'epoch': 1.269387755102041}\n",
            "\n",
            ">>> Step 934 metrics: {'loss': 0.745, 'grad_norm': 6.1875, 'learning_rate': 4.91387392581254e-05, 'epoch': 1.2707482993197279}\n",
            "\n",
            ">>> Step 935 metrics: {'loss': 1.1391, 'grad_norm': 8.75, 'learning_rate': 4.913688284582314e-05, 'epoch': 1.272108843537415}\n",
            "\n",
            ">>> Step 936 metrics: {'loss': 0.7244, 'grad_norm': 6.5625, 'learning_rate': 4.9135024470106717e-05, 'epoch': 1.273469387755102}\n",
            "\n",
            ">>> Step 937 metrics: {'loss': 1.322, 'grad_norm': 8.3125, 'learning_rate': 4.9133164131127274e-05, 'epoch': 1.2748299319727892}\n",
            "\n",
            ">>> Step 938 metrics: {'loss': 0.4568, 'grad_norm': 5.5, 'learning_rate': 4.913130182903617e-05, 'epoch': 1.276190476190476}\n",
            "\n",
            ">>> Step 939 metrics: {'loss': 0.3991, 'grad_norm': 6.15625, 'learning_rate': 4.912943756398487e-05, 'epoch': 1.2775510204081633}\n",
            "\n",
            ">>> Step 940 metrics: {'loss': 0.5223, 'grad_norm': 6.0, 'learning_rate': 4.9127571336125036e-05, 'epoch': 1.2789115646258504}\n",
            "\n",
            ">>> Step 941 metrics: {'loss': 1.0029, 'grad_norm': 6.40625, 'learning_rate': 4.912570314560847e-05, 'epoch': 1.2802721088435374}\n",
            "\n",
            ">>> Step 942 metrics: {'loss': 0.6546, 'grad_norm': 5.84375, 'learning_rate': 4.9123832992587136e-05, 'epoch': 1.2816326530612245}\n",
            "\n",
            ">>> Step 943 metrics: {'loss': 0.4815, 'grad_norm': 5.65625, 'learning_rate': 4.912196087721318e-05, 'epoch': 1.2829931972789117}\n",
            "\n",
            ">>> Step 944 metrics: {'loss': 0.419, 'grad_norm': 5.78125, 'learning_rate': 4.9120086799638865e-05, 'epoch': 1.2843537414965986}\n",
            "\n",
            ">>> Step 945 metrics: {'loss': 0.6461, 'grad_norm': 7.15625, 'learning_rate': 4.911821076001666e-05, 'epoch': 1.2857142857142856}\n",
            "\n",
            ">>> Step 946 metrics: {'loss': 1.0987, 'grad_norm': 7.71875, 'learning_rate': 4.9116332758499156e-05, 'epoch': 1.2870748299319728}\n",
            "\n",
            ">>> Step 947 metrics: {'loss': 0.735, 'grad_norm': 9.4375, 'learning_rate': 4.9114452795239126e-05, 'epoch': 1.28843537414966}\n",
            "\n",
            ">>> Step 948 metrics: {'loss': 0.3443, 'grad_norm': 6.34375, 'learning_rate': 4.9112570870389494e-05, 'epoch': 1.2897959183673469}\n",
            "\n",
            ">>> Step 949 metrics: {'loss': 0.7392, 'grad_norm': 6.90625, 'learning_rate': 4.9110686984103335e-05, 'epoch': 1.291156462585034}\n",
            "\n",
            ">>> Step 950 metrics: {'loss': 0.8659, 'grad_norm': 6.875, 'learning_rate': 4.910880113653391e-05, 'epoch': 1.2925170068027212}\n",
            "\n",
            ">>> Step 951 metrics: {'loss': 0.9202, 'grad_norm': 7.65625, 'learning_rate': 4.910691332783463e-05, 'epoch': 1.2938775510204081}\n",
            "\n",
            ">>> Step 952 metrics: {'loss': 0.5455, 'grad_norm': 6.34375, 'learning_rate': 4.910502355815902e-05, 'epoch': 1.2952380952380953}\n",
            "\n",
            ">>> Step 953 metrics: {'loss': 0.6197, 'grad_norm': 6.0625, 'learning_rate': 4.910313182766084e-05, 'epoch': 1.2965986394557822}\n",
            "\n",
            ">>> Step 954 metrics: {'loss': 0.5834, 'grad_norm': 6.65625, 'learning_rate': 4.9101238136493966e-05, 'epoch': 1.2979591836734694}\n",
            "\n",
            ">>> Step 955 metrics: {'loss': 0.7166, 'grad_norm': 7.3125, 'learning_rate': 4.909934248481244e-05, 'epoch': 1.2993197278911564}\n",
            "\n",
            ">>> Step 956 metrics: {'loss': 0.4613, 'grad_norm': 5.90625, 'learning_rate': 4.9097444872770445e-05, 'epoch': 1.3006802721088435}\n",
            "\n",
            ">>> Step 957 metrics: {'loss': 0.2665, 'grad_norm': 5.4375, 'learning_rate': 4.9095545300522364e-05, 'epoch': 1.3020408163265307}\n",
            "\n",
            ">>> Step 958 metrics: {'loss': 0.518, 'grad_norm': 5.90625, 'learning_rate': 4.909364376822271e-05, 'epoch': 1.3034013605442176}\n",
            "\n",
            ">>> Step 959 metrics: {'loss': 0.5189, 'grad_norm': 5.84375, 'learning_rate': 4.909174027602616e-05, 'epoch': 1.3047619047619048}\n",
            "\n",
            ">>> Step 960 metrics: {'loss': 0.3788, 'grad_norm': 5.25, 'learning_rate': 4.908983482408757e-05, 'epoch': 1.306122448979592}\n",
            "\n",
            ">>> Step 961 metrics: {'loss': 1.3015, 'grad_norm': 7.46875, 'learning_rate': 4.908792741256191e-05, 'epoch': 1.307482993197279}\n",
            "\n",
            ">>> Step 962 metrics: {'loss': 0.4678, 'grad_norm': 5.59375, 'learning_rate': 4.908601804160436e-05, 'epoch': 1.308843537414966}\n",
            "\n",
            ">>> Step 963 metrics: {'loss': 0.7322, 'grad_norm': 6.09375, 'learning_rate': 4.908410671137023e-05, 'epoch': 1.310204081632653}\n",
            "\n",
            ">>> Step 964 metrics: {'loss': 0.5175, 'grad_norm': 5.625, 'learning_rate': 4.9082193422015e-05, 'epoch': 1.3115646258503402}\n",
            "\n",
            ">>> Step 965 metrics: {'loss': 0.9133, 'grad_norm': 7.4375, 'learning_rate': 4.90802781736943e-05, 'epoch': 1.3129251700680271}\n",
            "\n",
            ">>> Step 966 metrics: {'loss': 0.7961, 'grad_norm': 5.9375, 'learning_rate': 4.907836096656394e-05, 'epoch': 1.3142857142857143}\n",
            "\n",
            ">>> Step 967 metrics: {'loss': 0.9461, 'grad_norm': 6.8125, 'learning_rate': 4.907644180077987e-05, 'epoch': 1.3156462585034014}\n",
            "\n",
            ">>> Step 968 metrics: {'loss': 0.4371, 'grad_norm': 8.1875, 'learning_rate': 4.907452067649819e-05, 'epoch': 1.3170068027210884}\n",
            "\n",
            ">>> Step 969 metrics: {'loss': 0.8147, 'grad_norm': 6.125, 'learning_rate': 4.907259759387519e-05, 'epoch': 1.3183673469387756}\n",
            "\n",
            ">>> Step 970 metrics: {'loss': 0.6075, 'grad_norm': 5.9375, 'learning_rate': 4.90706725530673e-05, 'epoch': 1.3197278911564627}\n",
            "\n",
            ">>> Step 971 metrics: {'loss': 0.8443, 'grad_norm': 6.4375, 'learning_rate': 4.906874555423111e-05, 'epoch': 1.3210884353741497}\n",
            "\n",
            ">>> Step 972 metrics: {'loss': 0.9693, 'grad_norm': 6.5, 'learning_rate': 4.9066816597523366e-05, 'epoch': 1.3224489795918366}\n",
            "\n",
            ">>> Step 973 metrics: {'loss': 0.7057, 'grad_norm': 7.15625, 'learning_rate': 4.9064885683100993e-05, 'epoch': 1.3238095238095238}\n",
            "\n",
            ">>> Step 974 metrics: {'loss': 0.4849, 'grad_norm': 6.0, 'learning_rate': 4.906295281112106e-05, 'epoch': 1.325170068027211}\n",
            "\n",
            ">>> Step 975 metrics: {'loss': 0.6036, 'grad_norm': 5.8125, 'learning_rate': 4.906101798174077e-05, 'epoch': 1.3265306122448979}\n",
            "\n",
            ">>> Step 976 metrics: {'loss': 0.4763, 'grad_norm': 6.96875, 'learning_rate': 4.9059081195117537e-05, 'epoch': 1.327891156462585}\n",
            "\n",
            ">>> Step 977 metrics: {'loss': 1.0656, 'grad_norm': 7.0625, 'learning_rate': 4.905714245140891e-05, 'epoch': 1.3292517006802722}\n",
            "\n",
            ">>> Step 978 metrics: {'loss': 0.6197, 'grad_norm': 7.0, 'learning_rate': 4.9055201750772586e-05, 'epoch': 1.3306122448979592}\n",
            "\n",
            ">>> Step 979 metrics: {'loss': 0.5825, 'grad_norm': 5.9375, 'learning_rate': 4.905325909336643e-05, 'epoch': 1.3319727891156463}\n",
            "\n",
            ">>> Step 980 metrics: {'loss': 0.7059, 'grad_norm': 6.28125, 'learning_rate': 4.9051314479348475e-05, 'epoch': 1.3333333333333333}\n",
            "\n",
            ">>> Step 981 metrics: {'loss': 0.5036, 'grad_norm': 6.28125, 'learning_rate': 4.9049367908876904e-05, 'epoch': 1.3346938775510204}\n",
            "\n",
            ">>> Step 982 metrics: {'loss': 0.8737, 'grad_norm': 7.5, 'learning_rate': 4.904741938211006e-05, 'epoch': 1.3360544217687074}\n",
            "\n",
            ">>> Step 983 metrics: {'loss': 1.1408, 'grad_norm': 8.0625, 'learning_rate': 4.904546889920644e-05, 'epoch': 1.3374149659863945}\n",
            "\n",
            ">>> Step 984 metrics: {'loss': 0.853, 'grad_norm': 6.71875, 'learning_rate': 4.904351646032471e-05, 'epoch': 1.3387755102040817}\n",
            "\n",
            ">>> Step 985 metrics: {'loss': 0.482, 'grad_norm': 5.71875, 'learning_rate': 4.9041562065623706e-05, 'epoch': 1.3401360544217686}\n",
            "\n",
            ">>> Step 986 metrics: {'loss': 0.5809, 'grad_norm': 5.34375, 'learning_rate': 4.903960571526238e-05, 'epoch': 1.3414965986394558}\n",
            "\n",
            ">>> Step 987 metrics: {'loss': 1.1691, 'grad_norm': 7.53125, 'learning_rate': 4.903764740939989e-05, 'epoch': 1.342857142857143}\n",
            "\n",
            ">>> Step 988 metrics: {'loss': 0.5953, 'grad_norm': 5.75, 'learning_rate': 4.903568714819553e-05, 'epoch': 1.34421768707483}\n",
            "\n",
            ">>> Step 989 metrics: {'loss': 0.8269, 'grad_norm': 6.4375, 'learning_rate': 4.903372493180876e-05, 'epoch': 1.345578231292517}\n",
            "\n",
            ">>> Step 990 metrics: {'loss': 1.2522, 'grad_norm': 7.1875, 'learning_rate': 4.903176076039918e-05, 'epoch': 1.346938775510204}\n",
            "\n",
            ">>> Step 991 metrics: {'loss': 0.7026, 'grad_norm': 6.6875, 'learning_rate': 4.9029794634126594e-05, 'epoch': 1.3482993197278912}\n",
            "\n",
            ">>> Step 992 metrics: {'loss': 0.4638, 'grad_norm': 6.4375, 'learning_rate': 4.902782655315092e-05, 'epoch': 1.3496598639455781}\n",
            "\n",
            ">>> Step 993 metrics: {'loss': 1.0824, 'grad_norm': 7.9375, 'learning_rate': 4.902585651763225e-05, 'epoch': 1.3510204081632653}\n",
            "\n",
            ">>> Step 994 metrics: {'loss': 0.8739, 'grad_norm': 7.9375, 'learning_rate': 4.902388452773083e-05, 'epoch': 1.3523809523809525}\n",
            "\n",
            ">>> Step 995 metrics: {'loss': 0.6123, 'grad_norm': 5.96875, 'learning_rate': 4.90219105836071e-05, 'epoch': 1.3537414965986394}\n",
            "\n",
            ">>> Step 996 metrics: {'loss': 0.5709, 'grad_norm': 5.4375, 'learning_rate': 4.90199346854216e-05, 'epoch': 1.3551020408163266}\n",
            "\n",
            ">>> Step 997 metrics: {'loss': 0.5443, 'grad_norm': 5.84375, 'learning_rate': 4.901795683333508e-05, 'epoch': 1.3564625850340137}\n",
            "\n",
            ">>> Step 998 metrics: {'loss': 0.46, 'grad_norm': 5.40625, 'learning_rate': 4.901597702750841e-05, 'epoch': 1.3578231292517007}\n",
            "\n",
            ">>> Step 999 metrics: {'loss': 0.8741, 'grad_norm': 7.5625, 'learning_rate': 4.901399526810265e-05, 'epoch': 1.3591836734693876}\n",
            "\n",
            ">>> Step 1000 metrics: {'loss': 0.3948, 'grad_norm': 7.125, 'learning_rate': 4.9012011555279e-05, 'epoch': 1.3605442176870748}\n",
            "\n",
            ">>> Step 1001 metrics: {'loss': 1.0713, 'grad_norm': 9.1875, 'learning_rate': 4.901002588919883e-05, 'epoch': 1.361904761904762}\n",
            "\n",
            ">>> Step 1002 metrics: {'loss': 1.0739, 'grad_norm': 8.375, 'learning_rate': 4.900803827002367e-05, 'epoch': 1.363265306122449}\n",
            "\n",
            ">>> Step 1003 metrics: {'loss': 0.4162, 'grad_norm': 5.59375, 'learning_rate': 4.9006048697915185e-05, 'epoch': 1.364625850340136}\n",
            "\n",
            ">>> Step 1004 metrics: {'loss': 0.9414, 'grad_norm': 6.5625, 'learning_rate': 4.900405717303522e-05, 'epoch': 1.3659863945578232}\n",
            "\n",
            ">>> Step 1005 metrics: {'loss': 0.8468, 'grad_norm': 7.03125, 'learning_rate': 4.9002063695545796e-05, 'epoch': 1.3673469387755102}\n",
            "\n",
            ">>> Step 1006 metrics: {'loss': 0.9551, 'grad_norm': 6.875, 'learning_rate': 4.900006826560905e-05, 'epoch': 1.3687074829931973}\n",
            "\n",
            ">>> Step 1007 metrics: {'loss': 0.3673, 'grad_norm': 5.90625, 'learning_rate': 4.8998070883387316e-05, 'epoch': 1.3700680272108843}\n",
            "\n",
            ">>> Step 1008 metrics: {'loss': 0.5147, 'grad_norm': 6.4375, 'learning_rate': 4.8996071549043054e-05, 'epoch': 1.3714285714285714}\n",
            "\n",
            ">>> Step 1009 metrics: {'loss': 0.6178, 'grad_norm': 7.0, 'learning_rate': 4.899407026273891e-05, 'epoch': 1.3727891156462584}\n",
            "\n",
            ">>> Step 1010 metrics: {'loss': 0.8894, 'grad_norm': 7.15625, 'learning_rate': 4.8992067024637686e-05, 'epoch': 1.3741496598639455}\n",
            "\n",
            ">>> Step 1011 metrics: {'loss': 0.6725, 'grad_norm': 6.65625, 'learning_rate': 4.899006183490232e-05, 'epoch': 1.3755102040816327}\n",
            "\n",
            ">>> Step 1012 metrics: {'loss': 0.5831, 'grad_norm': 6.125, 'learning_rate': 4.8988054693695935e-05, 'epoch': 1.3768707482993197}\n",
            "\n",
            ">>> Step 1013 metrics: {'loss': 0.7939, 'grad_norm': 7.84375, 'learning_rate': 4.8986045601181795e-05, 'epoch': 1.3782312925170068}\n",
            "\n",
            ">>> Step 1014 metrics: {'loss': 0.989, 'grad_norm': 7.09375, 'learning_rate': 4.898403455752334e-05, 'epoch': 1.379591836734694}\n",
            "\n",
            ">>> Step 1015 metrics: {'loss': 0.7529, 'grad_norm': 6.875, 'learning_rate': 4.898202156288414e-05, 'epoch': 1.380952380952381}\n",
            "\n",
            ">>> Step 1016 metrics: {'loss': 0.9652, 'grad_norm': 6.25, 'learning_rate': 4.898000661742796e-05, 'epoch': 1.382312925170068}\n",
            "\n",
            ">>> Step 1017 metrics: {'loss': 0.5966, 'grad_norm': 6.28125, 'learning_rate': 4.8977989721318697e-05, 'epoch': 1.383673469387755}\n",
            "\n",
            ">>> Step 1018 metrics: {'loss': 1.2061, 'grad_norm': 7.53125, 'learning_rate': 4.897597087472042e-05, 'epoch': 1.3850340136054422}\n",
            "\n",
            ">>> Step 1019 metrics: {'loss': 0.9072, 'grad_norm': 6.84375, 'learning_rate': 4.8973950077797346e-05, 'epoch': 1.3863945578231291}\n",
            "\n",
            ">>> Step 1020 metrics: {'loss': 0.796, 'grad_norm': 7.8125, 'learning_rate': 4.897192733071386e-05, 'epoch': 1.3877551020408163}\n",
            "\n",
            ">>> Step 1021 metrics: {'loss': 0.8369, 'grad_norm': 7.25, 'learning_rate': 4.89699026336345e-05, 'epoch': 1.3891156462585035}\n",
            "\n",
            ">>> Step 1022 metrics: {'loss': 0.6012, 'grad_norm': 6.3125, 'learning_rate': 4.896787598672397e-05, 'epoch': 1.3904761904761904}\n",
            "\n",
            ">>> Step 1023 metrics: {'loss': 0.4651, 'grad_norm': 7.5625, 'learning_rate': 4.896584739014712e-05, 'epoch': 1.3918367346938776}\n",
            "\n",
            ">>> Step 1024 metrics: {'loss': 0.9039, 'grad_norm': 8.375, 'learning_rate': 4.896381684406898e-05, 'epoch': 1.3931972789115648}\n",
            "\n",
            ">>> Step 1025 metrics: {'loss': 0.3919, 'grad_norm': 5.21875, 'learning_rate': 4.896178434865472e-05, 'epoch': 1.3945578231292517}\n",
            "\n",
            ">>> Step 1026 metrics: {'loss': 0.7178, 'grad_norm': 6.375, 'learning_rate': 4.895974990406965e-05, 'epoch': 1.3959183673469386}\n",
            "\n",
            ">>> Step 1027 metrics: {'loss': 0.8052, 'grad_norm': 7.53125, 'learning_rate': 4.895771351047929e-05, 'epoch': 1.3972789115646258}\n",
            "\n",
            ">>> Step 1028 metrics: {'loss': 0.7479, 'grad_norm': 6.65625, 'learning_rate': 4.8955675168049285e-05, 'epoch': 1.398639455782313}\n",
            "\n",
            ">>> Step 1029 metrics: {'loss': 0.7283, 'grad_norm': 6.65625, 'learning_rate': 4.895363487694543e-05, 'epoch': 1.4}\n",
            "\n",
            ">>> Step 1030 metrics: {'loss': 0.7169, 'grad_norm': 6.84375, 'learning_rate': 4.895159263733371e-05, 'epoch': 1.401360544217687}\n",
            "\n",
            ">>> Step 1031 metrics: {'loss': 0.6244, 'grad_norm': 6.0, 'learning_rate': 4.8949548449380246e-05, 'epoch': 1.4027210884353742}\n",
            "\n",
            ">>> Step 1032 metrics: {'loss': 0.782, 'grad_norm': 5.875, 'learning_rate': 4.8947502313251314e-05, 'epoch': 1.4040816326530612}\n",
            "\n",
            ">>> Step 1033 metrics: {'loss': 0.2767, 'grad_norm': 4.71875, 'learning_rate': 4.894545422911336e-05, 'epoch': 1.4054421768707483}\n",
            "\n",
            ">>> Step 1034 metrics: {'loss': 0.5809, 'grad_norm': 5.21875, 'learning_rate': 4.8943404197132994e-05, 'epoch': 1.4068027210884353}\n",
            "\n",
            ">>> Step 1035 metrics: {'loss': 0.9383, 'grad_norm': 7.9375, 'learning_rate': 4.894135221747697e-05, 'epoch': 1.4081632653061225}\n",
            "\n",
            ">>> Step 1036 metrics: {'loss': 0.5638, 'grad_norm': 7.65625, 'learning_rate': 4.893929829031221e-05, 'epoch': 1.4095238095238094}\n",
            "\n",
            ">>> Step 1037 metrics: {'loss': 0.4519, 'grad_norm': 5.15625, 'learning_rate': 4.8937242415805775e-05, 'epoch': 1.4108843537414966}\n",
            "\n",
            ">>> Step 1038 metrics: {'loss': 0.6043, 'grad_norm': 6.8125, 'learning_rate': 4.8935184594124916e-05, 'epoch': 1.4122448979591837}\n",
            "\n",
            ">>> Step 1039 metrics: {'loss': 0.5796, 'grad_norm': 6.4375, 'learning_rate': 4.893312482543703e-05, 'epoch': 1.4136054421768707}\n",
            "\n",
            ">>> Step 1040 metrics: {'loss': 0.907, 'grad_norm': 7.15625, 'learning_rate': 4.893106310990965e-05, 'epoch': 1.4149659863945578}\n",
            "\n",
            ">>> Step 1041 metrics: {'loss': 0.462, 'grad_norm': 5.71875, 'learning_rate': 4.89289994477105e-05, 'epoch': 1.416326530612245}\n",
            "\n",
            ">>> Step 1042 metrics: {'loss': 0.6224, 'grad_norm': 6.5625, 'learning_rate': 4.892693383900745e-05, 'epoch': 1.417687074829932}\n",
            "\n",
            ">>> Step 1043 metrics: {'loss': 0.5946, 'grad_norm': 6.34375, 'learning_rate': 4.892486628396852e-05, 'epoch': 1.4190476190476191}\n",
            "\n",
            ">>> Step 1044 metrics: {'loss': 0.6297, 'grad_norm': 6.65625, 'learning_rate': 4.892279678276189e-05, 'epoch': 1.420408163265306}\n",
            "\n",
            ">>> Step 1045 metrics: {'loss': 0.9808, 'grad_norm': 6.59375, 'learning_rate': 4.8920725335555926e-05, 'epoch': 1.4217687074829932}\n",
            "\n",
            ">>> Step 1046 metrics: {'loss': 0.8383, 'grad_norm': 7.34375, 'learning_rate': 4.891865194251911e-05, 'epoch': 1.4231292517006802}\n",
            "\n",
            ">>> Step 1047 metrics: {'loss': 0.7368, 'grad_norm': 8.3125, 'learning_rate': 4.891657660382011e-05, 'epoch': 1.4244897959183673}\n",
            "\n",
            ">>> Step 1048 metrics: {'loss': 1.1497, 'grad_norm': 6.875, 'learning_rate': 4.891449931962773e-05, 'epoch': 1.4258503401360545}\n",
            "\n",
            ">>> Step 1049 metrics: {'loss': 1.2442, 'grad_norm': 10.125, 'learning_rate': 4.891242009011097e-05, 'epoch': 1.4272108843537414}\n",
            "\n",
            ">>> Step 1050 metrics: {'loss': 0.4817, 'grad_norm': 5.4375, 'learning_rate': 4.8910338915438944e-05, 'epoch': 1.4285714285714286}\n",
            "\n",
            ">>> Step 1051 metrics: {'loss': 0.7352, 'grad_norm': 6.71875, 'learning_rate': 4.8908255795780967e-05, 'epoch': 1.4299319727891158}\n",
            "\n",
            ">>> Step 1052 metrics: {'loss': 0.5449, 'grad_norm': 6.125, 'learning_rate': 4.890617073130647e-05, 'epoch': 1.4312925170068027}\n",
            "\n",
            ">>> Step 1053 metrics: {'loss': 0.8625, 'grad_norm': 8.125, 'learning_rate': 4.8904083722185066e-05, 'epoch': 1.4326530612244899}\n",
            "\n",
            ">>> Step 1054 metrics: {'loss': 0.5274, 'grad_norm': 5.1875, 'learning_rate': 4.890199476858653e-05, 'epoch': 1.4340136054421768}\n",
            "\n",
            ">>> Step 1055 metrics: {'loss': 0.4789, 'grad_norm': 5.5, 'learning_rate': 4.889990387068078e-05, 'epoch': 1.435374149659864}\n",
            "\n",
            ">>> Step 1056 metrics: {'loss': 0.5979, 'grad_norm': 6.75, 'learning_rate': 4.889781102863792e-05, 'epoch': 1.436734693877551}\n",
            "\n",
            ">>> Step 1057 metrics: {'loss': 1.2689, 'grad_norm': 7.78125, 'learning_rate': 4.889571624262817e-05, 'epoch': 1.438095238095238}\n",
            "\n",
            ">>> Step 1058 metrics: {'loss': 0.9667, 'grad_norm': 7.6875, 'learning_rate': 4.889361951282193e-05, 'epoch': 1.4394557823129253}\n",
            "\n",
            ">>> Step 1059 metrics: {'loss': 0.7441, 'grad_norm': 6.125, 'learning_rate': 4.889152083938977e-05, 'epoch': 1.4408163265306122}\n",
            "\n",
            ">>> Step 1060 metrics: {'loss': 0.7654, 'grad_norm': 10.75, 'learning_rate': 4.888942022250241e-05, 'epoch': 1.4421768707482994}\n",
            "\n",
            ">>> Step 1061 metrics: {'loss': 0.796, 'grad_norm': 8.3125, 'learning_rate': 4.888731766233071e-05, 'epoch': 1.4435374149659865}\n",
            "\n",
            ">>> Step 1062 metrics: {'loss': 0.4162, 'grad_norm': 7.1875, 'learning_rate': 4.888521315904571e-05, 'epoch': 1.4448979591836735}\n",
            "\n",
            ">>> Step 1063 metrics: {'loss': 0.6246, 'grad_norm': 6.78125, 'learning_rate': 4.88831067128186e-05, 'epoch': 1.4462585034013604}\n",
            "\n",
            ">>> Step 1064 metrics: {'loss': 1.0363, 'grad_norm': 6.75, 'learning_rate': 4.8880998323820726e-05, 'epoch': 1.4476190476190476}\n",
            "\n",
            ">>> Step 1065 metrics: {'loss': 1.1346, 'grad_norm': 6.9375, 'learning_rate': 4.8878887992223605e-05, 'epoch': 1.4489795918367347}\n",
            "\n",
            ">>> Step 1066 metrics: {'loss': 0.8339, 'grad_norm': 6.6875, 'learning_rate': 4.8876775718198895e-05, 'epoch': 1.4503401360544217}\n",
            "\n",
            ">>> Step 1067 metrics: {'loss': 0.5215, 'grad_norm': 5.8125, 'learning_rate': 4.8874661501918414e-05, 'epoch': 1.4517006802721089}\n",
            "\n",
            ">>> Step 1068 metrics: {'loss': 0.5625, 'grad_norm': 6.0625, 'learning_rate': 4.887254534355415e-05, 'epoch': 1.453061224489796}\n",
            "\n",
            ">>> Step 1069 metrics: {'loss': 0.8317, 'grad_norm': 6.125, 'learning_rate': 4.8870427243278236e-05, 'epoch': 1.454421768707483}\n",
            "\n",
            ">>> Step 1070 metrics: {'loss': 0.9973, 'grad_norm': 7.625, 'learning_rate': 4.886830720126298e-05, 'epoch': 1.4557823129251701}\n",
            "\n",
            ">>> Step 1071 metrics: {'loss': 0.5739, 'grad_norm': 6.375, 'learning_rate': 4.886618521768082e-05, 'epoch': 1.457142857142857}\n",
            "\n",
            ">>> Step 1072 metrics: {'loss': 0.8073, 'grad_norm': 5.625, 'learning_rate': 4.886406129270438e-05, 'epoch': 1.4585034013605442}\n",
            "\n",
            ">>> Step 1073 metrics: {'loss': 0.94, 'grad_norm': 6.28125, 'learning_rate': 4.8861935426506435e-05, 'epoch': 1.4598639455782312}\n",
            "\n",
            ">>> Step 1074 metrics: {'loss': 0.5219, 'grad_norm': 4.875, 'learning_rate': 4.88598076192599e-05, 'epoch': 1.4612244897959183}\n",
            "\n",
            ">>> Step 1075 metrics: {'loss': 0.8367, 'grad_norm': 6.75, 'learning_rate': 4.8857677871137874e-05, 'epoch': 1.4625850340136055}\n",
            "\n",
            ">>> Step 1076 metrics: {'loss': 0.5971, 'grad_norm': 5.84375, 'learning_rate': 4.8855546182313595e-05, 'epoch': 1.4639455782312925}\n",
            "\n",
            ">>> Step 1077 metrics: {'loss': 0.9512, 'grad_norm': 7.46875, 'learning_rate': 4.8853412552960464e-05, 'epoch': 1.4653061224489796}\n",
            "\n",
            ">>> Step 1078 metrics: {'loss': 1.1088, 'grad_norm': 7.3125, 'learning_rate': 4.885127698325204e-05, 'epoch': 1.4666666666666668}\n",
            "\n",
            ">>> Step 1079 metrics: {'loss': 0.718, 'grad_norm': 8.6875, 'learning_rate': 4.884913947336205e-05, 'epoch': 1.4680272108843537}\n",
            "\n",
            ">>> Step 1080 metrics: {'loss': 1.3859, 'grad_norm': 8.375, 'learning_rate': 4.884700002346436e-05, 'epoch': 1.469387755102041}\n",
            "\n",
            ">>> Step 1081 metrics: {'loss': 1.5344, 'grad_norm': 10.75, 'learning_rate': 4.8844858633733004e-05, 'epoch': 1.4707482993197278}\n",
            "\n",
            ">>> Step 1082 metrics: {'loss': 0.8043, 'grad_norm': 6.53125, 'learning_rate': 4.884271530434218e-05, 'epoch': 1.472108843537415}\n",
            "\n",
            ">>> Step 1083 metrics: {'loss': 0.6462, 'grad_norm': 5.9375, 'learning_rate': 4.884057003546624e-05, 'epoch': 1.473469387755102}\n",
            "\n",
            ">>> Step 1084 metrics: {'loss': 0.9433, 'grad_norm': 6.59375, 'learning_rate': 4.8838422827279675e-05, 'epoch': 1.474829931972789}\n",
            "\n",
            ">>> Step 1085 metrics: {'loss': 0.9885, 'grad_norm': 12.9375, 'learning_rate': 4.883627367995715e-05, 'epoch': 1.4761904761904763}\n",
            "\n",
            ">>> Step 1086 metrics: {'loss': 0.999, 'grad_norm': 7.09375, 'learning_rate': 4.8834122593673515e-05, 'epoch': 1.4775510204081632}\n",
            "\n",
            ">>> Step 1087 metrics: {'loss': 0.981, 'grad_norm': 6.8125, 'learning_rate': 4.883196956860372e-05, 'epoch': 1.4789115646258504}\n",
            "\n",
            ">>> Step 1088 metrics: {'loss': 0.7802, 'grad_norm': 6.78125, 'learning_rate': 4.882981460492293e-05, 'epoch': 1.4802721088435375}\n",
            "\n",
            ">>> Step 1089 metrics: {'loss': 0.6961, 'grad_norm': 6.71875, 'learning_rate': 4.882765770280641e-05, 'epoch': 1.4816326530612245}\n",
            "\n",
            ">>> Step 1090 metrics: {'loss': 0.5837, 'grad_norm': 6.3125, 'learning_rate': 4.8825498862429624e-05, 'epoch': 1.4829931972789114}\n",
            "\n",
            ">>> Step 1091 metrics: {'loss': 0.6836, 'grad_norm': 5.625, 'learning_rate': 4.882333808396819e-05, 'epoch': 1.4843537414965986}\n",
            "\n",
            ">>> Step 1092 metrics: {'loss': 0.4413, 'grad_norm': 6.125, 'learning_rate': 4.8821175367597884e-05, 'epoch': 1.4857142857142858}\n",
            "\n",
            ">>> Step 1093 metrics: {'loss': 0.5643, 'grad_norm': 5.90625, 'learning_rate': 4.881901071349461e-05, 'epoch': 1.4870748299319727}\n",
            "\n",
            ">>> Step 1094 metrics: {'loss': 1.2841, 'grad_norm': 7.0625, 'learning_rate': 4.8816844121834465e-05, 'epoch': 1.4884353741496599}\n",
            "\n",
            ">>> Step 1095 metrics: {'loss': 0.7026, 'grad_norm': 7.5625, 'learning_rate': 4.881467559279369e-05, 'epoch': 1.489795918367347}\n",
            "\n",
            ">>> Step 1096 metrics: {'loss': 0.8051, 'grad_norm': 6.15625, 'learning_rate': 4.881250512654868e-05, 'epoch': 1.491156462585034}\n",
            "\n",
            ">>> Step 1097 metrics: {'loss': 0.7439, 'grad_norm': 6.625, 'learning_rate': 4.881033272327599e-05, 'epoch': 1.4925170068027211}\n",
            "\n",
            ">>> Step 1098 metrics: {'loss': 1.1315, 'grad_norm': 6.75, 'learning_rate': 4.880815838315235e-05, 'epoch': 1.493877551020408}\n",
            "\n",
            ">>> Step 1099 metrics: {'loss': 0.6, 'grad_norm': 6.28125, 'learning_rate': 4.8805982106354606e-05, 'epoch': 1.4952380952380953}\n",
            "\n",
            ">>> Step 1100 metrics: {'loss': 0.5434, 'grad_norm': 5.40625, 'learning_rate': 4.8803803893059806e-05, 'epoch': 1.4965986394557822}\n",
            "\n",
            ">>> Step 1101 metrics: {'loss': 1.1014, 'grad_norm': 6.9375, 'learning_rate': 4.8801623743445134e-05, 'epoch': 1.4979591836734694}\n",
            "\n",
            ">>> Step 1102 metrics: {'loss': 0.5778, 'grad_norm': 5.71875, 'learning_rate': 4.8799441657687925e-05, 'epoch': 1.4993197278911565}\n",
            "\n",
            ">>> Step 1103 metrics: {'loss': 1.1045, 'grad_norm': 8.5, 'learning_rate': 4.879725763596569e-05, 'epoch': 1.5006802721088435}\n",
            "\n",
            ">>> Step 1104 metrics: {'loss': 0.5175, 'grad_norm': 5.65625, 'learning_rate': 4.879507167845609e-05, 'epoch': 1.5020408163265306}\n",
            "\n",
            ">>> Step 1105 metrics: {'loss': 0.9259, 'grad_norm': 6.09375, 'learning_rate': 4.879288378533693e-05, 'epoch': 1.5034013605442178}\n",
            "\n",
            ">>> Step 1106 metrics: {'loss': 0.598, 'grad_norm': 6.59375, 'learning_rate': 4.879069395678619e-05, 'epoch': 1.5047619047619047}\n",
            "\n",
            ">>> Step 1107 metrics: {'loss': 0.6872, 'grad_norm': 6.375, 'learning_rate': 4.878850219298201e-05, 'epoch': 1.5061224489795917}\n",
            "\n",
            ">>> Step 1108 metrics: {'loss': 0.7164, 'grad_norm': 5.875, 'learning_rate': 4.878630849410267e-05, 'epoch': 1.507482993197279}\n",
            "\n",
            ">>> Step 1109 metrics: {'loss': 1.1331, 'grad_norm': 6.28125, 'learning_rate': 4.878411286032662e-05, 'epoch': 1.508843537414966}\n",
            "\n",
            ">>> Step 1110 metrics: {'loss': 0.8031, 'grad_norm': 5.90625, 'learning_rate': 4.878191529183246e-05, 'epoch': 1.510204081632653}\n",
            "\n",
            ">>> Step 1111 metrics: {'loss': 0.9613, 'grad_norm': 6.53125, 'learning_rate': 4.8779715788798955e-05, 'epoch': 1.5115646258503401}\n",
            "\n",
            ">>> Step 1112 metrics: {'loss': 0.4245, 'grad_norm': 5.21875, 'learning_rate': 4.877751435140502e-05, 'epoch': 1.5129251700680273}\n",
            "\n",
            ">>> Step 1113 metrics: {'loss': 0.7426, 'grad_norm': 7.03125, 'learning_rate': 4.8775310979829723e-05, 'epoch': 1.5142857142857142}\n",
            "\n",
            ">>> Step 1114 metrics: {'loss': 0.4016, 'grad_norm': 5.84375, 'learning_rate': 4.8773105674252317e-05, 'epoch': 1.5156462585034014}\n",
            "\n",
            ">>> Step 1115 metrics: {'loss': 0.8757, 'grad_norm': 7.15625, 'learning_rate': 4.877089843485218e-05, 'epoch': 1.5170068027210886}\n",
            "\n",
            ">>> Step 1116 metrics: {'loss': 0.8978, 'grad_norm': 9.6875, 'learning_rate': 4.876868926180887e-05, 'epoch': 1.5183673469387755}\n",
            "\n",
            ">>> Step 1117 metrics: {'loss': 1.1458, 'grad_norm': 13.0, 'learning_rate': 4.876647815530207e-05, 'epoch': 1.5197278911564625}\n",
            "\n",
            ">>> Step 1118 metrics: {'loss': 0.5677, 'grad_norm': 6.625, 'learning_rate': 4.8764265115511666e-05, 'epoch': 1.5210884353741496}\n",
            "\n",
            ">>> Step 1119 metrics: {'loss': 0.5348, 'grad_norm': 5.625, 'learning_rate': 4.876205014261766e-05, 'epoch': 1.5224489795918368}\n",
            "\n",
            ">>> Step 1120 metrics: {'loss': 1.5217, 'grad_norm': 14.5625, 'learning_rate': 4.875983323680025e-05, 'epoch': 1.5238095238095237}\n",
            "\n",
            ">>> Step 1121 metrics: {'loss': 0.7993, 'grad_norm': 6.84375, 'learning_rate': 4.8757614398239744e-05, 'epoch': 1.525170068027211}\n",
            "\n",
            ">>> Step 1122 metrics: {'loss': 1.1449, 'grad_norm': 7.875, 'learning_rate': 4.875539362711665e-05, 'epoch': 1.526530612244898}\n",
            "\n",
            ">>> Step 1123 metrics: {'loss': 0.5082, 'grad_norm': 6.09375, 'learning_rate': 4.8753170923611614e-05, 'epoch': 1.527891156462585}\n",
            "\n",
            ">>> Step 1124 metrics: {'loss': 0.5517, 'grad_norm': 6.25, 'learning_rate': 4.875094628790544e-05, 'epoch': 1.5292517006802722}\n",
            "\n",
            ">>> Step 1125 metrics: {'loss': 0.7872, 'grad_norm': 6.59375, 'learning_rate': 4.874871972017909e-05, 'epoch': 1.5306122448979593}\n",
            "\n",
            ">>> Step 1126 metrics: {'loss': 0.6616, 'grad_norm': 6.3125, 'learning_rate': 4.87464912206137e-05, 'epoch': 1.5319727891156463}\n",
            "\n",
            ">>> Step 1127 metrics: {'loss': 1.088, 'grad_norm': 8.625, 'learning_rate': 4.8744260789390516e-05, 'epoch': 1.5333333333333332}\n",
            "\n",
            ">>> Step 1128 metrics: {'loss': 0.6437, 'grad_norm': 5.78125, 'learning_rate': 4.8742028426691e-05, 'epoch': 1.5346938775510204}\n",
            "\n",
            ">>> Step 1129 metrics: {'loss': 0.5525, 'grad_norm': 5.53125, 'learning_rate': 4.873979413269673e-05, 'epoch': 1.5360544217687075}\n",
            "\n",
            ">>> Step 1130 metrics: {'loss': 1.0471, 'grad_norm': 6.84375, 'learning_rate': 4.873755790758946e-05, 'epoch': 1.5374149659863945}\n",
            "\n",
            ">>> Step 1131 metrics: {'loss': 0.6187, 'grad_norm': 7.0, 'learning_rate': 4.873531975155109e-05, 'epoch': 1.5387755102040817}\n",
            "\n",
            ">>> Step 1132 metrics: {'loss': 0.4373, 'grad_norm': 7.25, 'learning_rate': 4.873307966476369e-05, 'epoch': 1.5401360544217688}\n",
            "\n",
            ">>> Step 1133 metrics: {'loss': 0.84, 'grad_norm': 10.1875, 'learning_rate': 4.8730837647409466e-05, 'epoch': 1.5414965986394558}\n",
            "\n",
            ">>> Step 1134 metrics: {'loss': 0.7766, 'grad_norm': 7.28125, 'learning_rate': 4.872859369967082e-05, 'epoch': 1.5428571428571427}\n",
            "\n",
            ">>> Step 1135 metrics: {'loss': 0.6539, 'grad_norm': 6.03125, 'learning_rate': 4.8726347821730254e-05, 'epoch': 1.54421768707483}\n",
            "\n",
            ">>> Step 1136 metrics: {'loss': 0.6479, 'grad_norm': 6.53125, 'learning_rate': 4.872410001377049e-05, 'epoch': 1.545578231292517}\n",
            "\n",
            ">>> Step 1137 metrics: {'loss': 0.4117, 'grad_norm': 5.46875, 'learning_rate': 4.8721850275974354e-05, 'epoch': 1.546938775510204}\n",
            "\n",
            ">>> Step 1138 metrics: {'loss': 0.5464, 'grad_norm': 6.09375, 'learning_rate': 4.871959860852486e-05, 'epoch': 1.5482993197278911}\n",
            "\n",
            ">>> Step 1139 metrics: {'loss': 0.6558, 'grad_norm': 6.0, 'learning_rate': 4.871734501160517e-05, 'epoch': 1.5496598639455783}\n",
            "\n",
            ">>> Step 1140 metrics: {'loss': 0.6291, 'grad_norm': 5.375, 'learning_rate': 4.87150894853986e-05, 'epoch': 1.5510204081632653}\n",
            "\n",
            ">>> Step 1141 metrics: {'loss': 0.5857, 'grad_norm': 5.3125, 'learning_rate': 4.871283203008863e-05, 'epoch': 1.5523809523809524}\n",
            "\n",
            ">>> Step 1142 metrics: {'loss': 0.491, 'grad_norm': 6.25, 'learning_rate': 4.871057264585888e-05, 'epoch': 1.5537414965986396}\n",
            "\n",
            ">>> Step 1143 metrics: {'loss': 1.17, 'grad_norm': 7.84375, 'learning_rate': 4.870831133289315e-05, 'epoch': 1.5551020408163265}\n",
            "\n",
            ">>> Step 1144 metrics: {'loss': 0.8525, 'grad_norm': 7.4375, 'learning_rate': 4.8706048091375396e-05, 'epoch': 1.5564625850340135}\n",
            "\n",
            ">>> Step 1145 metrics: {'loss': 0.6543, 'grad_norm': 7.40625, 'learning_rate': 4.87037829214897e-05, 'epoch': 1.5578231292517006}\n",
            "\n",
            ">>> Step 1146 metrics: {'loss': 1.0788, 'grad_norm': 7.1875, 'learning_rate': 4.870151582342034e-05, 'epoch': 1.5591836734693878}\n",
            "\n",
            ">>> Step 1147 metrics: {'loss': 0.7736, 'grad_norm': 6.625, 'learning_rate': 4.869924679735173e-05, 'epoch': 1.5605442176870747}\n",
            "\n",
            ">>> Step 1148 metrics: {'loss': 0.293, 'grad_norm': 6.75, 'learning_rate': 4.8696975843468426e-05, 'epoch': 1.561904761904762}\n",
            "\n",
            ">>> Step 1149 metrics: {'loss': 0.7881, 'grad_norm': 6.78125, 'learning_rate': 4.8694702961955185e-05, 'epoch': 1.563265306122449}\n",
            "\n",
            ">>> Step 1150 metrics: {'loss': 1.0169, 'grad_norm': 7.53125, 'learning_rate': 4.869242815299687e-05, 'epoch': 1.564625850340136}\n",
            "\n",
            ">>> Step 1151 metrics: {'loss': 0.988, 'grad_norm': 6.78125, 'learning_rate': 4.8690151416778546e-05, 'epoch': 1.5659863945578232}\n",
            "\n",
            ">>> Step 1152 metrics: {'loss': 0.6247, 'grad_norm': 6.875, 'learning_rate': 4.8687872753485404e-05, 'epoch': 1.5673469387755103}\n",
            "\n",
            ">>> Step 1153 metrics: {'loss': 0.7824, 'grad_norm': 6.0, 'learning_rate': 4.86855921633028e-05, 'epoch': 1.5687074829931973}\n",
            "\n",
            ">>> Step 1154 metrics: {'loss': 0.6788, 'grad_norm': 6.125, 'learning_rate': 4.868330964641625e-05, 'epoch': 1.5700680272108842}\n",
            "\n",
            ">>> Step 1155 metrics: {'loss': 0.9721, 'grad_norm': 8.4375, 'learning_rate': 4.8681025203011434e-05, 'epoch': 1.5714285714285714}\n",
            "\n",
            ">>> Step 1156 metrics: {'loss': 0.7028, 'grad_norm': 6.875, 'learning_rate': 4.8678738833274174e-05, 'epoch': 1.5727891156462586}\n",
            "\n",
            ">>> Step 1157 metrics: {'loss': 0.5121, 'grad_norm': 5.53125, 'learning_rate': 4.867645053739045e-05, 'epoch': 1.5741496598639455}\n",
            "\n",
            ">>> Step 1158 metrics: {'loss': 0.6409, 'grad_norm': 6.3125, 'learning_rate': 4.86741603155464e-05, 'epoch': 1.5755102040816327}\n",
            "\n",
            ">>> Step 1159 metrics: {'loss': 0.5906, 'grad_norm': 9.0, 'learning_rate': 4.867186816792834e-05, 'epoch': 1.5768707482993198}\n",
            "\n",
            ">>> Step 1160 metrics: {'loss': 0.9931, 'grad_norm': 7.6875, 'learning_rate': 4.8669574094722706e-05, 'epoch': 1.5782312925170068}\n",
            "\n",
            ">>> Step 1161 metrics: {'loss': 0.6547, 'grad_norm': 5.875, 'learning_rate': 4.866727809611612e-05, 'epoch': 1.5795918367346937}\n",
            "\n",
            ">>> Step 1162 metrics: {'loss': 1.1365, 'grad_norm': 9.5625, 'learning_rate': 4.866498017229535e-05, 'epoch': 1.580952380952381}\n",
            "\n",
            ">>> Step 1163 metrics: {'loss': 0.847, 'grad_norm': 6.84375, 'learning_rate': 4.866268032344732e-05, 'epoch': 1.582312925170068}\n",
            "\n",
            ">>> Step 1164 metrics: {'loss': 0.6068, 'grad_norm': 6.5625, 'learning_rate': 4.86603785497591e-05, 'epoch': 1.583673469387755}\n",
            "\n",
            ">>> Step 1165 metrics: {'loss': 0.3896, 'grad_norm': 6.09375, 'learning_rate': 4.865807485141794e-05, 'epoch': 1.5850340136054422}\n",
            "\n",
            ">>> Step 1166 metrics: {'loss': 0.5361, 'grad_norm': 6.21875, 'learning_rate': 4.865576922861123e-05, 'epoch': 1.5863945578231293}\n",
            "\n",
            ">>> Step 1167 metrics: {'loss': 0.5959, 'grad_norm': 5.40625, 'learning_rate': 4.865346168152652e-05, 'epoch': 1.5877551020408163}\n",
            "\n",
            ">>> Step 1168 metrics: {'loss': 0.853, 'grad_norm': 6.0, 'learning_rate': 4.8651152210351527e-05, 'epoch': 1.5891156462585034}\n",
            "\n",
            ">>> Step 1169 metrics: {'loss': 0.3794, 'grad_norm': 5.15625, 'learning_rate': 4.86488408152741e-05, 'epoch': 1.5904761904761906}\n",
            "\n",
            ">>> Step 1170 metrics: {'loss': 0.6951, 'grad_norm': 6.21875, 'learning_rate': 4.8646527496482266e-05, 'epoch': 1.5918367346938775}\n",
            "\n",
            ">>> Step 1171 metrics: {'loss': 0.4752, 'grad_norm': 5.1875, 'learning_rate': 4.8644212254164206e-05, 'epoch': 1.5931972789115645}\n",
            "\n",
            ">>> Step 1172 metrics: {'loss': 0.8083, 'grad_norm': 6.34375, 'learning_rate': 4.864189508850825e-05, 'epoch': 1.5945578231292517}\n",
            "\n",
            ">>> Step 1173 metrics: {'loss': 0.4545, 'grad_norm': 6.0625, 'learning_rate': 4.863957599970288e-05, 'epoch': 1.5959183673469388}\n",
            "\n",
            ">>> Step 1174 metrics: {'loss': 0.5655, 'grad_norm': 6.4375, 'learning_rate': 4.863725498793675e-05, 'epoch': 1.5972789115646258}\n",
            "\n",
            ">>> Step 1175 metrics: {'loss': 0.6731, 'grad_norm': 7.59375, 'learning_rate': 4.8634932053398677e-05, 'epoch': 1.598639455782313}\n",
            "\n",
            ">>> Step 1176 metrics: {'loss': 0.7614, 'grad_norm': 6.78125, 'learning_rate': 4.863260719627759e-05, 'epoch': 1.6}\n",
            "\n",
            ">>> Step 1177 metrics: {'loss': 0.5955, 'grad_norm': 6.25, 'learning_rate': 4.8630280416762625e-05, 'epoch': 1.601360544217687}\n",
            "\n",
            ">>> Step 1178 metrics: {'loss': 0.4364, 'grad_norm': 5.03125, 'learning_rate': 4.8627951715043044e-05, 'epoch': 1.6027210884353742}\n",
            "\n",
            ">>> Step 1179 metrics: {'loss': 0.743, 'grad_norm': 6.8125, 'learning_rate': 4.862562109130828e-05, 'epoch': 1.6040816326530614}\n",
            "\n",
            ">>> Step 1180 metrics: {'loss': 0.9461, 'grad_norm': 7.875, 'learning_rate': 4.862328854574792e-05, 'epoch': 1.6054421768707483}\n",
            "\n",
            ">>> Step 1181 metrics: {'loss': 0.6754, 'grad_norm': 8.4375, 'learning_rate': 4.86209540785517e-05, 'epoch': 1.6068027210884352}\n",
            "\n",
            ">>> Step 1182 metrics: {'loss': 0.9049, 'grad_norm': 7.34375, 'learning_rate': 4.861861768990953e-05, 'epoch': 1.6081632653061224}\n",
            "\n",
            ">>> Step 1183 metrics: {'loss': 0.5109, 'grad_norm': 5.625, 'learning_rate': 4.8616279380011445e-05, 'epoch': 1.6095238095238096}\n",
            "\n",
            ">>> Step 1184 metrics: {'loss': 0.6338, 'grad_norm': 6.78125, 'learning_rate': 4.8613939149047664e-05, 'epoch': 1.6108843537414965}\n",
            "\n",
            ">>> Step 1185 metrics: {'loss': 0.5144, 'grad_norm': 5.875, 'learning_rate': 4.8611596997208544e-05, 'epoch': 1.6122448979591837}\n",
            "\n",
            ">>> Step 1186 metrics: {'loss': 0.9403, 'grad_norm': 6.84375, 'learning_rate': 4.8609252924684616e-05, 'epoch': 1.6136054421768709}\n",
            "\n",
            ">>> Step 1187 metrics: {'loss': 0.6891, 'grad_norm': 6.625, 'learning_rate': 4.860690693166656e-05, 'epoch': 1.6149659863945578}\n",
            "\n",
            ">>> Step 1188 metrics: {'loss': 0.6519, 'grad_norm': 6.4375, 'learning_rate': 4.860455901834521e-05, 'epoch': 1.6163265306122447}\n",
            "\n",
            ">>> Step 1189 metrics: {'loss': 0.5294, 'grad_norm': 5.53125, 'learning_rate': 4.860220918491155e-05, 'epoch': 1.6176870748299321}\n",
            "\n",
            ">>> Step 1190 metrics: {'loss': 0.4791, 'grad_norm': 6.65625, 'learning_rate': 4.859985743155674e-05, 'epoch': 1.619047619047619}\n",
            "\n",
            ">>> Step 1191 metrics: {'loss': 0.6421, 'grad_norm': 6.40625, 'learning_rate': 4.859750375847207e-05, 'epoch': 1.620408163265306}\n",
            "\n",
            ">>> Step 1192 metrics: {'loss': 0.5574, 'grad_norm': 5.96875, 'learning_rate': 4.859514816584901e-05, 'epoch': 1.6217687074829932}\n",
            "\n",
            ">>> Step 1193 metrics: {'loss': 0.7235, 'grad_norm': 6.5625, 'learning_rate': 4.859279065387916e-05, 'epoch': 1.6231292517006803}\n",
            "\n",
            ">>> Step 1194 metrics: {'loss': 0.5976, 'grad_norm': 6.46875, 'learning_rate': 4.859043122275431e-05, 'epoch': 1.6244897959183673}\n",
            "\n",
            ">>> Step 1195 metrics: {'loss': 0.9228, 'grad_norm': 7.46875, 'learning_rate': 4.8588069872666385e-05, 'epoch': 1.6258503401360545}\n",
            "\n",
            ">>> Step 1196 metrics: {'loss': 0.4902, 'grad_norm': 6.4375, 'learning_rate': 4.858570660380746e-05, 'epoch': 1.6272108843537416}\n",
            "\n",
            ">>> Step 1197 metrics: {'loss': 0.7428, 'grad_norm': 7.6875, 'learning_rate': 4.858334141636978e-05, 'epoch': 1.6285714285714286}\n",
            "\n",
            ">>> Step 1198 metrics: {'loss': 1.003, 'grad_norm': 7.15625, 'learning_rate': 4.8580974310545735e-05, 'epoch': 1.6299319727891155}\n",
            "\n",
            ">>> Step 1199 metrics: {'loss': 1.1029, 'grad_norm': 6.5625, 'learning_rate': 4.857860528652788e-05, 'epoch': 1.6312925170068027}\n",
            "\n",
            ">>> Step 1200 metrics: {'loss': 0.8948, 'grad_norm': 7.25, 'learning_rate': 4.857623434450893e-05, 'epoch': 1.6326530612244898}\n",
            "\n",
            ">>> Step 1201 metrics: {'loss': 0.8834, 'grad_norm': 6.71875, 'learning_rate': 4.857386148468175e-05, 'epoch': 1.6340136054421768}\n",
            "\n",
            ">>> Step 1202 metrics: {'loss': 0.4267, 'grad_norm': 5.25, 'learning_rate': 4.857148670723935e-05, 'epoch': 1.635374149659864}\n",
            "\n",
            ">>> Step 1203 metrics: {'loss': 1.265, 'grad_norm': 6.75, 'learning_rate': 4.856911001237491e-05, 'epoch': 1.636734693877551}\n",
            "\n",
            ">>> Step 1204 metrics: {'loss': 0.5539, 'grad_norm': 4.8125, 'learning_rate': 4.8566731400281765e-05, 'epoch': 1.638095238095238}\n",
            "\n",
            ">>> Step 1205 metrics: {'loss': 0.6935, 'grad_norm': 6.59375, 'learning_rate': 4.8564350871153405e-05, 'epoch': 1.6394557823129252}\n",
            "\n",
            ">>> Step 1206 metrics: {'loss': 1.2825, 'grad_norm': 7.90625, 'learning_rate': 4.856196842518347e-05, 'epoch': 1.6408163265306124}\n",
            "\n",
            ">>> Step 1207 metrics: {'loss': 0.8642, 'grad_norm': 13.1875, 'learning_rate': 4.8559584062565755e-05, 'epoch': 1.6421768707482993}\n",
            "\n",
            ">>> Step 1208 metrics: {'loss': 0.8151, 'grad_norm': 7.8125, 'learning_rate': 4.855719778349423e-05, 'epoch': 1.6435374149659863}\n",
            "\n",
            ">>> Step 1209 metrics: {'loss': 1.0, 'grad_norm': 7.59375, 'learning_rate': 4.8554809588163e-05, 'epoch': 1.6448979591836734}\n",
            "\n",
            ">>> Step 1210 metrics: {'loss': 0.4461, 'grad_norm': 6.5625, 'learning_rate': 4.855241947676632e-05, 'epoch': 1.6462585034013606}\n",
            "\n",
            ">>> Step 1211 metrics: {'loss': 0.9874, 'grad_norm': 6.8125, 'learning_rate': 4.855002744949863e-05, 'epoch': 1.6476190476190475}\n",
            "\n",
            ">>> Step 1212 metrics: {'loss': 0.6933, 'grad_norm': 6.34375, 'learning_rate': 4.85476335065545e-05, 'epoch': 1.6489795918367347}\n",
            "\n",
            ">>> Step 1213 metrics: {'loss': 0.9699, 'grad_norm': 6.09375, 'learning_rate': 4.8545237648128685e-05, 'epoch': 1.6503401360544219}\n",
            "\n",
            ">>> Step 1214 metrics: {'loss': 0.5165, 'grad_norm': 6.5625, 'learning_rate': 4.8542839874416044e-05, 'epoch': 1.6517006802721088}\n",
            "\n",
            ">>> Step 1215 metrics: {'loss': 1.097, 'grad_norm': 7.3125, 'learning_rate': 4.854044018561165e-05, 'epoch': 1.6530612244897958}\n",
            "\n",
            ">>> Step 1216 metrics: {'loss': 0.4645, 'grad_norm': 4.875, 'learning_rate': 4.8538038581910685e-05, 'epoch': 1.6544217687074831}\n",
            "\n",
            ">>> Step 1217 metrics: {'loss': 1.0277, 'grad_norm': 6.75, 'learning_rate': 4.853563506350853e-05, 'epoch': 1.65578231292517}\n",
            "\n",
            ">>> Step 1218 metrics: {'loss': 0.4669, 'grad_norm': 5.09375, 'learning_rate': 4.853322963060068e-05, 'epoch': 1.657142857142857}\n",
            "\n",
            ">>> Step 1219 metrics: {'loss': 0.7354, 'grad_norm': 7.1875, 'learning_rate': 4.8530822283382814e-05, 'epoch': 1.6585034013605442}\n",
            "\n",
            ">>> Step 1220 metrics: {'loss': 0.621, 'grad_norm': 6.625, 'learning_rate': 4.852841302205076e-05, 'epoch': 1.6598639455782314}\n",
            "\n",
            ">>> Step 1221 metrics: {'loss': 0.7107, 'grad_norm': 6.21875, 'learning_rate': 4.852600184680049e-05, 'epoch': 1.6612244897959183}\n",
            "\n",
            ">>> Step 1222 metrics: {'loss': 0.8389, 'grad_norm': 7.1875, 'learning_rate': 4.8523588757828145e-05, 'epoch': 1.6625850340136055}\n",
            "\n",
            ">>> Step 1223 metrics: {'loss': 0.5864, 'grad_norm': 5.875, 'learning_rate': 4.852117375533003e-05, 'epoch': 1.6639455782312926}\n",
            "\n",
            ">>> Step 1224 metrics: {'loss': 0.5732, 'grad_norm': 6.375, 'learning_rate': 4.8518756839502574e-05, 'epoch': 1.6653061224489796}\n",
            "\n",
            ">>> Step 1225 metrics: {'loss': 1.5801, 'grad_norm': 10.8125, 'learning_rate': 4.851633801054238e-05, 'epoch': 1.6666666666666665}\n",
            "\n",
            ">>> Step 1226 metrics: {'loss': 0.7762, 'grad_norm': 6.40625, 'learning_rate': 4.851391726864623e-05, 'epoch': 1.668027210884354}\n",
            "\n",
            ">>> Step 1227 metrics: {'loss': 0.4589, 'grad_norm': 5.4375, 'learning_rate': 4.851149461401102e-05, 'epoch': 1.6693877551020408}\n",
            "\n",
            ">>> Step 1228 metrics: {'loss': 0.896, 'grad_norm': 7.34375, 'learning_rate': 4.850907004683384e-05, 'epoch': 1.6707482993197278}\n",
            "\n",
            ">>> Step 1229 metrics: {'loss': 1.0019, 'grad_norm': 8.125, 'learning_rate': 4.850664356731189e-05, 'epoch': 1.672108843537415}\n",
            "\n",
            ">>> Step 1230 metrics: {'loss': 0.6913, 'grad_norm': 6.34375, 'learning_rate': 4.8504215175642565e-05, 'epoch': 1.6734693877551021}\n",
            "\n",
            ">>> Step 1231 metrics: {'loss': 0.5452, 'grad_norm': 5.6875, 'learning_rate': 4.85017848720234e-05, 'epoch': 1.674829931972789}\n",
            "\n",
            ">>> Step 1232 metrics: {'loss': 0.4267, 'grad_norm': 5.125, 'learning_rate': 4.8499352656652095e-05, 'epoch': 1.6761904761904762}\n",
            "\n",
            ">>> Step 1233 metrics: {'loss': 0.6465, 'grad_norm': 6.4375, 'learning_rate': 4.84969185297265e-05, 'epoch': 1.6775510204081634}\n",
            "\n",
            ">>> Step 1234 metrics: {'loss': 1.1028, 'grad_norm': 7.5625, 'learning_rate': 4.849448249144461e-05, 'epoch': 1.6789115646258503}\n",
            "\n",
            ">>> Step 1235 metrics: {'loss': 1.2893, 'grad_norm': 7.96875, 'learning_rate': 4.849204454200458e-05, 'epoch': 1.6802721088435373}\n",
            "\n",
            ">>> Step 1236 metrics: {'loss': 0.9473, 'grad_norm': 7.53125, 'learning_rate': 4.848960468160474e-05, 'epoch': 1.6816326530612244}\n",
            "\n",
            ">>> Step 1237 metrics: {'loss': 0.6875, 'grad_norm': 6.0, 'learning_rate': 4.848716291044355e-05, 'epoch': 1.6829931972789116}\n",
            "\n",
            ">>> Step 1238 metrics: {'loss': 0.7597, 'grad_norm': 5.78125, 'learning_rate': 4.8484719228719643e-05, 'epoch': 1.6843537414965986}\n",
            "\n",
            ">>> Step 1239 metrics: {'loss': 1.0061, 'grad_norm': 6.65625, 'learning_rate': 4.8482273636631794e-05, 'epoch': 1.6857142857142857}\n",
            "\n",
            ">>> Step 1240 metrics: {'loss': 0.334, 'grad_norm': 6.8125, 'learning_rate': 4.847982613437894e-05, 'epoch': 1.6870748299319729}\n",
            "\n",
            ">>> Step 1241 metrics: {'loss': 1.076, 'grad_norm': 8.625, 'learning_rate': 4.847737672216017e-05, 'epoch': 1.6884353741496598}\n",
            "\n",
            ">>> Step 1242 metrics: {'loss': 0.6274, 'grad_norm': 6.84375, 'learning_rate': 4.847492540017474e-05, 'epoch': 1.689795918367347}\n",
            "\n",
            ">>> Step 1243 metrics: {'loss': 1.1907, 'grad_norm': 8.25, 'learning_rate': 4.847247216862205e-05, 'epoch': 1.6911564625850342}\n",
            "\n",
            ">>> Step 1244 metrics: {'loss': 0.4563, 'grad_norm': 5.78125, 'learning_rate': 4.847001702770165e-05, 'epoch': 1.692517006802721}\n",
            "\n",
            ">>> Step 1245 metrics: {'loss': 0.809, 'grad_norm': 7.65625, 'learning_rate': 4.846755997761327e-05, 'epoch': 1.693877551020408}\n",
            "\n",
            ">>> Step 1246 metrics: {'loss': 0.3236, 'grad_norm': 5.46875, 'learning_rate': 4.8465101018556765e-05, 'epoch': 1.6952380952380952}\n",
            "\n",
            ">>> Step 1247 metrics: {'loss': 0.5393, 'grad_norm': 5.90625, 'learning_rate': 4.8462640150732164e-05, 'epoch': 1.6965986394557824}\n",
            "\n",
            ">>> Step 1248 metrics: {'loss': 0.9029, 'grad_norm': 7.9375, 'learning_rate': 4.8460177374339636e-05, 'epoch': 1.6979591836734693}\n",
            "\n",
            ">>> Step 1249 metrics: {'loss': 0.6798, 'grad_norm': 6.75, 'learning_rate': 4.845771268957954e-05, 'epoch': 1.6993197278911565}\n",
            "\n",
            ">>> Step 1250 metrics: {'loss': 0.5211, 'grad_norm': 6.40625, 'learning_rate': 4.845524609665234e-05, 'epoch': 1.7006802721088436}\n",
            "\n",
            ">>> Step 1251 metrics: {'loss': 0.3909, 'grad_norm': 5.09375, 'learning_rate': 4.8452777595758694e-05, 'epoch': 1.7020408163265306}\n",
            "\n",
            ">>> Step 1252 metrics: {'loss': 0.3444, 'grad_norm': 5.15625, 'learning_rate': 4.845030718709939e-05, 'epoch': 1.7034013605442175}\n",
            "\n",
            ">>> Step 1253 metrics: {'loss': 0.4019, 'grad_norm': 6.6875, 'learning_rate': 4.84478348708754e-05, 'epoch': 1.704761904761905}\n",
            "\n",
            ">>> Step 1254 metrics: {'loss': 0.5832, 'grad_norm': 7.1875, 'learning_rate': 4.844536064728783e-05, 'epoch': 1.7061224489795919}\n",
            "\n",
            ">>> Step 1255 metrics: {'loss': 0.3315, 'grad_norm': 5.1875, 'learning_rate': 4.8442884516537936e-05, 'epoch': 1.7074829931972788}\n",
            "\n",
            ">>> Step 1256 metrics: {'loss': 0.4235, 'grad_norm': 5.40625, 'learning_rate': 4.844040647882715e-05, 'epoch': 1.708843537414966}\n",
            "\n",
            ">>> Step 1257 metrics: {'loss': 0.5694, 'grad_norm': 6.6875, 'learning_rate': 4.843792653435705e-05, 'epoch': 1.7102040816326531}\n",
            "\n",
            ">>> Step 1258 metrics: {'loss': 0.932, 'grad_norm': 7.25, 'learning_rate': 4.843544468332935e-05, 'epoch': 1.71156462585034}\n",
            "\n",
            ">>> Step 1259 metrics: {'loss': 0.4803, 'grad_norm': 6.0625, 'learning_rate': 4.843296092594595e-05, 'epoch': 1.7129251700680272}\n",
            "\n",
            ">>> Step 1260 metrics: {'loss': 1.1001, 'grad_norm': 7.09375, 'learning_rate': 4.843047526240889e-05, 'epoch': 1.7142857142857144}\n",
            "\n",
            ">>> Step 1261 metrics: {'loss': 0.882, 'grad_norm': 7.0625, 'learning_rate': 4.842798769292036e-05, 'epoch': 1.7156462585034014}\n",
            "\n",
            ">>> Step 1262 metrics: {'loss': 0.9379, 'grad_norm': 5.875, 'learning_rate': 4.842549821768272e-05, 'epoch': 1.7170068027210883}\n",
            "\n",
            ">>> Step 1263 metrics: {'loss': 1.0704, 'grad_norm': 6.78125, 'learning_rate': 4.8423006836898466e-05, 'epoch': 1.7183673469387755}\n",
            "\n",
            ">>> Step 1264 metrics: {'loss': 0.4132, 'grad_norm': 4.90625, 'learning_rate': 4.8420513550770276e-05, 'epoch': 1.7197278911564626}\n",
            "\n",
            ">>> Step 1265 metrics: {'loss': 0.4507, 'grad_norm': 6.1875, 'learning_rate': 4.841801835950094e-05, 'epoch': 1.7210884353741496}\n",
            "\n",
            ">>> Step 1266 metrics: {'loss': 0.5927, 'grad_norm': 6.28125, 'learning_rate': 4.841552126329346e-05, 'epoch': 1.7224489795918367}\n",
            "\n",
            ">>> Step 1267 metrics: {'loss': 0.4845, 'grad_norm': 4.84375, 'learning_rate': 4.841302226235094e-05, 'epoch': 1.723809523809524}\n",
            "\n",
            ">>> Step 1268 metrics: {'loss': 0.5845, 'grad_norm': 5.5625, 'learning_rate': 4.841052135687666e-05, 'epoch': 1.7251700680272108}\n",
            "\n",
            ">>> Step 1269 metrics: {'loss': 0.5615, 'grad_norm': 5.96875, 'learning_rate': 4.8408018547074075e-05, 'epoch': 1.726530612244898}\n",
            "\n",
            ">>> Step 1270 metrics: {'loss': 0.5258, 'grad_norm': 6.59375, 'learning_rate': 4.8405513833146755e-05, 'epoch': 1.7278911564625852}\n",
            "\n",
            ">>> Step 1271 metrics: {'loss': 1.0191, 'grad_norm': 6.75, 'learning_rate': 4.8403007215298465e-05, 'epoch': 1.7292517006802721}\n",
            "\n",
            ">>> Step 1272 metrics: {'loss': 0.3012, 'grad_norm': 4.84375, 'learning_rate': 4.8400498693733096e-05, 'epoch': 1.730612244897959}\n",
            "\n",
            ">>> Step 1273 metrics: {'loss': 0.7401, 'grad_norm': 5.90625, 'learning_rate': 4.83979882686547e-05, 'epoch': 1.7319727891156462}\n",
            "\n",
            ">>> Step 1274 metrics: {'loss': 0.8904, 'grad_norm': 5.9375, 'learning_rate': 4.8395475940267496e-05, 'epoch': 1.7333333333333334}\n",
            "\n",
            ">>> Step 1275 metrics: {'loss': 0.8218, 'grad_norm': 7.4375, 'learning_rate': 4.839296170877584e-05, 'epoch': 1.7346938775510203}\n",
            "\n",
            ">>> Step 1276 metrics: {'loss': 0.8013, 'grad_norm': 6.84375, 'learning_rate': 4.8390445574384256e-05, 'epoch': 1.7360544217687075}\n",
            "\n",
            ">>> Step 1277 metrics: {'loss': 0.5744, 'grad_norm': 6.6875, 'learning_rate': 4.8387927537297425e-05, 'epoch': 1.7374149659863947}\n",
            "\n",
            ">>> Step 1278 metrics: {'loss': 0.695, 'grad_norm': 6.5625, 'learning_rate': 4.8385407597720175e-05, 'epoch': 1.7387755102040816}\n",
            "\n",
            ">>> Step 1279 metrics: {'loss': 0.479, 'grad_norm': 5.75, 'learning_rate': 4.838288575585748e-05, 'epoch': 1.7401360544217686}\n",
            "\n",
            ">>> Step 1280 metrics: {'loss': 0.999, 'grad_norm': 7.90625, 'learning_rate': 4.838036201191449e-05, 'epoch': 1.741496598639456}\n",
            "\n",
            ">>> Step 1281 metrics: {'loss': 0.661, 'grad_norm': 5.90625, 'learning_rate': 4.83778363660965e-05, 'epoch': 1.7428571428571429}\n",
            "\n",
            ">>> Step 1282 metrics: {'loss': 0.5801, 'grad_norm': 5.75, 'learning_rate': 4.837530881860895e-05, 'epoch': 1.7442176870748298}\n",
            "\n",
            ">>> Step 1283 metrics: {'loss': 0.7109, 'grad_norm': 5.71875, 'learning_rate': 4.837277936965744e-05, 'epoch': 1.745578231292517}\n",
            "\n",
            ">>> Step 1284 metrics: {'loss': 0.7079, 'grad_norm': 6.25, 'learning_rate': 4.837024801944774e-05, 'epoch': 1.7469387755102042}\n",
            "\n",
            ">>> Step 1285 metrics: {'loss': 0.6832, 'grad_norm': 7.15625, 'learning_rate': 4.836771476818576e-05, 'epoch': 1.748299319727891}\n",
            "\n",
            ">>> Step 1286 metrics: {'loss': 0.6522, 'grad_norm': 6.0625, 'learning_rate': 4.836517961607756e-05, 'epoch': 1.7496598639455783}\n",
            "\n",
            ">>> Step 1287 metrics: {'loss': 0.4718, 'grad_norm': 6.5, 'learning_rate': 4.836264256332938e-05, 'epoch': 1.7510204081632654}\n",
            "\n",
            ">>> Step 1288 metrics: {'loss': 0.8281, 'grad_norm': 6.8125, 'learning_rate': 4.8360103610147566e-05, 'epoch': 1.7523809523809524}\n",
            "\n",
            ">>> Step 1289 metrics: {'loss': 0.9507, 'grad_norm': 6.625, 'learning_rate': 4.8357562756738675e-05, 'epoch': 1.7537414965986393}\n",
            "\n",
            ">>> Step 1290 metrics: {'loss': 1.0666, 'grad_norm': 6.9375, 'learning_rate': 4.835502000330938e-05, 'epoch': 1.7551020408163265}\n",
            "\n",
            ">>> Step 1291 metrics: {'loss': 0.5999, 'grad_norm': 6.21875, 'learning_rate': 4.835247535006653e-05, 'epoch': 1.7564625850340136}\n",
            "\n",
            ">>> Step 1292 metrics: {'loss': 0.6555, 'grad_norm': 6.84375, 'learning_rate': 4.8349928797217116e-05, 'epoch': 1.7578231292517006}\n",
            "\n",
            ">>> Step 1293 metrics: {'loss': 0.9082, 'grad_norm': 7.125, 'learning_rate': 4.8347380344968285e-05, 'epoch': 1.7591836734693878}\n",
            "\n",
            ">>> Step 1294 metrics: {'loss': 0.6486, 'grad_norm': 5.96875, 'learning_rate': 4.834482999352734e-05, 'epoch': 1.760544217687075}\n",
            "\n",
            ">>> Step 1295 metrics: {'loss': 0.4681, 'grad_norm': 5.65625, 'learning_rate': 4.8342277743101744e-05, 'epoch': 1.7619047619047619}\n",
            "\n",
            ">>> Step 1296 metrics: {'loss': 1.1814, 'grad_norm': 9.75, 'learning_rate': 4.8339723593899106e-05, 'epoch': 1.763265306122449}\n",
            "\n",
            ">>> Step 1297 metrics: {'loss': 0.9242, 'grad_norm': 6.40625, 'learning_rate': 4.8337167546127195e-05, 'epoch': 1.7646258503401362}\n",
            "\n",
            ">>> Step 1298 metrics: {'loss': 0.7698, 'grad_norm': 6.5, 'learning_rate': 4.833460959999393e-05, 'epoch': 1.7659863945578231}\n",
            "\n",
            ">>> Step 1299 metrics: {'loss': 0.6034, 'grad_norm': 6.6875, 'learning_rate': 4.833204975570739e-05, 'epoch': 1.76734693877551}\n",
            "\n",
            ">>> Step 1300 metrics: {'loss': 1.3744, 'grad_norm': 7.375, 'learning_rate': 4.832948801347581e-05, 'epoch': 1.7687074829931972}\n",
            "\n",
            ">>> Step 1301 metrics: {'loss': 0.4701, 'grad_norm': 5.5, 'learning_rate': 4.8326924373507566e-05, 'epoch': 1.7700680272108844}\n",
            "\n",
            ">>> Step 1302 metrics: {'loss': 0.6541, 'grad_norm': 6.34375, 'learning_rate': 4.83243588360112e-05, 'epoch': 1.7714285714285714}\n",
            "\n",
            ">>> Step 1303 metrics: {'loss': 0.3865, 'grad_norm': 4.75, 'learning_rate': 4.8321791401195405e-05, 'epoch': 1.7727891156462585}\n",
            "\n",
            ">>> Step 1304 metrics: {'loss': 0.702, 'grad_norm': 5.75, 'learning_rate': 4.831922206926903e-05, 'epoch': 1.7741496598639457}\n",
            "\n",
            ">>> Step 1305 metrics: {'loss': 1.059, 'grad_norm': 6.625, 'learning_rate': 4.831665084044108e-05, 'epoch': 1.7755102040816326}\n",
            "\n",
            ">>> Step 1306 metrics: {'loss': 0.4627, 'grad_norm': 9.0625, 'learning_rate': 4.83140777149207e-05, 'epoch': 1.7768707482993196}\n",
            "\n",
            ">>> Step 1307 metrics: {'loss': 0.5411, 'grad_norm': 5.53125, 'learning_rate': 4.831150269291722e-05, 'epoch': 1.778231292517007}\n",
            "\n",
            ">>> Step 1308 metrics: {'loss': 0.3979, 'grad_norm': 5.125, 'learning_rate': 4.830892577464009e-05, 'epoch': 1.779591836734694}\n",
            "\n",
            ">>> Step 1309 metrics: {'loss': 0.6112, 'grad_norm': 6.375, 'learning_rate': 4.830634696029894e-05, 'epoch': 1.7809523809523808}\n",
            "\n",
            ">>> Step 1310 metrics: {'loss': 1.0732, 'grad_norm': 7.625, 'learning_rate': 4.830376625010353e-05, 'epoch': 1.782312925170068}\n",
            "\n",
            ">>> Step 1311 metrics: {'loss': 0.5117, 'grad_norm': 5.78125, 'learning_rate': 4.83011836442638e-05, 'epoch': 1.7836734693877552}\n",
            "\n",
            ">>> Step 1312 metrics: {'loss': 0.8203, 'grad_norm': 6.46875, 'learning_rate': 4.829859914298983e-05, 'epoch': 1.7850340136054421}\n",
            "\n",
            ">>> Step 1313 metrics: {'loss': 0.8781, 'grad_norm': 7.21875, 'learning_rate': 4.829601274649186e-05, 'epoch': 1.7863945578231293}\n",
            "\n",
            ">>> Step 1314 metrics: {'loss': 0.3623, 'grad_norm': 6.34375, 'learning_rate': 4.8293424454980264e-05, 'epoch': 1.7877551020408164}\n",
            "\n",
            ">>> Step 1315 metrics: {'loss': 0.7431, 'grad_norm': 5.6875, 'learning_rate': 4.82908342686656e-05, 'epoch': 1.7891156462585034}\n",
            "\n",
            ">>> Step 1316 metrics: {'loss': 1.0801, 'grad_norm': 7.46875, 'learning_rate': 4.828824218775856e-05, 'epoch': 1.7904761904761903}\n",
            "\n",
            ">>> Step 1317 metrics: {'loss': 0.5972, 'grad_norm': 5.5625, 'learning_rate': 4.8285648212470004e-05, 'epoch': 1.7918367346938775}\n",
            "\n",
            ">>> Step 1318 metrics: {'loss': 0.5935, 'grad_norm': 6.90625, 'learning_rate': 4.828305234301094e-05, 'epoch': 1.7931972789115647}\n",
            "\n",
            ">>> Step 1319 metrics: {'loss': 0.6499, 'grad_norm': 6.0, 'learning_rate': 4.828045457959251e-05, 'epoch': 1.7945578231292516}\n",
            "\n",
            ">>> Step 1320 metrics: {'loss': 0.645, 'grad_norm': 6.28125, 'learning_rate': 4.827785492242605e-05, 'epoch': 1.7959183673469388}\n",
            "\n",
            ">>> Step 1321 metrics: {'loss': 0.9318, 'grad_norm': 6.96875, 'learning_rate': 4.827525337172303e-05, 'epoch': 1.797278911564626}\n",
            "\n",
            ">>> Step 1322 metrics: {'loss': 0.6494, 'grad_norm': 6.59375, 'learning_rate': 4.827264992769506e-05, 'epoch': 1.7986394557823129}\n",
            "\n",
            ">>> Step 1323 metrics: {'loss': 0.7222, 'grad_norm': 6.75, 'learning_rate': 4.8270044590553916e-05, 'epoch': 1.8}\n",
            "\n",
            ">>> Step 1324 metrics: {'loss': 0.4462, 'grad_norm': 5.90625, 'learning_rate': 4.826743736051153e-05, 'epoch': 1.8013605442176872}\n",
            "\n",
            ">>> Step 1325 metrics: {'loss': 0.8992, 'grad_norm': 7.625, 'learning_rate': 4.826482823778001e-05, 'epoch': 1.8027210884353742}\n",
            "\n",
            ">>> Step 1326 metrics: {'loss': 0.8133, 'grad_norm': 6.8125, 'learning_rate': 4.826221722257157e-05, 'epoch': 1.804081632653061}\n",
            "\n",
            ">>> Step 1327 metrics: {'loss': 0.3197, 'grad_norm': 4.78125, 'learning_rate': 4.8259604315098607e-05, 'epoch': 1.8054421768707483}\n",
            "\n",
            ">>> Step 1328 metrics: {'loss': 0.5185, 'grad_norm': 5.59375, 'learning_rate': 4.825698951557368e-05, 'epoch': 1.8068027210884354}\n",
            "\n",
            ">>> Step 1329 metrics: {'loss': 1.3092, 'grad_norm': 7.9375, 'learning_rate': 4.825437282420947e-05, 'epoch': 1.8081632653061224}\n",
            "\n",
            ">>> Step 1330 metrics: {'loss': 0.7379, 'grad_norm': 7.4375, 'learning_rate': 4.825175424121885e-05, 'epoch': 1.8095238095238095}\n",
            "\n",
            ">>> Step 1331 metrics: {'loss': 0.6342, 'grad_norm': 6.96875, 'learning_rate': 4.8249133766814816e-05, 'epoch': 1.8108843537414967}\n",
            "\n",
            ">>> Step 1332 metrics: {'loss': 0.6939, 'grad_norm': 5.96875, 'learning_rate': 4.824651140121054e-05, 'epoch': 1.8122448979591836}\n",
            "\n",
            ">>> Step 1333 metrics: {'loss': 0.7566, 'grad_norm': 6.875, 'learning_rate': 4.8243887144619334e-05, 'epoch': 1.8136054421768706}\n",
            "\n",
            ">>> Step 1334 metrics: {'loss': 0.3586, 'grad_norm': 4.6875, 'learning_rate': 4.8241260997254666e-05, 'epoch': 1.814965986394558}\n",
            "\n",
            ">>> Step 1335 metrics: {'loss': 0.6777, 'grad_norm': 6.4375, 'learning_rate': 4.8238632959330167e-05, 'epoch': 1.816326530612245}\n",
            "\n",
            ">>> Step 1336 metrics: {'loss': 0.2988, 'grad_norm': 5.78125, 'learning_rate': 4.8236003031059606e-05, 'epoch': 1.8176870748299319}\n",
            "\n",
            ">>> Step 1337 metrics: {'loss': 0.6767, 'grad_norm': 6.625, 'learning_rate': 4.823337121265692e-05, 'epoch': 1.819047619047619}\n",
            "\n",
            ">>> Step 1338 metrics: {'loss': 0.9829, 'grad_norm': 6.59375, 'learning_rate': 4.823073750433619e-05, 'epoch': 1.8204081632653062}\n",
            "\n",
            ">>> Step 1339 metrics: {'loss': 1.0092, 'grad_norm': 6.8125, 'learning_rate': 4.822810190631166e-05, 'epoch': 1.8217687074829931}\n",
            "\n",
            ">>> Step 1340 metrics: {'loss': 1.1264, 'grad_norm': 8.4375, 'learning_rate': 4.822546441879772e-05, 'epoch': 1.8231292517006803}\n",
            "\n",
            ">>> Step 1341 metrics: {'loss': 0.7839, 'grad_norm': 9.4375, 'learning_rate': 4.8222825042008914e-05, 'epoch': 1.8244897959183675}\n",
            "\n",
            ">>> Step 1342 metrics: {'loss': 0.5986, 'grad_norm': 5.65625, 'learning_rate': 4.822018377615994e-05, 'epoch': 1.8258503401360544}\n",
            "\n",
            ">>> Step 1343 metrics: {'loss': 1.4129, 'grad_norm': 7.875, 'learning_rate': 4.821754062146566e-05, 'epoch': 1.8272108843537413}\n",
            "\n",
            ">>> Step 1344 metrics: {'loss': 0.5841, 'grad_norm': 6.25, 'learning_rate': 4.821489557814108e-05, 'epoch': 1.8285714285714287}\n",
            "\n",
            ">>> Step 1345 metrics: {'loss': 0.7294, 'grad_norm': 6.84375, 'learning_rate': 4.821224864640136e-05, 'epoch': 1.8299319727891157}\n",
            "\n",
            ">>> Step 1346 metrics: {'loss': 0.9343, 'grad_norm': 6.65625, 'learning_rate': 4.820959982646181e-05, 'epoch': 1.8312925170068026}\n",
            "\n",
            ">>> Step 1347 metrics: {'loss': 0.7649, 'grad_norm': 6.75, 'learning_rate': 4.8206949118537905e-05, 'epoch': 1.8326530612244898}\n",
            "\n",
            ">>> Step 1348 metrics: {'loss': 0.491, 'grad_norm': 5.53125, 'learning_rate': 4.8204296522845253e-05, 'epoch': 1.834013605442177}\n",
            "\n",
            ">>> Step 1349 metrics: {'loss': 0.8867, 'grad_norm': 6.4375, 'learning_rate': 4.820164203959965e-05, 'epoch': 1.835374149659864}\n",
            "\n",
            ">>> Step 1350 metrics: {'loss': 0.5239, 'grad_norm': 5.25, 'learning_rate': 4.8198985669017e-05, 'epoch': 1.836734693877551}\n",
            "\n",
            ">>> Step 1351 metrics: {'loss': 0.8178, 'grad_norm': 6.6875, 'learning_rate': 4.819632741131342e-05, 'epoch': 1.8380952380952382}\n",
            "\n",
            ">>> Step 1352 metrics: {'loss': 0.9024, 'grad_norm': 6.15625, 'learning_rate': 4.819366726670511e-05, 'epoch': 1.8394557823129252}\n",
            "\n",
            ">>> Step 1353 metrics: {'loss': 0.3985, 'grad_norm': 5.15625, 'learning_rate': 4.819100523540848e-05, 'epoch': 1.8408163265306121}\n",
            "\n",
            ">>> Step 1354 metrics: {'loss': 0.816, 'grad_norm': 6.5, 'learning_rate': 4.818834131764007e-05, 'epoch': 1.8421768707482993}\n",
            "\n",
            ">>> Step 1355 metrics: {'loss': 0.8594, 'grad_norm': 7.46875, 'learning_rate': 4.8185675513616574e-05, 'epoch': 1.8435374149659864}\n",
            "\n",
            ">>> Step 1356 metrics: {'loss': 0.3567, 'grad_norm': 5.4375, 'learning_rate': 4.8183007823554836e-05, 'epoch': 1.8448979591836734}\n",
            "\n",
            ">>> Step 1357 metrics: {'loss': 0.6074, 'grad_norm': 5.84375, 'learning_rate': 4.818033824767187e-05, 'epoch': 1.8462585034013606}\n",
            "\n",
            ">>> Step 1358 metrics: {'loss': 0.5575, 'grad_norm': 5.5625, 'learning_rate': 4.817766678618483e-05, 'epoch': 1.8476190476190477}\n",
            "\n",
            ">>> Step 1359 metrics: {'loss': 0.9702, 'grad_norm': 6.25, 'learning_rate': 4.817499343931102e-05, 'epoch': 1.8489795918367347}\n",
            "\n",
            ">>> Step 1360 metrics: {'loss': 0.5228, 'grad_norm': 6.34375, 'learning_rate': 4.817231820726791e-05, 'epoch': 1.8503401360544216}\n",
            "\n",
            ">>> Step 1361 metrics: {'loss': 1.0221, 'grad_norm': 7.0625, 'learning_rate': 4.816964109027311e-05, 'epoch': 1.851700680272109}\n",
            "\n",
            ">>> Step 1362 metrics: {'loss': 0.6788, 'grad_norm': 6.90625, 'learning_rate': 4.81669620885444e-05, 'epoch': 1.853061224489796}\n",
            "\n",
            ">>> Step 1363 metrics: {'loss': 0.8267, 'grad_norm': 6.59375, 'learning_rate': 4.816428120229969e-05, 'epoch': 1.8544217687074829}\n",
            "\n",
            ">>> Step 1364 metrics: {'loss': 0.7105, 'grad_norm': 5.53125, 'learning_rate': 4.8161598431757065e-05, 'epoch': 1.85578231292517}\n",
            "\n",
            ">>> Step 1365 metrics: {'loss': 0.7143, 'grad_norm': 6.46875, 'learning_rate': 4.815891377713475e-05, 'epoch': 1.8571428571428572}\n",
            "\n",
            ">>> Step 1366 metrics: {'loss': 0.533, 'grad_norm': 5.3125, 'learning_rate': 4.815622723865114e-05, 'epoch': 1.8585034013605441}\n",
            "\n",
            ">>> Step 1367 metrics: {'loss': 0.6375, 'grad_norm': 6.28125, 'learning_rate': 4.8153538816524755e-05, 'epoch': 1.8598639455782313}\n",
            "\n",
            ">>> Step 1368 metrics: {'loss': 1.0052, 'grad_norm': 6.53125, 'learning_rate': 4.81508485109743e-05, 'epoch': 1.8612244897959185}\n",
            "\n",
            ">>> Step 1369 metrics: {'loss': 0.6303, 'grad_norm': 5.75, 'learning_rate': 4.814815632221861e-05, 'epoch': 1.8625850340136054}\n",
            "\n",
            ">>> Step 1370 metrics: {'loss': 0.6614, 'grad_norm': 5.96875, 'learning_rate': 4.814546225047668e-05, 'epoch': 1.8639455782312924}\n",
            "\n",
            ">>> Step 1371 metrics: {'loss': 1.1298, 'grad_norm': 6.71875, 'learning_rate': 4.814276629596766e-05, 'epoch': 1.8653061224489798}\n",
            "\n",
            ">>> Step 1372 metrics: {'loss': 0.5908, 'grad_norm': 6.1875, 'learning_rate': 4.814006845891086e-05, 'epoch': 1.8666666666666667}\n",
            "\n",
            ">>> Step 1373 metrics: {'loss': 0.7834, 'grad_norm': 7.09375, 'learning_rate': 4.813736873952572e-05, 'epoch': 1.8680272108843536}\n",
            "\n",
            ">>> Step 1374 metrics: {'loss': 0.7554, 'grad_norm': 7.0625, 'learning_rate': 4.813466713803186e-05, 'epoch': 1.8693877551020408}\n",
            "\n",
            ">>> Step 1375 metrics: {'loss': 1.1436, 'grad_norm': 7.03125, 'learning_rate': 4.8131963654649035e-05, 'epoch': 1.870748299319728}\n",
            "\n",
            ">>> Step 1376 metrics: {'loss': 0.6691, 'grad_norm': 5.59375, 'learning_rate': 4.812925828959717e-05, 'epoch': 1.872108843537415}\n",
            "\n",
            ">>> Step 1377 metrics: {'loss': 0.7603, 'grad_norm': 6.625, 'learning_rate': 4.8126551043096325e-05, 'epoch': 1.873469387755102}\n",
            "\n",
            ">>> Step 1378 metrics: {'loss': 0.564, 'grad_norm': 6.78125, 'learning_rate': 4.812384191536672e-05, 'epoch': 1.8748299319727892}\n",
            "\n",
            ">>> Step 1379 metrics: {'loss': 1.1177, 'grad_norm': 7.90625, 'learning_rate': 4.812113090662873e-05, 'epoch': 1.8761904761904762}\n",
            "\n",
            ">>> Step 1380 metrics: {'loss': 0.6597, 'grad_norm': 5.65625, 'learning_rate': 4.8118418017102895e-05, 'epoch': 1.8775510204081631}\n",
            "\n",
            ">>> Step 1381 metrics: {'loss': 0.563, 'grad_norm': 5.625, 'learning_rate': 4.811570324700988e-05, 'epoch': 1.8789115646258503}\n",
            "\n",
            ">>> Step 1382 metrics: {'loss': 0.6389, 'grad_norm': 7.5625, 'learning_rate': 4.811298659657051e-05, 'epoch': 1.8802721088435375}\n",
            "\n",
            ">>> Step 1383 metrics: {'loss': 0.5036, 'grad_norm': 5.78125, 'learning_rate': 4.811026806600579e-05, 'epoch': 1.8816326530612244}\n",
            "\n",
            ">>> Step 1384 metrics: {'loss': 0.8192, 'grad_norm': 6.09375, 'learning_rate': 4.810754765553685e-05, 'epoch': 1.8829931972789116}\n",
            "\n",
            ">>> Step 1385 metrics: {'loss': 0.9014, 'grad_norm': 6.3125, 'learning_rate': 4.8104825365384984e-05, 'epoch': 1.8843537414965987}\n",
            "\n",
            ">>> Step 1386 metrics: {'loss': 0.7599, 'grad_norm': 6.21875, 'learning_rate': 4.810210119577163e-05, 'epoch': 1.8857142857142857}\n",
            "\n",
            ">>> Step 1387 metrics: {'loss': 0.8665, 'grad_norm': 9.25, 'learning_rate': 4.80993751469184e-05, 'epoch': 1.8870748299319728}\n",
            "\n",
            ">>> Step 1388 metrics: {'loss': 1.2093, 'grad_norm': 6.75, 'learning_rate': 4.809664721904703e-05, 'epoch': 1.88843537414966}\n",
            "\n",
            ">>> Step 1389 metrics: {'loss': 0.6766, 'grad_norm': 6.6875, 'learning_rate': 4.809391741237943e-05, 'epoch': 1.889795918367347}\n",
            "\n",
            ">>> Step 1390 metrics: {'loss': 0.5198, 'grad_norm': 5.46875, 'learning_rate': 4.809118572713765e-05, 'epoch': 1.891156462585034}\n",
            "\n",
            ">>> Step 1391 metrics: {'loss': 0.3403, 'grad_norm': 4.71875, 'learning_rate': 4.808845216354391e-05, 'epoch': 1.892517006802721}\n",
            "\n",
            ">>> Step 1392 metrics: {'loss': 0.3631, 'grad_norm': 5.15625, 'learning_rate': 4.808571672182056e-05, 'epoch': 1.8938775510204082}\n",
            "\n",
            ">>> Step 1393 metrics: {'loss': 0.7384, 'grad_norm': 6.15625, 'learning_rate': 4.808297940219012e-05, 'epoch': 1.8952380952380952}\n",
            "\n",
            ">>> Step 1394 metrics: {'loss': 0.691, 'grad_norm': 6.78125, 'learning_rate': 4.808024020487526e-05, 'epoch': 1.8965986394557823}\n",
            "\n",
            ">>> Step 1395 metrics: {'loss': 0.569, 'grad_norm': 6.40625, 'learning_rate': 4.80774991300988e-05, 'epoch': 1.8979591836734695}\n",
            "\n",
            ">>> Step 1396 metrics: {'loss': 0.6968, 'grad_norm': 6.5625, 'learning_rate': 4.80747561780837e-05, 'epoch': 1.8993197278911564}\n",
            "\n",
            ">>> Step 1397 metrics: {'loss': 0.7856, 'grad_norm': 6.65625, 'learning_rate': 4.8072011349053096e-05, 'epoch': 1.9006802721088434}\n",
            "\n",
            ">>> Step 1398 metrics: {'loss': 0.6834, 'grad_norm': 6.625, 'learning_rate': 4.806926464323027e-05, 'epoch': 1.9020408163265308}\n",
            "\n",
            ">>> Step 1399 metrics: {'loss': 0.6349, 'grad_norm': 8.6875, 'learning_rate': 4.8066516060838645e-05, 'epoch': 1.9034013605442177}\n",
            "\n",
            ">>> Step 1400 metrics: {'loss': 0.3853, 'grad_norm': 5.46875, 'learning_rate': 4.8063765602101807e-05, 'epoch': 1.9047619047619047}\n",
            "\n",
            ">>> Step 1401 metrics: {'loss': 0.7849, 'grad_norm': 6.09375, 'learning_rate': 4.806101326724349e-05, 'epoch': 1.9061224489795918}\n",
            "\n",
            ">>> Step 1402 metrics: {'loss': 0.5045, 'grad_norm': 6.84375, 'learning_rate': 4.8058259056487585e-05, 'epoch': 1.907482993197279}\n",
            "\n",
            ">>> Step 1403 metrics: {'loss': 0.7352, 'grad_norm': 6.03125, 'learning_rate': 4.805550297005814e-05, 'epoch': 1.908843537414966}\n",
            "\n",
            ">>> Step 1404 metrics: {'loss': 0.2437, 'grad_norm': 4.96875, 'learning_rate': 4.8052745008179334e-05, 'epoch': 1.910204081632653}\n",
            "\n",
            ">>> Step 1405 metrics: {'loss': 0.5539, 'grad_norm': 6.40625, 'learning_rate': 4.8049985171075524e-05, 'epoch': 1.9115646258503403}\n",
            "\n",
            ">>> Step 1406 metrics: {'loss': 0.4441, 'grad_norm': 5.03125, 'learning_rate': 4.8047223458971205e-05, 'epoch': 1.9129251700680272}\n",
            "\n",
            ">>> Step 1407 metrics: {'loss': 0.9661, 'grad_norm': 7.375, 'learning_rate': 4.804445987209103e-05, 'epoch': 1.9142857142857141}\n",
            "\n",
            ">>> Step 1408 metrics: {'loss': 0.8744, 'grad_norm': 7.15625, 'learning_rate': 4.8041694410659796e-05, 'epoch': 1.9156462585034013}\n",
            "\n",
            ">>> Step 1409 metrics: {'loss': 0.732, 'grad_norm': 6.84375, 'learning_rate': 4.803892707490247e-05, 'epoch': 1.9170068027210885}\n",
            "\n",
            ">>> Step 1410 metrics: {'loss': 0.7292, 'grad_norm': 6.375, 'learning_rate': 4.8036157865044156e-05, 'epoch': 1.9183673469387754}\n",
            "\n",
            ">>> Step 1411 metrics: {'loss': 0.5117, 'grad_norm': 5.90625, 'learning_rate': 4.803338678131011e-05, 'epoch': 1.9197278911564626}\n",
            "\n",
            ">>> Step 1412 metrics: {'loss': 0.6563, 'grad_norm': 6.25, 'learning_rate': 4.803061382392576e-05, 'epoch': 1.9210884353741497}\n",
            "\n",
            ">>> Step 1413 metrics: {'loss': 0.8473, 'grad_norm': 6.28125, 'learning_rate': 4.8027838993116666e-05, 'epoch': 1.9224489795918367}\n",
            "\n",
            ">>> Step 1414 metrics: {'loss': 0.533, 'grad_norm': 8.3125, 'learning_rate': 4.802506228910854e-05, 'epoch': 1.9238095238095239}\n",
            "\n",
            ">>> Step 1415 metrics: {'loss': 0.4611, 'grad_norm': 5.25, 'learning_rate': 4.8022283712127246e-05, 'epoch': 1.925170068027211}\n",
            "\n",
            ">>> Step 1416 metrics: {'loss': 0.8838, 'grad_norm': 7.59375, 'learning_rate': 4.801950326239884e-05, 'epoch': 1.926530612244898}\n",
            "\n",
            ">>> Step 1417 metrics: {'loss': 0.7081, 'grad_norm': 5.78125, 'learning_rate': 4.801672094014945e-05, 'epoch': 1.927891156462585}\n",
            "\n",
            ">>> Step 1418 metrics: {'loss': 0.7441, 'grad_norm': 6.375, 'learning_rate': 4.801393674560545e-05, 'epoch': 1.929251700680272}\n",
            "\n",
            ">>> Step 1419 metrics: {'loss': 0.379, 'grad_norm': 5.40625, 'learning_rate': 4.801115067899329e-05, 'epoch': 1.9306122448979592}\n",
            "\n",
            ">>> Step 1420 metrics: {'loss': 1.1269, 'grad_norm': 7.25, 'learning_rate': 4.800836274053962e-05, 'epoch': 1.9319727891156462}\n",
            "\n",
            ">>> Step 1421 metrics: {'loss': 1.1166, 'grad_norm': 8.4375, 'learning_rate': 4.800557293047121e-05, 'epoch': 1.9333333333333333}\n",
            "\n",
            ">>> Step 1422 metrics: {'loss': 0.6815, 'grad_norm': 7.0, 'learning_rate': 4.8002781249015e-05, 'epoch': 1.9346938775510205}\n",
            "\n",
            ">>> Step 1423 metrics: {'loss': 0.7241, 'grad_norm': 5.875, 'learning_rate': 4.79999876963981e-05, 'epoch': 1.9360544217687075}\n",
            "\n",
            ">>> Step 1424 metrics: {'loss': 1.0871, 'grad_norm': 6.875, 'learning_rate': 4.799719227284772e-05, 'epoch': 1.9374149659863944}\n",
            "\n",
            ">>> Step 1425 metrics: {'loss': 0.5062, 'grad_norm': 6.09375, 'learning_rate': 4.7994394978591285e-05, 'epoch': 1.9387755102040818}\n",
            "\n",
            ">>> Step 1426 metrics: {'loss': 0.7306, 'grad_norm': 6.84375, 'learning_rate': 4.79915958138563e-05, 'epoch': 1.9401360544217687}\n",
            "\n",
            ">>> Step 1427 metrics: {'loss': 0.9219, 'grad_norm': 6.8125, 'learning_rate': 4.798879477887051e-05, 'epoch': 1.9414965986394557}\n",
            "\n",
            ">>> Step 1428 metrics: {'loss': 0.6261, 'grad_norm': 5.875, 'learning_rate': 4.798599187386173e-05, 'epoch': 1.9428571428571428}\n",
            "\n",
            ">>> Step 1429 metrics: {'loss': 0.7239, 'grad_norm': 6.65625, 'learning_rate': 4.7983187099057985e-05, 'epoch': 1.94421768707483}\n",
            "\n",
            ">>> Step 1430 metrics: {'loss': 0.6448, 'grad_norm': 6.0625, 'learning_rate': 4.798038045468741e-05, 'epoch': 1.945578231292517}\n",
            "\n",
            ">>> Step 1431 metrics: {'loss': 0.7101, 'grad_norm': 7.625, 'learning_rate': 4.797757194097833e-05, 'epoch': 1.9469387755102041}\n",
            "\n",
            ">>> Step 1432 metrics: {'loss': 0.6844, 'grad_norm': 6.5625, 'learning_rate': 4.7974761558159184e-05, 'epoch': 1.9482993197278913}\n",
            "\n",
            ">>> Step 1433 metrics: {'loss': 0.7393, 'grad_norm': 6.71875, 'learning_rate': 4.797194930645861e-05, 'epoch': 1.9496598639455782}\n",
            "\n",
            ">>> Step 1434 metrics: {'loss': 0.5522, 'grad_norm': 5.0625, 'learning_rate': 4.796913518610533e-05, 'epoch': 1.9510204081632652}\n",
            "\n",
            ">>> Step 1435 metrics: {'loss': 0.604, 'grad_norm': 6.34375, 'learning_rate': 4.79663191973283e-05, 'epoch': 1.9523809523809523}\n",
            "\n",
            ">>> Step 1436 metrics: {'loss': 0.5295, 'grad_norm': 5.375, 'learning_rate': 4.796350134035656e-05, 'epoch': 1.9537414965986395}\n",
            "\n",
            ">>> Step 1437 metrics: {'loss': 0.4193, 'grad_norm': 5.375, 'learning_rate': 4.796068161541934e-05, 'epoch': 1.9551020408163264}\n",
            "\n",
            ">>> Step 1438 metrics: {'loss': 0.3976, 'grad_norm': 4.875, 'learning_rate': 4.795786002274601e-05, 'epoch': 1.9564625850340136}\n",
            "\n",
            ">>> Step 1439 metrics: {'loss': 0.598, 'grad_norm': 6.25, 'learning_rate': 4.795503656256608e-05, 'epoch': 1.9578231292517008}\n",
            "\n",
            ">>> Step 1440 metrics: {'loss': 0.7396, 'grad_norm': 5.09375, 'learning_rate': 4.795221123510924e-05, 'epoch': 1.9591836734693877}\n",
            "\n",
            ">>> Step 1441 metrics: {'loss': 0.8746, 'grad_norm': 7.28125, 'learning_rate': 4.7949384040605304e-05, 'epoch': 1.9605442176870749}\n",
            "\n",
            ">>> Step 1442 metrics: {'loss': 0.6038, 'grad_norm': 7.28125, 'learning_rate': 4.794655497928427e-05, 'epoch': 1.961904761904762}\n",
            "\n",
            ">>> Step 1443 metrics: {'loss': 0.48, 'grad_norm': 6.25, 'learning_rate': 4.794372405137624e-05, 'epoch': 1.963265306122449}\n",
            "\n",
            ">>> Step 1444 metrics: {'loss': 0.936, 'grad_norm': 7.28125, 'learning_rate': 4.794089125711152e-05, 'epoch': 1.964625850340136}\n",
            "\n",
            ">>> Step 1445 metrics: {'loss': 0.4587, 'grad_norm': 6.1875, 'learning_rate': 4.793805659672053e-05, 'epoch': 1.965986394557823}\n",
            "\n",
            ">>> Step 1446 metrics: {'loss': 0.4244, 'grad_norm': 7.75, 'learning_rate': 4.793522007043386e-05, 'epoch': 1.9673469387755103}\n",
            "\n",
            ">>> Step 1447 metrics: {'loss': 1.1665, 'grad_norm': 7.3125, 'learning_rate': 4.7932381678482244e-05, 'epoch': 1.9687074829931972}\n",
            "\n",
            ">>> Step 1448 metrics: {'loss': 0.904, 'grad_norm': 6.46875, 'learning_rate': 4.7929541421096576e-05, 'epoch': 1.9700680272108844}\n",
            "\n",
            ">>> Step 1449 metrics: {'loss': 0.3857, 'grad_norm': 5.09375, 'learning_rate': 4.7926699298507896e-05, 'epoch': 1.9714285714285715}\n",
            "\n",
            ">>> Step 1450 metrics: {'loss': 0.5642, 'grad_norm': 5.3125, 'learning_rate': 4.792385531094739e-05, 'epoch': 1.9727891156462585}\n",
            "\n",
            ">>> Step 1451 metrics: {'loss': 1.0878, 'grad_norm': 7.9375, 'learning_rate': 4.79210094586464e-05, 'epoch': 1.9741496598639454}\n",
            "\n",
            ">>> Step 1452 metrics: {'loss': 0.8639, 'grad_norm': 6.71875, 'learning_rate': 4.7918161741836434e-05, 'epoch': 1.9755102040816328}\n",
            "\n",
            ">>> Step 1453 metrics: {'loss': 0.7956, 'grad_norm': 5.78125, 'learning_rate': 4.791531216074914e-05, 'epoch': 1.9768707482993197}\n",
            "\n",
            ">>> Step 1454 metrics: {'loss': 0.778, 'grad_norm': 6.34375, 'learning_rate': 4.79124607156163e-05, 'epoch': 1.9782312925170067}\n",
            "\n",
            ">>> Step 1455 metrics: {'loss': 0.5105, 'grad_norm': 5.34375, 'learning_rate': 4.7909607406669875e-05, 'epoch': 1.9795918367346939}\n",
            "\n",
            ">>> Step 1456 metrics: {'loss': 0.7685, 'grad_norm': 6.8125, 'learning_rate': 4.790675223414197e-05, 'epoch': 1.980952380952381}\n",
            "\n",
            ">>> Step 1457 metrics: {'loss': 0.4808, 'grad_norm': 6.0, 'learning_rate': 4.7903895198264846e-05, 'epoch': 1.982312925170068}\n",
            "\n",
            ">>> Step 1458 metrics: {'loss': 0.5445, 'grad_norm': 5.53125, 'learning_rate': 4.790103629927089e-05, 'epoch': 1.9836734693877551}\n",
            "\n",
            ">>> Step 1459 metrics: {'loss': 0.527, 'grad_norm': 6.4375, 'learning_rate': 4.789817553739267e-05, 'epoch': 1.9850340136054423}\n",
            "\n",
            ">>> Step 1460 metrics: {'loss': 0.3308, 'grad_norm': 5.0625, 'learning_rate': 4.789531291286289e-05, 'epoch': 1.9863945578231292}\n",
            "\n",
            ">>> Step 1461 metrics: {'loss': 0.6277, 'grad_norm': 6.3125, 'learning_rate': 4.7892448425914424e-05, 'epoch': 1.9877551020408162}\n",
            "\n",
            ">>> Step 1462 metrics: {'loss': 0.72, 'grad_norm': 7.25, 'learning_rate': 4.788958207678026e-05, 'epoch': 1.9891156462585036}\n",
            "\n",
            ">>> Step 1463 metrics: {'loss': 0.6939, 'grad_norm': 5.90625, 'learning_rate': 4.788671386569358e-05, 'epoch': 1.9904761904761905}\n",
            "\n",
            ">>> Step 1464 metrics: {'loss': 0.6584, 'grad_norm': 9.0, 'learning_rate': 4.788384379288769e-05, 'epoch': 1.9918367346938775}\n",
            "\n",
            ">>> Step 1465 metrics: {'loss': 1.0683, 'grad_norm': 9.0, 'learning_rate': 4.788097185859606e-05, 'epoch': 1.9931972789115646}\n",
            "\n",
            ">>> Step 1466 metrics: {'loss': 1.199, 'grad_norm': 7.625, 'learning_rate': 4.78780980630523e-05, 'epoch': 1.9945578231292518}\n",
            "\n",
            ">>> Step 1467 metrics: {'loss': 0.7046, 'grad_norm': 6.0, 'learning_rate': 4.7875222406490195e-05, 'epoch': 1.9959183673469387}\n",
            "\n",
            ">>> Step 1468 metrics: {'loss': 0.6167, 'grad_norm': 5.71875, 'learning_rate': 4.787234488914365e-05, 'epoch': 1.997278911564626}\n",
            "\n",
            ">>> Step 1469 metrics: {'loss': 0.4848, 'grad_norm': 6.34375, 'learning_rate': 4.7869465511246736e-05, 'epoch': 1.998639455782313}\n",
            "\n",
            ">>> Step 1470 metrics: {'loss': 0.6333, 'grad_norm': 6.125, 'learning_rate': 4.786658427303369e-05, 'epoch': 2.0}\n",
            "\n",
            ">>> Step 1471 metrics: {'loss': 0.5606, 'grad_norm': 5.53125, 'learning_rate': 4.786370117473887e-05, 'epoch': 2.001360544217687}\n",
            "\n",
            ">>> Step 1472 metrics: {'loss': 0.4485, 'grad_norm': 6.9375, 'learning_rate': 4.786081621659681e-05, 'epoch': 2.0027210884353743}\n",
            "\n",
            ">>> Step 1473 metrics: {'loss': 0.4511, 'grad_norm': 5.65625, 'learning_rate': 4.785792939884219e-05, 'epoch': 2.0040816326530613}\n",
            "\n",
            ">>> Step 1474 metrics: {'loss': 0.4122, 'grad_norm': 5.28125, 'learning_rate': 4.785504072170983e-05, 'epoch': 2.005442176870748}\n",
            "\n",
            ">>> Step 1475 metrics: {'loss': 0.5347, 'grad_norm': 6.78125, 'learning_rate': 4.785215018543472e-05, 'epoch': 2.006802721088435}\n",
            "\n",
            ">>> Step 1476 metrics: {'loss': 0.4646, 'grad_norm': 5.71875, 'learning_rate': 4.784925779025198e-05, 'epoch': 2.0081632653061225}\n",
            "\n",
            ">>> Step 1477 metrics: {'loss': 0.3394, 'grad_norm': 5.59375, 'learning_rate': 4.784636353639689e-05, 'epoch': 2.0095238095238095}\n",
            "\n",
            ">>> Step 1478 metrics: {'loss': 0.491, 'grad_norm': 6.0625, 'learning_rate': 4.784346742410489e-05, 'epoch': 2.0108843537414964}\n",
            "\n",
            ">>> Step 1479 metrics: {'loss': 0.1909, 'grad_norm': 5.28125, 'learning_rate': 4.7840569453611574e-05, 'epoch': 2.012244897959184}\n",
            "\n",
            ">>> Step 1480 metrics: {'loss': 0.416, 'grad_norm': 6.25, 'learning_rate': 4.783766962515266e-05, 'epoch': 2.0136054421768708}\n",
            "\n",
            ">>> Step 1481 metrics: {'loss': 0.64, 'grad_norm': 6.71875, 'learning_rate': 4.7834767938964036e-05, 'epoch': 2.0149659863945577}\n",
            "\n",
            ">>> Step 1482 metrics: {'loss': 0.7423, 'grad_norm': 8.1875, 'learning_rate': 4.783186439528175e-05, 'epoch': 2.016326530612245}\n",
            "\n",
            ">>> Step 1483 metrics: {'loss': 0.4523, 'grad_norm': 6.90625, 'learning_rate': 4.782895899434198e-05, 'epoch': 2.017687074829932}\n",
            "\n",
            ">>> Step 1484 metrics: {'loss': 0.3985, 'grad_norm': 7.6875, 'learning_rate': 4.7826051736381075e-05, 'epoch': 2.019047619047619}\n",
            "\n",
            ">>> Step 1485 metrics: {'loss': 0.346, 'grad_norm': 6.0625, 'learning_rate': 4.782314262163552e-05, 'epoch': 2.020408163265306}\n",
            "\n",
            ">>> Step 1486 metrics: {'loss': 0.6543, 'grad_norm': 9.1875, 'learning_rate': 4.782023165034196e-05, 'epoch': 2.0217687074829933}\n",
            "\n",
            ">>> Step 1487 metrics: {'loss': 0.3073, 'grad_norm': 6.15625, 'learning_rate': 4.781731882273719e-05, 'epoch': 2.0231292517006803}\n",
            "\n",
            ">>> Step 1488 metrics: {'loss': 0.5065, 'grad_norm': 10.125, 'learning_rate': 4.781440413905814e-05, 'epoch': 2.024489795918367}\n",
            "\n",
            ">>> Step 1489 metrics: {'loss': 0.3475, 'grad_norm': 5.90625, 'learning_rate': 4.7811487599541927e-05, 'epoch': 2.0258503401360546}\n",
            "\n",
            ">>> Step 1490 metrics: {'loss': 0.2007, 'grad_norm': 4.90625, 'learning_rate': 4.780856920442578e-05, 'epoch': 2.0272108843537415}\n",
            "\n",
            ">>> Step 1491 metrics: {'loss': 1.0919, 'grad_norm': 8.3125, 'learning_rate': 4.78056489539471e-05, 'epoch': 2.0285714285714285}\n",
            "\n",
            ">>> Step 1492 metrics: {'loss': 0.6533, 'grad_norm': 6.84375, 'learning_rate': 4.7802726848343436e-05, 'epoch': 2.029931972789116}\n",
            "\n",
            ">>> Step 1493 metrics: {'loss': 0.3209, 'grad_norm': 4.78125, 'learning_rate': 4.7799802887852496e-05, 'epoch': 2.031292517006803}\n",
            "\n",
            ">>> Step 1494 metrics: {'loss': 0.395, 'grad_norm': 5.71875, 'learning_rate': 4.779687707271211e-05, 'epoch': 2.0326530612244897}\n",
            "\n",
            ">>> Step 1495 metrics: {'loss': 0.3131, 'grad_norm': 7.0625, 'learning_rate': 4.779394940316029e-05, 'epoch': 2.0340136054421767}\n",
            "\n",
            ">>> Step 1496 metrics: {'loss': 0.414, 'grad_norm': 6.75, 'learning_rate': 4.779101987943518e-05, 'epoch': 2.035374149659864}\n",
            "\n",
            ">>> Step 1497 metrics: {'loss': 0.3254, 'grad_norm': 6.09375, 'learning_rate': 4.778808850177509e-05, 'epoch': 2.036734693877551}\n",
            "\n",
            ">>> Step 1498 metrics: {'loss': 0.3994, 'grad_norm': 5.84375, 'learning_rate': 4.778515527041847e-05, 'epoch': 2.038095238095238}\n",
            "\n",
            ">>> Step 1499 metrics: {'loss': 0.6604, 'grad_norm': 6.71875, 'learning_rate': 4.778222018560393e-05, 'epoch': 2.0394557823129253}\n",
            "\n",
            ">>> Step 1500 metrics: {'loss': 0.4171, 'grad_norm': 9.75, 'learning_rate': 4.777928324757021e-05, 'epoch': 2.0408163265306123}\n",
            "\n",
            ">>> Step 1501 metrics: {'loss': 0.4081, 'grad_norm': 7.3125, 'learning_rate': 4.777634445655623e-05, 'epoch': 2.0421768707482992}\n",
            "\n",
            ">>> Step 1502 metrics: {'loss': 0.5011, 'grad_norm': 9.5625, 'learning_rate': 4.777340381280103e-05, 'epoch': 2.043537414965986}\n",
            "\n",
            ">>> Step 1503 metrics: {'loss': 0.4191, 'grad_norm': 6.40625, 'learning_rate': 4.7770461316543835e-05, 'epoch': 2.0448979591836736}\n",
            "\n",
            ">>> Step 1504 metrics: {'loss': 0.2274, 'grad_norm': 6.5625, 'learning_rate': 4.7767516968024e-05, 'epoch': 2.0462585034013605}\n",
            "\n",
            ">>> Step 1505 metrics: {'loss': 0.7685, 'grad_norm': 7.96875, 'learning_rate': 4.776457076748101e-05, 'epoch': 2.0476190476190474}\n",
            "\n",
            ">>> Step 1506 metrics: {'loss': 0.4344, 'grad_norm': 7.1875, 'learning_rate': 4.776162271515454e-05, 'epoch': 2.048979591836735}\n",
            "\n",
            ">>> Step 1507 metrics: {'loss': 0.2951, 'grad_norm': 5.46875, 'learning_rate': 4.7758672811284414e-05, 'epoch': 2.050340136054422}\n",
            "\n",
            ">>> Step 1508 metrics: {'loss': 0.4493, 'grad_norm': 6.21875, 'learning_rate': 4.7755721056110564e-05, 'epoch': 2.0517006802721087}\n",
            "\n",
            ">>> Step 1509 metrics: {'loss': 0.1283, 'grad_norm': 5.5, 'learning_rate': 4.7752767449873114e-05, 'epoch': 2.053061224489796}\n",
            "\n",
            ">>> Step 1510 metrics: {'loss': 0.6736, 'grad_norm': 8.5, 'learning_rate': 4.774981199281233e-05, 'epoch': 2.054421768707483}\n",
            "\n",
            ">>> Step 1511 metrics: {'loss': 0.2607, 'grad_norm': 5.1875, 'learning_rate': 4.774685468516862e-05, 'epoch': 2.05578231292517}\n",
            "\n",
            ">>> Step 1512 metrics: {'loss': 0.6629, 'grad_norm': 6.90625, 'learning_rate': 4.7743895527182534e-05, 'epoch': 2.057142857142857}\n",
            "\n",
            ">>> Step 1513 metrics: {'loss': 0.7894, 'grad_norm': 11.875, 'learning_rate': 4.7740934519094806e-05, 'epoch': 2.0585034013605443}\n",
            "\n",
            ">>> Step 1514 metrics: {'loss': 0.8397, 'grad_norm': 8.3125, 'learning_rate': 4.773797166114627e-05, 'epoch': 2.0598639455782313}\n",
            "\n",
            ">>> Step 1515 metrics: {'loss': 0.2609, 'grad_norm': 4.9375, 'learning_rate': 4.7735006953577975e-05, 'epoch': 2.061224489795918}\n",
            "\n",
            ">>> Step 1516 metrics: {'loss': 0.3258, 'grad_norm': 5.9375, 'learning_rate': 4.7732040396631064e-05, 'epoch': 2.0625850340136056}\n",
            "\n",
            ">>> Step 1517 metrics: {'loss': 0.2787, 'grad_norm': 5.28125, 'learning_rate': 4.7729071990546855e-05, 'epoch': 2.0639455782312925}\n",
            "\n",
            ">>> Step 1518 metrics: {'loss': 0.6158, 'grad_norm': 7.5625, 'learning_rate': 4.7726101735566806e-05, 'epoch': 2.0653061224489795}\n",
            "\n",
            ">>> Step 1519 metrics: {'loss': 0.4233, 'grad_norm': 5.96875, 'learning_rate': 4.772312963193255e-05, 'epoch': 2.066666666666667}\n",
            "\n",
            ">>> Step 1520 metrics: {'loss': 0.5671, 'grad_norm': 6.46875, 'learning_rate': 4.7720155679885836e-05, 'epoch': 2.068027210884354}\n",
            "\n",
            ">>> Step 1521 metrics: {'loss': 0.2965, 'grad_norm': 5.59375, 'learning_rate': 4.7717179879668585e-05, 'epoch': 2.0693877551020408}\n",
            "\n",
            ">>> Step 1522 metrics: {'loss': 0.1527, 'grad_norm': 5.1875, 'learning_rate': 4.7714202231522866e-05, 'epoch': 2.0707482993197277}\n",
            "\n",
            ">>> Step 1523 metrics: {'loss': 0.4762, 'grad_norm': 9.8125, 'learning_rate': 4.771122273569089e-05, 'epoch': 2.072108843537415}\n",
            "\n",
            ">>> Step 1524 metrics: {'loss': 0.6125, 'grad_norm': 7.21875, 'learning_rate': 4.7708241392415034e-05, 'epoch': 2.073469387755102}\n",
            "\n",
            ">>> Step 1525 metrics: {'loss': 0.4507, 'grad_norm': 7.3125, 'learning_rate': 4.770525820193781e-05, 'epoch': 2.074829931972789}\n",
            "\n",
            ">>> Step 1526 metrics: {'loss': 0.5085, 'grad_norm': 8.0, 'learning_rate': 4.7702273164501874e-05, 'epoch': 2.0761904761904764}\n",
            "\n",
            ">>> Step 1527 metrics: {'loss': 0.3647, 'grad_norm': 6.59375, 'learning_rate': 4.7699286280350065e-05, 'epoch': 2.0775510204081633}\n",
            "\n",
            ">>> Step 1528 metrics: {'loss': 0.7768, 'grad_norm': 8.0625, 'learning_rate': 4.769629754972534e-05, 'epoch': 2.0789115646258503}\n",
            "\n",
            ">>> Step 1529 metrics: {'loss': 0.2481, 'grad_norm': 6.625, 'learning_rate': 4.7693306972870814e-05, 'epoch': 2.0802721088435376}\n",
            "\n",
            ">>> Step 1530 metrics: {'loss': 0.9404, 'grad_norm': 7.96875, 'learning_rate': 4.769031455002977e-05, 'epoch': 2.0816326530612246}\n",
            "\n",
            ">>> Step 1531 metrics: {'loss': 0.5906, 'grad_norm': 6.5, 'learning_rate': 4.7687320281445604e-05, 'epoch': 2.0829931972789115}\n",
            "\n",
            ">>> Step 1532 metrics: {'loss': 0.6016, 'grad_norm': 7.34375, 'learning_rate': 4.7684324167361905e-05, 'epoch': 2.0843537414965985}\n",
            "\n",
            ">>> Step 1533 metrics: {'loss': 0.4706, 'grad_norm': 6.875, 'learning_rate': 4.768132620802237e-05, 'epoch': 2.085714285714286}\n",
            "\n",
            ">>> Step 1534 metrics: {'loss': 0.689, 'grad_norm': 7.21875, 'learning_rate': 4.767832640367089e-05, 'epoch': 2.087074829931973}\n",
            "\n",
            ">>> Step 1535 metrics: {'loss': 0.6545, 'grad_norm': 7.84375, 'learning_rate': 4.7675324754551484e-05, 'epoch': 2.0884353741496597}\n",
            "\n",
            ">>> Step 1536 metrics: {'loss': 0.357, 'grad_norm': 5.6875, 'learning_rate': 4.7672321260908304e-05, 'epoch': 2.089795918367347}\n",
            "\n",
            ">>> Step 1537 metrics: {'loss': 0.4359, 'grad_norm': 7.0625, 'learning_rate': 4.766931592298568e-05, 'epoch': 2.091156462585034}\n",
            "\n",
            ">>> Step 1538 metrics: {'loss': 0.5248, 'grad_norm': 7.0625, 'learning_rate': 4.766630874102808e-05, 'epoch': 2.092517006802721}\n",
            "\n",
            ">>> Step 1539 metrics: {'loss': 0.3626, 'grad_norm': 5.71875, 'learning_rate': 4.7663299715280115e-05, 'epoch': 2.093877551020408}\n",
            "\n",
            ">>> Step 1540 metrics: {'loss': 0.9093, 'grad_norm': 8.4375, 'learning_rate': 4.7660288845986566e-05, 'epoch': 2.0952380952380953}\n",
            "\n",
            ">>> Step 1541 metrics: {'loss': 0.218, 'grad_norm': 5.59375, 'learning_rate': 4.765727613339235e-05, 'epoch': 2.0965986394557823}\n",
            "\n",
            ">>> Step 1542 metrics: {'loss': 0.2065, 'grad_norm': 4.46875, 'learning_rate': 4.765426157774253e-05, 'epoch': 2.0979591836734692}\n",
            "\n",
            ">>> Step 1543 metrics: {'loss': 0.3944, 'grad_norm': 7.96875, 'learning_rate': 4.765124517928232e-05, 'epoch': 2.0993197278911566}\n",
            "\n",
            ">>> Step 1544 metrics: {'loss': 0.6913, 'grad_norm': 8.25, 'learning_rate': 4.7648226938257115e-05, 'epoch': 2.1006802721088436}\n",
            "\n",
            ">>> Step 1545 metrics: {'loss': 0.6606, 'grad_norm': 8.25, 'learning_rate': 4.7645206854912406e-05, 'epoch': 2.1020408163265305}\n",
            "\n",
            ">>> Step 1546 metrics: {'loss': 0.3388, 'grad_norm': 6.6875, 'learning_rate': 4.764218492949387e-05, 'epoch': 2.103401360544218}\n",
            "\n",
            ">>> Step 1547 metrics: {'loss': 0.9137, 'grad_norm': 9.6875, 'learning_rate': 4.763916116224733e-05, 'epoch': 2.104761904761905}\n",
            "\n",
            ">>> Step 1548 metrics: {'loss': 0.3178, 'grad_norm': 7.28125, 'learning_rate': 4.7636135553418746e-05, 'epoch': 2.1061224489795918}\n",
            "\n",
            ">>> Step 1549 metrics: {'loss': 0.3855, 'grad_norm': 7.21875, 'learning_rate': 4.763310810325424e-05, 'epoch': 2.1074829931972787}\n",
            "\n",
            ">>> Step 1550 metrics: {'loss': 0.219, 'grad_norm': 5.96875, 'learning_rate': 4.763007881200009e-05, 'epoch': 2.108843537414966}\n",
            "\n",
            ">>> Step 1551 metrics: {'loss': 0.5052, 'grad_norm': 6.96875, 'learning_rate': 4.7627047679902706e-05, 'epoch': 2.110204081632653}\n",
            "\n",
            ">>> Step 1552 metrics: {'loss': 0.2902, 'grad_norm': 5.5, 'learning_rate': 4.762401470720864e-05, 'epoch': 2.11156462585034}\n",
            "\n",
            ">>> Step 1553 metrics: {'loss': 0.3633, 'grad_norm': 5.5625, 'learning_rate': 4.762097989416464e-05, 'epoch': 2.1129251700680274}\n",
            "\n",
            ">>> Step 1554 metrics: {'loss': 0.8269, 'grad_norm': 7.625, 'learning_rate': 4.761794324101754e-05, 'epoch': 2.1142857142857143}\n",
            "\n",
            ">>> Step 1555 metrics: {'loss': 0.4583, 'grad_norm': 9.3125, 'learning_rate': 4.7614904748014384e-05, 'epoch': 2.1156462585034013}\n",
            "\n",
            ">>> Step 1556 metrics: {'loss': 0.3606, 'grad_norm': 5.78125, 'learning_rate': 4.7611864415402324e-05, 'epoch': 2.1170068027210887}\n",
            "\n",
            ">>> Step 1557 metrics: {'loss': 0.7543, 'grad_norm': 8.5, 'learning_rate': 4.7608822243428674e-05, 'epoch': 2.1183673469387756}\n",
            "\n",
            ">>> Step 1558 metrics: {'loss': 0.8091, 'grad_norm': 7.3125, 'learning_rate': 4.760577823234091e-05, 'epoch': 2.1197278911564625}\n",
            "\n",
            ">>> Step 1559 metrics: {'loss': 0.6216, 'grad_norm': 7.625, 'learning_rate': 4.760273238238664e-05, 'epoch': 2.1210884353741495}\n",
            "\n",
            ">>> Step 1560 metrics: {'loss': 0.5015, 'grad_norm': 8.375, 'learning_rate': 4.7599684693813626e-05, 'epoch': 2.122448979591837}\n",
            "\n",
            ">>> Step 1561 metrics: {'loss': 0.1825, 'grad_norm': 5.4375, 'learning_rate': 4.759663516686979e-05, 'epoch': 2.123809523809524}\n",
            "\n",
            ">>> Step 1562 metrics: {'loss': 0.5128, 'grad_norm': 12.3125, 'learning_rate': 4.759358380180319e-05, 'epoch': 2.1251700680272108}\n",
            "\n",
            ">>> Step 1563 metrics: {'loss': 0.581, 'grad_norm': 9.0, 'learning_rate': 4.759053059886204e-05, 'epoch': 2.126530612244898}\n",
            "\n",
            ">>> Step 1564 metrics: {'loss': 0.5967, 'grad_norm': 7.0, 'learning_rate': 4.758747555829469e-05, 'epoch': 2.127891156462585}\n",
            "\n",
            ">>> Step 1565 metrics: {'loss': 0.3323, 'grad_norm': 5.09375, 'learning_rate': 4.7584418680349684e-05, 'epoch': 2.129251700680272}\n",
            "\n",
            ">>> Step 1566 metrics: {'loss': 0.8984, 'grad_norm': 9.5625, 'learning_rate': 4.7581359965275656e-05, 'epoch': 2.130612244897959}\n",
            "\n",
            ">>> Step 1567 metrics: {'loss': 0.6737, 'grad_norm': 7.09375, 'learning_rate': 4.7578299413321425e-05, 'epoch': 2.1319727891156464}\n",
            "\n",
            ">>> Step 1568 metrics: {'loss': 0.668, 'grad_norm': 7.1875, 'learning_rate': 4.7575237024735963e-05, 'epoch': 2.1333333333333333}\n",
            "\n",
            ">>> Step 1569 metrics: {'loss': 0.5283, 'grad_norm': 6.78125, 'learning_rate': 4.757217279976836e-05, 'epoch': 2.1346938775510202}\n",
            "\n",
            ">>> Step 1570 metrics: {'loss': 0.3362, 'grad_norm': 6.5, 'learning_rate': 4.756910673866789e-05, 'epoch': 2.1360544217687076}\n",
            "\n",
            ">>> Step 1571 metrics: {'loss': 0.7063, 'grad_norm': 7.28125, 'learning_rate': 4.756603884168395e-05, 'epoch': 2.1374149659863946}\n",
            "\n",
            ">>> Step 1572 metrics: {'loss': 0.7277, 'grad_norm': 8.375, 'learning_rate': 4.756296910906611e-05, 'epoch': 2.1387755102040815}\n",
            "\n",
            ">>> Step 1573 metrics: {'loss': 0.4788, 'grad_norm': 7.34375, 'learning_rate': 4.755989754106407e-05, 'epoch': 2.140136054421769}\n",
            "\n",
            ">>> Step 1574 metrics: {'loss': 0.2287, 'grad_norm': 5.1875, 'learning_rate': 4.755682413792769e-05, 'epoch': 2.141496598639456}\n",
            "\n",
            ">>> Step 1575 metrics: {'loss': 0.3051, 'grad_norm': 6.75, 'learning_rate': 4.755374889990698e-05, 'epoch': 2.142857142857143}\n",
            "\n",
            ">>> Step 1576 metrics: {'loss': 0.4535, 'grad_norm': 7.03125, 'learning_rate': 4.755067182725208e-05, 'epoch': 2.1442176870748297}\n",
            "\n",
            ">>> Step 1577 metrics: {'loss': 0.5193, 'grad_norm': 6.125, 'learning_rate': 4.754759292021331e-05, 'epoch': 2.145578231292517}\n",
            "\n",
            ">>> Step 1578 metrics: {'loss': 0.4568, 'grad_norm': 6.8125, 'learning_rate': 4.754451217904111e-05, 'epoch': 2.146938775510204}\n",
            "\n",
            ">>> Step 1579 metrics: {'loss': 0.6305, 'grad_norm': 7.71875, 'learning_rate': 4.75414296039861e-05, 'epoch': 2.148299319727891}\n",
            "\n",
            ">>> Step 1580 metrics: {'loss': 0.4571, 'grad_norm': 7.21875, 'learning_rate': 4.753834519529902e-05, 'epoch': 2.1496598639455784}\n",
            "\n",
            ">>> Step 1581 metrics: {'loss': 0.5215, 'grad_norm': 6.6875, 'learning_rate': 4.753525895323077e-05, 'epoch': 2.1510204081632653}\n",
            "\n",
            ">>> Step 1582 metrics: {'loss': 0.4999, 'grad_norm': 7.3125, 'learning_rate': 4.7532170878032405e-05, 'epoch': 2.1523809523809523}\n",
            "\n",
            ">>> Step 1583 metrics: {'loss': 0.1902, 'grad_norm': 6.71875, 'learning_rate': 4.7529080969955125e-05, 'epoch': 2.1537414965986397}\n",
            "\n",
            ">>> Step 1584 metrics: {'loss': 0.2693, 'grad_norm': 5.1875, 'learning_rate': 4.7525989229250275e-05, 'epoch': 2.1551020408163266}\n",
            "\n",
            ">>> Step 1585 metrics: {'loss': 0.2716, 'grad_norm': 6.84375, 'learning_rate': 4.752289565616936e-05, 'epoch': 2.1564625850340136}\n",
            "\n",
            ">>> Step 1586 metrics: {'loss': 1.0041, 'grad_norm': 12.375, 'learning_rate': 4.7519800250964016e-05, 'epoch': 2.1578231292517005}\n",
            "\n",
            ">>> Step 1587 metrics: {'loss': 0.6369, 'grad_norm': 7.59375, 'learning_rate': 4.751670301388604e-05, 'epoch': 2.159183673469388}\n",
            "\n",
            ">>> Step 1588 metrics: {'loss': 1.0273, 'grad_norm': 10.5, 'learning_rate': 4.751360394518739e-05, 'epoch': 2.160544217687075}\n",
            "\n",
            ">>> Step 1589 metrics: {'loss': 0.4445, 'grad_norm': 6.5, 'learning_rate': 4.7510503045120144e-05, 'epoch': 2.1619047619047618}\n",
            "\n",
            ">>> Step 1590 metrics: {'loss': 0.4343, 'grad_norm': 9.125, 'learning_rate': 4.750740031393655e-05, 'epoch': 2.163265306122449}\n",
            "\n",
            ">>> Step 1591 metrics: {'loss': 0.6379, 'grad_norm': 7.3125, 'learning_rate': 4.750429575188901e-05, 'epoch': 2.164625850340136}\n",
            "\n",
            ">>> Step 1592 metrics: {'loss': 0.7433, 'grad_norm': 7.84375, 'learning_rate': 4.7501189359230036e-05, 'epoch': 2.165986394557823}\n",
            "\n",
            ">>> Step 1593 metrics: {'loss': 0.6109, 'grad_norm': 6.34375, 'learning_rate': 4.7498081136212344e-05, 'epoch': 2.16734693877551}\n",
            "\n",
            ">>> Step 1594 metrics: {'loss': 0.3837, 'grad_norm': 7.90625, 'learning_rate': 4.7494971083088774e-05, 'epoch': 2.1687074829931974}\n",
            "\n",
            ">>> Step 1595 metrics: {'loss': 0.4146, 'grad_norm': 6.40625, 'learning_rate': 4.74918592001123e-05, 'epoch': 2.1700680272108843}\n",
            "\n",
            ">>> Step 1596 metrics: {'loss': 0.214, 'grad_norm': 4.6875, 'learning_rate': 4.748874548753606e-05, 'epoch': 2.1714285714285713}\n",
            "\n",
            ">>> Step 1597 metrics: {'loss': 0.3663, 'grad_norm': 5.65625, 'learning_rate': 4.7485629945613336e-05, 'epoch': 2.1727891156462587}\n",
            "\n",
            ">>> Step 1598 metrics: {'loss': 0.5321, 'grad_norm': 6.84375, 'learning_rate': 4.748251257459757e-05, 'epoch': 2.1741496598639456}\n",
            "\n",
            ">>> Step 1599 metrics: {'loss': 0.3695, 'grad_norm': 6.21875, 'learning_rate': 4.7479393374742344e-05, 'epoch': 2.1755102040816325}\n",
            "\n",
            ">>> Step 1600 metrics: {'loss': 0.6836, 'grad_norm': 7.8125, 'learning_rate': 4.747627234630138e-05, 'epoch': 2.17687074829932}\n",
            "\n",
            ">>> Step 1601 metrics: {'loss': 0.2789, 'grad_norm': 5.71875, 'learning_rate': 4.747314948952857e-05, 'epoch': 2.178231292517007}\n",
            "\n",
            ">>> Step 1602 metrics: {'loss': 0.6555, 'grad_norm': 6.34375, 'learning_rate': 4.747002480467793e-05, 'epoch': 2.179591836734694}\n",
            "\n",
            ">>> Step 1603 metrics: {'loss': 0.8095, 'grad_norm': 6.65625, 'learning_rate': 4.746689829200365e-05, 'epoch': 2.1809523809523808}\n",
            "\n",
            ">>> Step 1604 metrics: {'loss': 1.2911, 'grad_norm': 11.0625, 'learning_rate': 4.746376995176005e-05, 'epoch': 2.182312925170068}\n",
            "\n",
            ">>> Step 1605 metrics: {'loss': 0.5052, 'grad_norm': 8.0, 'learning_rate': 4.74606397842016e-05, 'epoch': 2.183673469387755}\n",
            "\n",
            ">>> Step 1606 metrics: {'loss': 0.8364, 'grad_norm': 7.59375, 'learning_rate': 4.7457507789582934e-05, 'epoch': 2.185034013605442}\n",
            "\n",
            ">>> Step 1607 metrics: {'loss': 0.6193, 'grad_norm': 7.5625, 'learning_rate': 4.745437396815882e-05, 'epoch': 2.1863945578231294}\n",
            "\n",
            ">>> Step 1608 metrics: {'loss': 0.742, 'grad_norm': 7.78125, 'learning_rate': 4.7451238320184174e-05, 'epoch': 2.1877551020408164}\n",
            "\n",
            ">>> Step 1609 metrics: {'loss': 0.4379, 'grad_norm': 6.5, 'learning_rate': 4.744810084591407e-05, 'epoch': 2.1891156462585033}\n",
            "\n",
            ">>> Step 1610 metrics: {'loss': 0.6854, 'grad_norm': 7.8125, 'learning_rate': 4.7444961545603714e-05, 'epoch': 2.1904761904761907}\n",
            "\n",
            ">>> Step 1611 metrics: {'loss': 0.6099, 'grad_norm': 7.4375, 'learning_rate': 4.7441820419508495e-05, 'epoch': 2.1918367346938776}\n",
            "\n",
            ">>> Step 1612 metrics: {'loss': 0.2554, 'grad_norm': 6.34375, 'learning_rate': 4.743867746788391e-05, 'epoch': 2.1931972789115646}\n",
            "\n",
            ">>> Step 1613 metrics: {'loss': 0.8747, 'grad_norm': 7.46875, 'learning_rate': 4.743553269098563e-05, 'epoch': 2.1945578231292515}\n",
            "\n",
            ">>> Step 1614 metrics: {'loss': 0.4412, 'grad_norm': 6.46875, 'learning_rate': 4.743238608906946e-05, 'epoch': 2.195918367346939}\n",
            "\n",
            ">>> Step 1615 metrics: {'loss': 0.3475, 'grad_norm': 6.28125, 'learning_rate': 4.742923766239138e-05, 'epoch': 2.197278911564626}\n",
            "\n",
            ">>> Step 1616 metrics: {'loss': 0.3797, 'grad_norm': 7.0625, 'learning_rate': 4.7426087411207466e-05, 'epoch': 2.198639455782313}\n",
            "\n",
            ">>> Step 1617 metrics: {'loss': 0.262, 'grad_norm': 6.40625, 'learning_rate': 4.7422935335773996e-05, 'epoch': 2.2}\n",
            "\n",
            ">>> Step 1618 metrics: {'loss': 0.405, 'grad_norm': 7.40625, 'learning_rate': 4.7419781436347374e-05, 'epoch': 2.201360544217687}\n",
            "\n",
            ">>> Step 1619 metrics: {'loss': 0.5336, 'grad_norm': 6.5, 'learning_rate': 4.741662571318415e-05, 'epoch': 2.202721088435374}\n",
            "\n",
            ">>> Step 1620 metrics: {'loss': 0.4208, 'grad_norm': 5.84375, 'learning_rate': 4.741346816654103e-05, 'epoch': 2.204081632653061}\n",
            "\n",
            ">>> Step 1621 metrics: {'loss': 0.2901, 'grad_norm': 5.46875, 'learning_rate': 4.7410308796674864e-05, 'epoch': 2.2054421768707484}\n",
            "\n",
            ">>> Step 1622 metrics: {'loss': 0.7401, 'grad_norm': 7.40625, 'learning_rate': 4.740714760384265e-05, 'epoch': 2.2068027210884353}\n",
            "\n",
            ">>> Step 1623 metrics: {'loss': 0.3725, 'grad_norm': 5.875, 'learning_rate': 4.740398458830153e-05, 'epoch': 2.2081632653061223}\n",
            "\n",
            ">>> Step 1624 metrics: {'loss': 0.3558, 'grad_norm': 5.84375, 'learning_rate': 4.7400819750308805e-05, 'epoch': 2.2095238095238097}\n",
            "\n",
            ">>> Step 1625 metrics: {'loss': 0.3743, 'grad_norm': 6.625, 'learning_rate': 4.739765309012192e-05, 'epoch': 2.2108843537414966}\n",
            "\n",
            ">>> Step 1626 metrics: {'loss': 0.4266, 'grad_norm': 9.1875, 'learning_rate': 4.7394484607998465e-05, 'epoch': 2.2122448979591836}\n",
            "\n",
            ">>> Step 1627 metrics: {'loss': 0.8902, 'grad_norm': 7.75, 'learning_rate': 4.7391314304196175e-05, 'epoch': 2.213605442176871}\n",
            "\n",
            ">>> Step 1628 metrics: {'loss': 0.2532, 'grad_norm': 5.09375, 'learning_rate': 4.738814217897295e-05, 'epoch': 2.214965986394558}\n",
            "\n",
            ">>> Step 1629 metrics: {'loss': 0.2535, 'grad_norm': 5.25, 'learning_rate': 4.738496823258682e-05, 'epoch': 2.216326530612245}\n",
            "\n",
            ">>> Step 1630 metrics: {'loss': 0.5345, 'grad_norm': 7.96875, 'learning_rate': 4.7381792465295966e-05, 'epoch': 2.2176870748299318}\n",
            "\n",
            ">>> Step 1631 metrics: {'loss': 0.4584, 'grad_norm': 6.875, 'learning_rate': 4.737861487735872e-05, 'epoch': 2.219047619047619}\n",
            "\n",
            ">>> Step 1632 metrics: {'loss': 0.5552, 'grad_norm': 7.75, 'learning_rate': 4.737543546903357e-05, 'epoch': 2.220408163265306}\n",
            "\n",
            ">>> Step 1633 metrics: {'loss': 0.5897, 'grad_norm': 7.84375, 'learning_rate': 4.737225424057914e-05, 'epoch': 2.221768707482993}\n",
            "\n",
            ">>> Step 1634 metrics: {'loss': 0.3182, 'grad_norm': 5.78125, 'learning_rate': 4.736907119225421e-05, 'epoch': 2.2231292517006804}\n",
            "\n",
            ">>> Step 1635 metrics: {'loss': 0.4123, 'grad_norm': 6.46875, 'learning_rate': 4.736588632431771e-05, 'epoch': 2.2244897959183674}\n",
            "\n",
            ">>> Step 1636 metrics: {'loss': 0.2861, 'grad_norm': 5.71875, 'learning_rate': 4.7362699637028696e-05, 'epoch': 2.2258503401360543}\n",
            "\n",
            ">>> Step 1637 metrics: {'loss': 0.6033, 'grad_norm': 7.40625, 'learning_rate': 4.7359511130646405e-05, 'epoch': 2.2272108843537417}\n",
            "\n",
            ">>> Step 1638 metrics: {'loss': 0.3412, 'grad_norm': 6.46875, 'learning_rate': 4.73563208054302e-05, 'epoch': 2.2285714285714286}\n",
            "\n",
            ">>> Step 1639 metrics: {'loss': 0.6137, 'grad_norm': 7.125, 'learning_rate': 4.73531286616396e-05, 'epoch': 2.2299319727891156}\n",
            "\n",
            ">>> Step 1640 metrics: {'loss': 0.2515, 'grad_norm': 8.125, 'learning_rate': 4.734993469953426e-05, 'epoch': 2.2312925170068025}\n",
            "\n",
            ">>> Step 1641 metrics: {'loss': 0.4429, 'grad_norm': 7.09375, 'learning_rate': 4.734673891937401e-05, 'epoch': 2.23265306122449}\n",
            "\n",
            ">>> Step 1642 metrics: {'loss': 0.3953, 'grad_norm': 6.28125, 'learning_rate': 4.734354132141879e-05, 'epoch': 2.234013605442177}\n",
            "\n",
            ">>> Step 1643 metrics: {'loss': 0.3055, 'grad_norm': 5.6875, 'learning_rate': 4.734034190592874e-05, 'epoch': 2.235374149659864}\n",
            "\n",
            ">>> Step 1644 metrics: {'loss': 0.3755, 'grad_norm': 6.875, 'learning_rate': 4.733714067316408e-05, 'epoch': 2.236734693877551}\n",
            "\n",
            ">>> Step 1645 metrics: {'loss': 0.6389, 'grad_norm': 8.1875, 'learning_rate': 4.733393762338524e-05, 'epoch': 2.238095238095238}\n",
            "\n",
            ">>> Step 1646 metrics: {'loss': 0.3084, 'grad_norm': 6.34375, 'learning_rate': 4.7330732756852756e-05, 'epoch': 2.239455782312925}\n",
            "\n",
            ">>> Step 1647 metrics: {'loss': 0.7481, 'grad_norm': 7.8125, 'learning_rate': 4.732752607382734e-05, 'epoch': 2.240816326530612}\n",
            "\n",
            ">>> Step 1648 metrics: {'loss': 0.8448, 'grad_norm': 8.1875, 'learning_rate': 4.7324317574569834e-05, 'epoch': 2.2421768707482994}\n",
            "\n",
            ">>> Step 1649 metrics: {'loss': 0.3801, 'grad_norm': 6.78125, 'learning_rate': 4.732110725934122e-05, 'epoch': 2.2435374149659864}\n",
            "\n",
            ">>> Step 1650 metrics: {'loss': 0.3519, 'grad_norm': 6.1875, 'learning_rate': 4.7317895128402676e-05, 'epoch': 2.2448979591836733}\n",
            "\n",
            ">>> Step 1651 metrics: {'loss': 0.5449, 'grad_norm': 7.9375, 'learning_rate': 4.731468118201546e-05, 'epoch': 2.2462585034013607}\n",
            "\n",
            ">>> Step 1652 metrics: {'loss': 0.3241, 'grad_norm': 6.03125, 'learning_rate': 4.7311465420441026e-05, 'epoch': 2.2476190476190476}\n",
            "\n",
            ">>> Step 1653 metrics: {'loss': 1.0697, 'grad_norm': 9.9375, 'learning_rate': 4.730824784394094e-05, 'epoch': 2.2489795918367346}\n",
            "\n",
            ">>> Step 1654 metrics: {'loss': 0.3298, 'grad_norm': 6.53125, 'learning_rate': 4.7305028452776975e-05, 'epoch': 2.250340136054422}\n",
            "\n",
            ">>> Step 1655 metrics: {'loss': 0.3614, 'grad_norm': 6.5, 'learning_rate': 4.730180724721097e-05, 'epoch': 2.251700680272109}\n",
            "\n",
            ">>> Step 1656 metrics: {'loss': 0.463, 'grad_norm': 7.625, 'learning_rate': 4.7298584227504984e-05, 'epoch': 2.253061224489796}\n",
            "\n",
            ">>> Step 1657 metrics: {'loss': 0.2613, 'grad_norm': 7.65625, 'learning_rate': 4.729535939392118e-05, 'epoch': 2.2544217687074832}\n",
            "\n",
            ">>> Step 1658 metrics: {'loss': 0.301, 'grad_norm': 5.78125, 'learning_rate': 4.729213274672188e-05, 'epoch': 2.25578231292517}\n",
            "\n",
            ">>> Step 1659 metrics: {'loss': 0.3566, 'grad_norm': 6.75, 'learning_rate': 4.728890428616956e-05, 'epoch': 2.257142857142857}\n",
            "\n",
            ">>> Step 1660 metrics: {'loss': 0.3805, 'grad_norm': 6.65625, 'learning_rate': 4.728567401252684e-05, 'epoch': 2.258503401360544}\n",
            "\n",
            ">>> Step 1661 metrics: {'loss': 0.3907, 'grad_norm': 6.65625, 'learning_rate': 4.7282441926056485e-05, 'epoch': 2.2598639455782314}\n",
            "\n",
            ">>> Step 1662 metrics: {'loss': 0.6897, 'grad_norm': 7.75, 'learning_rate': 4.727920802702141e-05, 'epoch': 2.2612244897959184}\n",
            "\n",
            ">>> Step 1663 metrics: {'loss': 0.5301, 'grad_norm': 6.53125, 'learning_rate': 4.7275972315684676e-05, 'epoch': 2.2625850340136053}\n",
            "\n",
            ">>> Step 1664 metrics: {'loss': 0.492, 'grad_norm': 7.5, 'learning_rate': 4.7272734792309495e-05, 'epoch': 2.2639455782312927}\n",
            "\n",
            ">>> Step 1665 metrics: {'loss': 0.8243, 'grad_norm': 7.46875, 'learning_rate': 4.7269495457159204e-05, 'epoch': 2.2653061224489797}\n",
            "\n",
            ">>> Step 1666 metrics: {'loss': 0.6715, 'grad_norm': 7.15625, 'learning_rate': 4.7266254310497334e-05, 'epoch': 2.2666666666666666}\n",
            "\n",
            ">>> Step 1667 metrics: {'loss': 0.3783, 'grad_norm': 6.75, 'learning_rate': 4.726301135258753e-05, 'epoch': 2.2680272108843536}\n",
            "\n",
            ">>> Step 1668 metrics: {'loss': 0.2019, 'grad_norm': 6.0625, 'learning_rate': 4.725976658369357e-05, 'epoch': 2.269387755102041}\n",
            "\n",
            ">>> Step 1669 metrics: {'loss': 0.6922, 'grad_norm': 9.25, 'learning_rate': 4.725652000407943e-05, 'epoch': 2.270748299319728}\n",
            "\n",
            ">>> Step 1670 metrics: {'loss': 0.5036, 'grad_norm': 6.28125, 'learning_rate': 4.725327161400918e-05, 'epoch': 2.272108843537415}\n",
            "\n",
            ">>> Step 1671 metrics: {'loss': 0.384, 'grad_norm': 6.53125, 'learning_rate': 4.725002141374707e-05, 'epoch': 2.273469387755102}\n",
            "\n",
            ">>> Step 1672 metrics: {'loss': 0.4431, 'grad_norm': 6.71875, 'learning_rate': 4.7246769403557475e-05, 'epoch': 2.274829931972789}\n",
            "\n",
            ">>> Step 1673 metrics: {'loss': 0.4612, 'grad_norm': 6.125, 'learning_rate': 4.7243515583704956e-05, 'epoch': 2.276190476190476}\n",
            "\n",
            ">>> Step 1674 metrics: {'loss': 0.6536, 'grad_norm': 7.8125, 'learning_rate': 4.724025995445417e-05, 'epoch': 2.277551020408163}\n",
            "\n",
            ">>> Step 1675 metrics: {'loss': 0.4938, 'grad_norm': 7.78125, 'learning_rate': 4.723700251606995e-05, 'epoch': 2.2789115646258504}\n",
            "\n",
            ">>> Step 1676 metrics: {'loss': 0.3935, 'grad_norm': 7.875, 'learning_rate': 4.7233743268817275e-05, 'epoch': 2.2802721088435374}\n",
            "\n",
            ">>> Step 1677 metrics: {'loss': 0.1851, 'grad_norm': 5.53125, 'learning_rate': 4.723048221296128e-05, 'epoch': 2.2816326530612243}\n",
            "\n",
            ">>> Step 1678 metrics: {'loss': 0.7086, 'grad_norm': 6.46875, 'learning_rate': 4.7227219348767225e-05, 'epoch': 2.2829931972789117}\n",
            "\n",
            ">>> Step 1679 metrics: {'loss': 0.6224, 'grad_norm': 7.5, 'learning_rate': 4.7223954676500524e-05, 'epoch': 2.2843537414965986}\n",
            "\n",
            ">>> Step 1680 metrics: {'loss': 0.4756, 'grad_norm': 7.1875, 'learning_rate': 4.722068819642675e-05, 'epoch': 2.2857142857142856}\n",
            "\n",
            ">>> Step 1681 metrics: {'loss': 0.5649, 'grad_norm': 7.5, 'learning_rate': 4.721741990881161e-05, 'epoch': 2.287074829931973}\n",
            "\n",
            ">>> Step 1682 metrics: {'loss': 0.6332, 'grad_norm': 7.15625, 'learning_rate': 4.721414981392096e-05, 'epoch': 2.28843537414966}\n",
            "\n",
            ">>> Step 1683 metrics: {'loss': 0.6042, 'grad_norm': 7.0625, 'learning_rate': 4.7210877912020814e-05, 'epoch': 2.289795918367347}\n",
            "\n",
            ">>> Step 1684 metrics: {'loss': 0.4492, 'grad_norm': 8.1875, 'learning_rate': 4.7207604203377316e-05, 'epoch': 2.2911564625850342}\n",
            "\n",
            ">>> Step 1685 metrics: {'loss': 0.5167, 'grad_norm': 6.875, 'learning_rate': 4.720432868825677e-05, 'epoch': 2.292517006802721}\n",
            "\n",
            ">>> Step 1686 metrics: {'loss': 0.7052, 'grad_norm': 7.09375, 'learning_rate': 4.7201051366925623e-05, 'epoch': 2.293877551020408}\n",
            "\n",
            ">>> Step 1687 metrics: {'loss': 0.672, 'grad_norm': 9.4375, 'learning_rate': 4.719777223965047e-05, 'epoch': 2.295238095238095}\n",
            "\n",
            ">>> Step 1688 metrics: {'loss': 0.5643, 'grad_norm': 6.53125, 'learning_rate': 4.719449130669805e-05, 'epoch': 2.2965986394557825}\n",
            "\n",
            ">>> Step 1689 metrics: {'loss': 0.4455, 'grad_norm': 6.65625, 'learning_rate': 4.7191208568335253e-05, 'epoch': 2.2979591836734694}\n",
            "\n",
            ">>> Step 1690 metrics: {'loss': 0.3793, 'grad_norm': 6.34375, 'learning_rate': 4.7187924024829106e-05, 'epoch': 2.2993197278911564}\n",
            "\n",
            ">>> Step 1691 metrics: {'loss': 0.5345, 'grad_norm': 6.46875, 'learning_rate': 4.71846376764468e-05, 'epoch': 2.3006802721088437}\n",
            "\n",
            ">>> Step 1692 metrics: {'loss': 0.2965, 'grad_norm': 5.53125, 'learning_rate': 4.718134952345565e-05, 'epoch': 2.3020408163265307}\n",
            "\n",
            ">>> Step 1693 metrics: {'loss': 0.29, 'grad_norm': 6.25, 'learning_rate': 4.717805956612314e-05, 'epoch': 2.3034013605442176}\n",
            "\n",
            ">>> Step 1694 metrics: {'loss': 0.3095, 'grad_norm': 5.84375, 'learning_rate': 4.717476780471689e-05, 'epoch': 2.3047619047619046}\n",
            "\n",
            ">>> Step 1695 metrics: {'loss': 0.1862, 'grad_norm': 4.3125, 'learning_rate': 4.717147423950467e-05, 'epoch': 2.306122448979592}\n",
            "\n",
            ">>> Step 1696 metrics: {'loss': 0.3161, 'grad_norm': 6.3125, 'learning_rate': 4.716817887075439e-05, 'epoch': 2.307482993197279}\n",
            "\n",
            ">>> Step 1697 metrics: {'loss': 0.3955, 'grad_norm': 6.75, 'learning_rate': 4.716488169873412e-05, 'epoch': 2.308843537414966}\n",
            "\n",
            ">>> Step 1698 metrics: {'loss': 0.4052, 'grad_norm': 6.4375, 'learning_rate': 4.716158272371206e-05, 'epoch': 2.3102040816326532}\n",
            "\n",
            ">>> Step 1699 metrics: {'loss': 0.3859, 'grad_norm': 6.6875, 'learning_rate': 4.715828194595657e-05, 'epoch': 2.31156462585034}\n",
            "\n",
            ">>> Step 1700 metrics: {'loss': 0.3262, 'grad_norm': 6.375, 'learning_rate': 4.715497936573615e-05, 'epoch': 2.312925170068027}\n",
            "\n",
            ">>> Step 1701 metrics: {'loss': 0.2228, 'grad_norm': 5.9375, 'learning_rate': 4.7151674983319444e-05, 'epoch': 2.314285714285714}\n",
            "\n",
            ">>> Step 1702 metrics: {'loss': 0.3677, 'grad_norm': 6.34375, 'learning_rate': 4.714836879897525e-05, 'epoch': 2.3156462585034014}\n",
            "\n",
            ">>> Step 1703 metrics: {'loss': 0.184, 'grad_norm': 8.9375, 'learning_rate': 4.714506081297252e-05, 'epoch': 2.3170068027210884}\n",
            "\n",
            ">>> Step 1704 metrics: {'loss': 0.7614, 'grad_norm': 8.25, 'learning_rate': 4.714175102558033e-05, 'epoch': 2.3183673469387753}\n",
            "\n",
            ">>> Step 1705 metrics: {'loss': 0.7094, 'grad_norm': 10.6875, 'learning_rate': 4.7138439437067906e-05, 'epoch': 2.3197278911564627}\n",
            "\n",
            ">>> Step 1706 metrics: {'loss': 0.2892, 'grad_norm': 5.65625, 'learning_rate': 4.7135126047704645e-05, 'epoch': 2.3210884353741497}\n",
            "\n",
            ">>> Step 1707 metrics: {'loss': 0.596, 'grad_norm': 6.875, 'learning_rate': 4.713181085776008e-05, 'epoch': 2.3224489795918366}\n",
            "\n",
            ">>> Step 1708 metrics: {'loss': 0.4102, 'grad_norm': 7.53125, 'learning_rate': 4.7128493867503865e-05, 'epoch': 2.323809523809524}\n",
            "\n",
            ">>> Step 1709 metrics: {'loss': 0.3462, 'grad_norm': 5.71875, 'learning_rate': 4.712517507720583e-05, 'epoch': 2.325170068027211}\n",
            "\n",
            ">>> Step 1710 metrics: {'loss': 0.9983, 'grad_norm': 8.625, 'learning_rate': 4.7121854487135954e-05, 'epoch': 2.326530612244898}\n",
            "\n",
            ">>> Step 1711 metrics: {'loss': 0.2116, 'grad_norm': 5.125, 'learning_rate': 4.7118532097564325e-05, 'epoch': 2.3278911564625853}\n",
            "\n",
            ">>> Step 1712 metrics: {'loss': 0.6473, 'grad_norm': 8.1875, 'learning_rate': 4.7115207908761224e-05, 'epoch': 2.329251700680272}\n",
            "\n",
            ">>> Step 1713 metrics: {'loss': 0.2732, 'grad_norm': 6.09375, 'learning_rate': 4.711188192099704e-05, 'epoch': 2.330612244897959}\n",
            "\n",
            ">>> Step 1714 metrics: {'loss': 0.454, 'grad_norm': 6.40625, 'learning_rate': 4.7108554134542346e-05, 'epoch': 2.331972789115646}\n",
            "\n",
            ">>> Step 1715 metrics: {'loss': 0.247, 'grad_norm': 4.625, 'learning_rate': 4.7105224549667825e-05, 'epoch': 2.3333333333333335}\n",
            "\n",
            ">>> Step 1716 metrics: {'loss': 0.434, 'grad_norm': 6.75, 'learning_rate': 4.710189316664433e-05, 'epoch': 2.3346938775510204}\n",
            "\n",
            ">>> Step 1717 metrics: {'loss': 0.5434, 'grad_norm': 6.875, 'learning_rate': 4.7098559985742844e-05, 'epoch': 2.3360544217687074}\n",
            "\n",
            ">>> Step 1718 metrics: {'loss': 0.3268, 'grad_norm': 5.90625, 'learning_rate': 4.7095225007234506e-05, 'epoch': 2.3374149659863948}\n",
            "\n",
            ">>> Step 1719 metrics: {'loss': 0.4271, 'grad_norm': 6.9375, 'learning_rate': 4.7091888231390607e-05, 'epoch': 2.3387755102040817}\n",
            "\n",
            ">>> Step 1720 metrics: {'loss': 0.3306, 'grad_norm': 5.875, 'learning_rate': 4.7088549658482575e-05, 'epoch': 2.3401360544217686}\n",
            "\n",
            ">>> Step 1721 metrics: {'loss': 0.3983, 'grad_norm': 6.21875, 'learning_rate': 4.7085209288781984e-05, 'epoch': 2.3414965986394556}\n",
            "\n",
            ">>> Step 1722 metrics: {'loss': 0.3115, 'grad_norm': 5.78125, 'learning_rate': 4.7081867122560555e-05, 'epoch': 2.342857142857143}\n",
            "\n",
            ">>> Step 1723 metrics: {'loss': 0.7625, 'grad_norm': 7.78125, 'learning_rate': 4.707852316009016e-05, 'epoch': 2.34421768707483}\n",
            "\n",
            ">>> Step 1724 metrics: {'loss': 0.6517, 'grad_norm': 6.4375, 'learning_rate': 4.7075177401642807e-05, 'epoch': 2.345578231292517}\n",
            "\n",
            ">>> Step 1725 metrics: {'loss': 0.3988, 'grad_norm': 6.6875, 'learning_rate': 4.707182984749067e-05, 'epoch': 2.3469387755102042}\n",
            "\n",
            ">>> Step 1726 metrics: {'loss': 0.2048, 'grad_norm': 7.09375, 'learning_rate': 4.706848049790603e-05, 'epoch': 2.348299319727891}\n",
            "\n",
            ">>> Step 1727 metrics: {'loss': 0.8894, 'grad_norm': 9.4375, 'learning_rate': 4.706512935316137e-05, 'epoch': 2.349659863945578}\n",
            "\n",
            ">>> Step 1728 metrics: {'loss': 0.6022, 'grad_norm': 6.9375, 'learning_rate': 4.706177641352927e-05, 'epoch': 2.351020408163265}\n",
            "\n",
            ">>> Step 1729 metrics: {'loss': 0.4952, 'grad_norm': 7.09375, 'learning_rate': 4.7058421679282484e-05, 'epoch': 2.3523809523809525}\n",
            "\n",
            ">>> Step 1730 metrics: {'loss': 0.4184, 'grad_norm': 6.15625, 'learning_rate': 4.70550651506939e-05, 'epoch': 2.3537414965986394}\n",
            "\n",
            ">>> Step 1731 metrics: {'loss': 0.6934, 'grad_norm': 6.78125, 'learning_rate': 4.7051706828036545e-05, 'epoch': 2.3551020408163263}\n",
            "\n",
            ">>> Step 1732 metrics: {'loss': 0.3874, 'grad_norm': 6.28125, 'learning_rate': 4.7048346711583625e-05, 'epoch': 2.3564625850340137}\n",
            "\n",
            ">>> Step 1733 metrics: {'loss': 0.1561, 'grad_norm': 5.375, 'learning_rate': 4.704498480160844e-05, 'epoch': 2.3578231292517007}\n",
            "\n",
            ">>> Step 1734 metrics: {'loss': 0.3805, 'grad_norm': 6.4375, 'learning_rate': 4.704162109838448e-05, 'epoch': 2.3591836734693876}\n",
            "\n",
            ">>> Step 1735 metrics: {'loss': 0.1661, 'grad_norm': 4.71875, 'learning_rate': 4.703825560218537e-05, 'epoch': 2.360544217687075}\n",
            "\n",
            ">>> Step 1736 metrics: {'loss': 0.3348, 'grad_norm': 5.4375, 'learning_rate': 4.7034888313284866e-05, 'epoch': 2.361904761904762}\n",
            "\n",
            ">>> Step 1737 metrics: {'loss': 0.3215, 'grad_norm': 5.46875, 'learning_rate': 4.7031519231956887e-05, 'epoch': 2.363265306122449}\n",
            "\n",
            ">>> Step 1738 metrics: {'loss': 0.2712, 'grad_norm': 5.09375, 'learning_rate': 4.7028148358475486e-05, 'epoch': 2.3646258503401363}\n",
            "\n",
            ">>> Step 1739 metrics: {'loss': 0.4625, 'grad_norm': 6.875, 'learning_rate': 4.702477569311487e-05, 'epoch': 2.3659863945578232}\n",
            "\n",
            ">>> Step 1740 metrics: {'loss': 0.4886, 'grad_norm': 6.96875, 'learning_rate': 4.702140123614939e-05, 'epoch': 2.36734693877551}\n",
            "\n",
            ">>> Step 1741 metrics: {'loss': 0.4312, 'grad_norm': 6.59375, 'learning_rate': 4.7018024987853525e-05, 'epoch': 2.368707482993197}\n",
            "\n",
            ">>> Step 1742 metrics: {'loss': 0.5239, 'grad_norm': 7.71875, 'learning_rate': 4.701464694850194e-05, 'epoch': 2.3700680272108845}\n",
            "\n",
            ">>> Step 1743 metrics: {'loss': 0.6086, 'grad_norm': 9.375, 'learning_rate': 4.70112671183694e-05, 'epoch': 2.3714285714285714}\n",
            "\n",
            ">>> Step 1744 metrics: {'loss': 0.5933, 'grad_norm': 7.625, 'learning_rate': 4.7007885497730864e-05, 'epoch': 2.3727891156462584}\n",
            "\n",
            ">>> Step 1745 metrics: {'loss': 0.4159, 'grad_norm': 9.125, 'learning_rate': 4.7004502086861385e-05, 'epoch': 2.3741496598639458}\n",
            "\n",
            ">>> Step 1746 metrics: {'loss': 0.3444, 'grad_norm': 6.46875, 'learning_rate': 4.700111688603619e-05, 'epoch': 2.3755102040816327}\n",
            "\n",
            ">>> Step 1747 metrics: {'loss': 0.2004, 'grad_norm': 5.09375, 'learning_rate': 4.6997729895530664e-05, 'epoch': 2.3768707482993197}\n",
            "\n",
            ">>> Step 1748 metrics: {'loss': 1.1152, 'grad_norm': 13.0625, 'learning_rate': 4.69943411156203e-05, 'epoch': 2.3782312925170066}\n",
            "\n",
            ">>> Step 1749 metrics: {'loss': 0.3912, 'grad_norm': 6.21875, 'learning_rate': 4.699095054658077e-05, 'epoch': 2.379591836734694}\n",
            "\n",
            ">>> Step 1750 metrics: {'loss': 0.3649, 'grad_norm': 5.9375, 'learning_rate': 4.698755818868789e-05, 'epoch': 2.380952380952381}\n",
            "\n",
            ">>> Step 1751 metrics: {'loss': 0.5123, 'grad_norm': 7.28125, 'learning_rate': 4.698416404221759e-05, 'epoch': 2.382312925170068}\n",
            "\n",
            ">>> Step 1752 metrics: {'loss': 0.5984, 'grad_norm': 7.9375, 'learning_rate': 4.698076810744597e-05, 'epoch': 2.3836734693877553}\n",
            "\n",
            ">>> Step 1753 metrics: {'loss': 0.5096, 'grad_norm': 7.90625, 'learning_rate': 4.697737038464929e-05, 'epoch': 2.385034013605442}\n",
            "\n",
            ">>> Step 1754 metrics: {'loss': 0.3494, 'grad_norm': 6.0625, 'learning_rate': 4.697397087410392e-05, 'epoch': 2.386394557823129}\n",
            "\n",
            ">>> Step 1755 metrics: {'loss': 0.4222, 'grad_norm': 7.09375, 'learning_rate': 4.697056957608641e-05, 'epoch': 2.387755102040816}\n",
            "\n",
            ">>> Step 1756 metrics: {'loss': 0.7267, 'grad_norm': 8.0625, 'learning_rate': 4.696716649087342e-05, 'epoch': 2.3891156462585035}\n",
            "\n",
            ">>> Step 1757 metrics: {'loss': 0.2604, 'grad_norm': 5.34375, 'learning_rate': 4.6963761618741785e-05, 'epoch': 2.3904761904761904}\n",
            "\n",
            ">>> Step 1758 metrics: {'loss': 0.3711, 'grad_norm': 7.21875, 'learning_rate': 4.6960354959968466e-05, 'epoch': 2.3918367346938774}\n",
            "\n",
            ">>> Step 1759 metrics: {'loss': 0.4476, 'grad_norm': 5.9375, 'learning_rate': 4.695694651483059e-05, 'epoch': 2.3931972789115648}\n",
            "\n",
            ">>> Step 1760 metrics: {'loss': 0.447, 'grad_norm': 6.8125, 'learning_rate': 4.69535362836054e-05, 'epoch': 2.3945578231292517}\n",
            "\n",
            ">>> Step 1761 metrics: {'loss': 0.268, 'grad_norm': 5.65625, 'learning_rate': 4.695012426657033e-05, 'epoch': 2.3959183673469386}\n",
            "\n",
            ">>> Step 1762 metrics: {'loss': 0.2556, 'grad_norm': 6.96875, 'learning_rate': 4.6946710464002896e-05, 'epoch': 2.397278911564626}\n",
            "\n",
            ">>> Step 1763 metrics: {'loss': 0.5865, 'grad_norm': 6.1875, 'learning_rate': 4.694329487618081e-05, 'epoch': 2.398639455782313}\n",
            "\n",
            ">>> Step 1764 metrics: {'loss': 0.8136, 'grad_norm': 8.3125, 'learning_rate': 4.6939877503381914e-05, 'epoch': 2.4}\n",
            "\n",
            ">>> Step 1765 metrics: {'loss': 0.5045, 'grad_norm': 7.84375, 'learning_rate': 4.6936458345884195e-05, 'epoch': 2.4013605442176873}\n",
            "\n",
            ">>> Step 1766 metrics: {'loss': 0.3893, 'grad_norm': 7.34375, 'learning_rate': 4.6933037403965775e-05, 'epoch': 2.4027210884353742}\n",
            "\n",
            ">>> Step 1767 metrics: {'loss': 0.6129, 'grad_norm': 7.34375, 'learning_rate': 4.692961467790494e-05, 'epoch': 2.404081632653061}\n",
            "\n",
            ">>> Step 1768 metrics: {'loss': 0.5246, 'grad_norm': 6.9375, 'learning_rate': 4.692619016798011e-05, 'epoch': 2.405442176870748}\n",
            "\n",
            ">>> Step 1769 metrics: {'loss': 0.1972, 'grad_norm': 5.5625, 'learning_rate': 4.692276387446985e-05, 'epoch': 2.4068027210884355}\n",
            "\n",
            ">>> Step 1770 metrics: {'loss': 0.6601, 'grad_norm': 8.25, 'learning_rate': 4.691933579765288e-05, 'epoch': 2.4081632653061225}\n",
            "\n",
            ">>> Step 1771 metrics: {'loss': 0.446, 'grad_norm': 7.125, 'learning_rate': 4.6915905937808035e-05, 'epoch': 2.4095238095238094}\n",
            "\n",
            ">>> Step 1772 metrics: {'loss': 0.3182, 'grad_norm': 6.09375, 'learning_rate': 4.691247429521434e-05, 'epoch': 2.410884353741497}\n",
            "\n",
            ">>> Step 1773 metrics: {'loss': 0.2878, 'grad_norm': 5.46875, 'learning_rate': 4.690904087015092e-05, 'epoch': 2.4122448979591837}\n",
            "\n",
            ">>> Step 1774 metrics: {'loss': 0.4759, 'grad_norm': 7.1875, 'learning_rate': 4.690560566289709e-05, 'epoch': 2.4136054421768707}\n",
            "\n",
            ">>> Step 1775 metrics: {'loss': 0.4885, 'grad_norm': 6.5, 'learning_rate': 4.6902168673732275e-05, 'epoch': 2.4149659863945576}\n",
            "\n",
            ">>> Step 1776 metrics: {'loss': 0.5209, 'grad_norm': 7.5, 'learning_rate': 4.6898729902936056e-05, 'epoch': 2.416326530612245}\n",
            "\n",
            ">>> Step 1777 metrics: {'loss': 0.4952, 'grad_norm': 6.1875, 'learning_rate': 4.689528935078816e-05, 'epoch': 2.417687074829932}\n",
            "\n",
            ">>> Step 1778 metrics: {'loss': 0.3839, 'grad_norm': 6.28125, 'learning_rate': 4.689184701756846e-05, 'epoch': 2.419047619047619}\n",
            "\n",
            ">>> Step 1779 metrics: {'loss': 0.42, 'grad_norm': 7.84375, 'learning_rate': 4.6888402903556976e-05, 'epoch': 2.4204081632653063}\n",
            "\n",
            ">>> Step 1780 metrics: {'loss': 0.4225, 'grad_norm': 6.84375, 'learning_rate': 4.688495700903387e-05, 'epoch': 2.421768707482993}\n",
            "\n",
            ">>> Step 1781 metrics: {'loss': 0.421, 'grad_norm': 7.4375, 'learning_rate': 4.688150933427944e-05, 'epoch': 2.42312925170068}\n",
            "\n",
            ">>> Step 1782 metrics: {'loss': 0.4057, 'grad_norm': 7.3125, 'learning_rate': 4.687805987957414e-05, 'epoch': 2.424489795918367}\n",
            "\n",
            ">>> Step 1783 metrics: {'loss': 0.2758, 'grad_norm': 5.40625, 'learning_rate': 4.687460864519857e-05, 'epoch': 2.4258503401360545}\n",
            "\n",
            ">>> Step 1784 metrics: {'loss': 0.2798, 'grad_norm': 6.21875, 'learning_rate': 4.687115563143346e-05, 'epoch': 2.4272108843537414}\n",
            "\n",
            ">>> Step 1785 metrics: {'loss': 0.4192, 'grad_norm': 6.375, 'learning_rate': 4.6867700838559716e-05, 'epoch': 2.4285714285714284}\n",
            "\n",
            ">>> Step 1786 metrics: {'loss': 0.2906, 'grad_norm': 7.25, 'learning_rate': 4.686424426685835e-05, 'epoch': 2.4299319727891158}\n",
            "\n",
            ">>> Step 1787 metrics: {'loss': 0.3955, 'grad_norm': 6.53125, 'learning_rate': 4.686078591661054e-05, 'epoch': 2.4312925170068027}\n",
            "\n",
            ">>> Step 1788 metrics: {'loss': 0.4567, 'grad_norm': 6.875, 'learning_rate': 4.6857325788097606e-05, 'epoch': 2.4326530612244897}\n",
            "\n",
            ">>> Step 1789 metrics: {'loss': 0.209, 'grad_norm': 5.375, 'learning_rate': 4.685386388160102e-05, 'epoch': 2.434013605442177}\n",
            "\n",
            ">>> Step 1790 metrics: {'loss': 0.3358, 'grad_norm': 6.21875, 'learning_rate': 4.685040019740238e-05, 'epoch': 2.435374149659864}\n",
            "\n",
            ">>> Step 1791 metrics: {'loss': 0.3366, 'grad_norm': 6.625, 'learning_rate': 4.6846934735783443e-05, 'epoch': 2.436734693877551}\n",
            "\n",
            ">>> Step 1792 metrics: {'loss': 0.6963, 'grad_norm': 7.46875, 'learning_rate': 4.684346749702611e-05, 'epoch': 2.4380952380952383}\n",
            "\n",
            ">>> Step 1793 metrics: {'loss': 0.4982, 'grad_norm': 7.25, 'learning_rate': 4.6839998481412416e-05, 'epoch': 2.4394557823129253}\n",
            "\n",
            ">>> Step 1794 metrics: {'loss': 0.2699, 'grad_norm': 6.0, 'learning_rate': 4.6836527689224555e-05, 'epoch': 2.440816326530612}\n",
            "\n",
            ">>> Step 1795 metrics: {'loss': 0.594, 'grad_norm': 7.5, 'learning_rate': 4.683305512074486e-05, 'epoch': 2.442176870748299}\n",
            "\n",
            ">>> Step 1796 metrics: {'loss': 0.3803, 'grad_norm': 6.46875, 'learning_rate': 4.68295807762558e-05, 'epoch': 2.4435374149659865}\n",
            "\n",
            ">>> Step 1797 metrics: {'loss': 0.2932, 'grad_norm': 5.53125, 'learning_rate': 4.682610465604e-05, 'epoch': 2.4448979591836735}\n",
            "\n",
            ">>> Step 1798 metrics: {'loss': 0.5991, 'grad_norm': 8.5, 'learning_rate': 4.682262676038023e-05, 'epoch': 2.4462585034013604}\n",
            "\n",
            ">>> Step 1799 metrics: {'loss': 0.525, 'grad_norm': 6.5625, 'learning_rate': 4.681914708955938e-05, 'epoch': 2.447619047619048}\n",
            "\n",
            ">>> Step 1800 metrics: {'loss': 0.8106, 'grad_norm': 8.25, 'learning_rate': 4.6815665643860525e-05, 'epoch': 2.4489795918367347}\n",
            "\n",
            ">>> Step 1801 metrics: {'loss': 0.3829, 'grad_norm': 6.09375, 'learning_rate': 4.681218242356685e-05, 'epoch': 2.4503401360544217}\n",
            "\n",
            ">>> Step 1802 metrics: {'loss': 0.4107, 'grad_norm': 6.34375, 'learning_rate': 4.680869742896171e-05, 'epoch': 2.4517006802721086}\n",
            "\n",
            ">>> Step 1803 metrics: {'loss': 0.6711, 'grad_norm': 6.0625, 'learning_rate': 4.680521066032858e-05, 'epoch': 2.453061224489796}\n",
            "\n",
            ">>> Step 1804 metrics: {'loss': 0.2875, 'grad_norm': 5.59375, 'learning_rate': 4.6801722117951096e-05, 'epoch': 2.454421768707483}\n",
            "\n",
            ">>> Step 1805 metrics: {'loss': 0.3133, 'grad_norm': 6.34375, 'learning_rate': 4.679823180211303e-05, 'epoch': 2.45578231292517}\n",
            "\n",
            ">>> Step 1806 metrics: {'loss': 0.4203, 'grad_norm': 7.75, 'learning_rate': 4.679473971309831e-05, 'epoch': 2.4571428571428573}\n",
            "\n",
            ">>> Step 1807 metrics: {'loss': 0.4827, 'grad_norm': 7.125, 'learning_rate': 4.679124585119099e-05, 'epoch': 2.4585034013605442}\n",
            "\n",
            ">>> Step 1808 metrics: {'loss': 0.5236, 'grad_norm': 6.40625, 'learning_rate': 4.6787750216675286e-05, 'epoch': 2.459863945578231}\n",
            "\n",
            ">>> Step 1809 metrics: {'loss': 0.8235, 'grad_norm': 8.5625, 'learning_rate': 4.678425280983554e-05, 'epoch': 2.461224489795918}\n",
            "\n",
            ">>> Step 1810 metrics: {'loss': 0.7719, 'grad_norm': 7.9375, 'learning_rate': 4.678075363095626e-05, 'epoch': 2.4625850340136055}\n",
            "\n",
            ">>> Step 1811 metrics: {'loss': 0.2979, 'grad_norm': 6.46875, 'learning_rate': 4.677725268032208e-05, 'epoch': 2.4639455782312925}\n",
            "\n",
            ">>> Step 1812 metrics: {'loss': 0.3303, 'grad_norm': 6.90625, 'learning_rate': 4.6773749958217795e-05, 'epoch': 2.4653061224489794}\n",
            "\n",
            ">>> Step 1813 metrics: {'loss': 0.6487, 'grad_norm': 7.5, 'learning_rate': 4.6770245464928305e-05, 'epoch': 2.466666666666667}\n",
            "\n",
            ">>> Step 1814 metrics: {'loss': 0.2007, 'grad_norm': 6.40625, 'learning_rate': 4.6766739200738725e-05, 'epoch': 2.4680272108843537}\n",
            "\n",
            ">>> Step 1815 metrics: {'loss': 0.4246, 'grad_norm': 7.09375, 'learning_rate': 4.676323116593424e-05, 'epoch': 2.4693877551020407}\n",
            "\n",
            ">>> Step 1816 metrics: {'loss': 0.7192, 'grad_norm': 8.5625, 'learning_rate': 4.6759721360800226e-05, 'epoch': 2.470748299319728}\n",
            "\n",
            ">>> Step 1817 metrics: {'loss': 0.3249, 'grad_norm': 6.0625, 'learning_rate': 4.675620978562218e-05, 'epoch': 2.472108843537415}\n",
            "\n",
            ">>> Step 1818 metrics: {'loss': 0.7045, 'grad_norm': 12.9375, 'learning_rate': 4.675269644068575e-05, 'epoch': 2.473469387755102}\n",
            "\n",
            ">>> Step 1819 metrics: {'loss': 0.4984, 'grad_norm': 6.25, 'learning_rate': 4.674918132627675e-05, 'epoch': 2.4748299319727893}\n",
            "\n",
            ">>> Step 1820 metrics: {'loss': 0.7045, 'grad_norm': 7.15625, 'learning_rate': 4.674566444268108e-05, 'epoch': 2.4761904761904763}\n",
            "\n",
            ">>> Step 1821 metrics: {'loss': 0.2594, 'grad_norm': 5.96875, 'learning_rate': 4.674214579018486e-05, 'epoch': 2.477551020408163}\n",
            "\n",
            ">>> Step 1822 metrics: {'loss': 0.4769, 'grad_norm': 6.375, 'learning_rate': 4.673862536907427e-05, 'epoch': 2.47891156462585}\n",
            "\n",
            ">>> Step 1823 metrics: {'loss': 0.274, 'grad_norm': 6.5625, 'learning_rate': 4.673510317963573e-05, 'epoch': 2.4802721088435375}\n",
            "\n",
            ">>> Step 1824 metrics: {'loss': 0.3646, 'grad_norm': 6.09375, 'learning_rate': 4.6731579222155716e-05, 'epoch': 2.4816326530612245}\n",
            "\n",
            ">>> Step 1825 metrics: {'loss': 0.4574, 'grad_norm': 6.5, 'learning_rate': 4.672805349692089e-05, 'epoch': 2.4829931972789114}\n",
            "\n",
            ">>> Step 1826 metrics: {'loss': 0.371, 'grad_norm': 6.0625, 'learning_rate': 4.672452600421807e-05, 'epoch': 2.484353741496599}\n",
            "\n",
            ">>> Step 1827 metrics: {'loss': 0.4137, 'grad_norm': 6.5625, 'learning_rate': 4.672099674433418e-05, 'epoch': 2.4857142857142858}\n",
            "\n",
            ">>> Step 1828 metrics: {'loss': 0.3287, 'grad_norm': 5.84375, 'learning_rate': 4.671746571755631e-05, 'epoch': 2.4870748299319727}\n",
            "\n",
            ">>> Step 1829 metrics: {'loss': 0.5925, 'grad_norm': 7.15625, 'learning_rate': 4.67139329241717e-05, 'epoch': 2.4884353741496597}\n",
            "\n",
            ">>> Step 1830 metrics: {'loss': 0.3561, 'grad_norm': 6.46875, 'learning_rate': 4.671039836446772e-05, 'epoch': 2.489795918367347}\n",
            "\n",
            ">>> Step 1831 metrics: {'loss': 0.4783, 'grad_norm': 8.6875, 'learning_rate': 4.670686203873189e-05, 'epoch': 2.491156462585034}\n",
            "\n",
            ">>> Step 1832 metrics: {'loss': 0.364, 'grad_norm': 7.96875, 'learning_rate': 4.670332394725187e-05, 'epoch': 2.492517006802721}\n",
            "\n",
            ">>> Step 1833 metrics: {'loss': 0.5537, 'grad_norm': 7.0625, 'learning_rate': 4.669978409031547e-05, 'epoch': 2.4938775510204083}\n",
            "\n",
            ">>> Step 1834 metrics: {'loss': 0.3217, 'grad_norm': 6.96875, 'learning_rate': 4.669624246821064e-05, 'epoch': 2.4952380952380953}\n",
            "\n",
            ">>> Step 1835 metrics: {'loss': 0.3728, 'grad_norm': 7.34375, 'learning_rate': 4.6692699081225464e-05, 'epoch': 2.496598639455782}\n",
            "\n",
            ">>> Step 1836 metrics: {'loss': 0.4943, 'grad_norm': 7.0625, 'learning_rate': 4.668915392964819e-05, 'epoch': 2.497959183673469}\n",
            "\n",
            ">>> Step 1837 metrics: {'loss': 0.3161, 'grad_norm': 5.90625, 'learning_rate': 4.6685607013767184e-05, 'epoch': 2.4993197278911565}\n",
            "\n",
            ">>> Step 1838 metrics: {'loss': 0.2787, 'grad_norm': 5.6875, 'learning_rate': 4.6682058333870996e-05, 'epoch': 2.5006802721088435}\n",
            "\n",
            ">>> Step 1839 metrics: {'loss': 0.3844, 'grad_norm': 6.3125, 'learning_rate': 4.667850789024826e-05, 'epoch': 2.502040816326531}\n",
            "\n",
            ">>> Step 1840 metrics: {'loss': 0.8389, 'grad_norm': 7.625, 'learning_rate': 4.667495568318781e-05, 'epoch': 2.503401360544218}\n",
            "\n",
            ">>> Step 1841 metrics: {'loss': 0.6525, 'grad_norm': 7.28125, 'learning_rate': 4.667140171297859e-05, 'epoch': 2.5047619047619047}\n",
            "\n",
            ">>> Step 1842 metrics: {'loss': 0.665, 'grad_norm': 6.5625, 'learning_rate': 4.6667845979909706e-05, 'epoch': 2.5061224489795917}\n",
            "\n",
            ">>> Step 1843 metrics: {'loss': 1.1779, 'grad_norm': 10.6875, 'learning_rate': 4.666428848427039e-05, 'epoch': 2.507482993197279}\n",
            "\n",
            ">>> Step 1844 metrics: {'loss': 0.28, 'grad_norm': 5.84375, 'learning_rate': 4.6660729226350044e-05, 'epoch': 2.508843537414966}\n",
            "\n",
            ">>> Step 1845 metrics: {'loss': 0.4447, 'grad_norm': 8.625, 'learning_rate': 4.665716820643816e-05, 'epoch': 2.510204081632653}\n",
            "\n",
            ">>> Step 1846 metrics: {'loss': 0.5252, 'grad_norm': 7.375, 'learning_rate': 4.6653605424824445e-05, 'epoch': 2.5115646258503403}\n",
            "\n",
            ">>> Step 1847 metrics: {'loss': 0.2382, 'grad_norm': 5.8125, 'learning_rate': 4.66500408817987e-05, 'epoch': 2.5129251700680273}\n",
            "\n",
            ">>> Step 1848 metrics: {'loss': 0.5745, 'grad_norm': 7.25, 'learning_rate': 4.664647457765088e-05, 'epoch': 2.5142857142857142}\n",
            "\n",
            ">>> Step 1849 metrics: {'loss': 0.3169, 'grad_norm': 6.65625, 'learning_rate': 4.664290651267109e-05, 'epoch': 2.515646258503401}\n",
            "\n",
            ">>> Step 1850 metrics: {'loss': 0.283, 'grad_norm': 5.625, 'learning_rate': 4.6639336687149574e-05, 'epoch': 2.5170068027210886}\n",
            "\n",
            ">>> Step 1851 metrics: {'loss': 0.4532, 'grad_norm': 7.125, 'learning_rate': 4.663576510137671e-05, 'epoch': 2.5183673469387755}\n",
            "\n",
            ">>> Step 1852 metrics: {'loss': 0.3062, 'grad_norm': 7.5625, 'learning_rate': 4.663219175564304e-05, 'epoch': 2.5197278911564625}\n",
            "\n",
            ">>> Step 1853 metrics: {'loss': 0.484, 'grad_norm': 6.65625, 'learning_rate': 4.6628616650239235e-05, 'epoch': 2.52108843537415}\n",
            "\n",
            ">>> Step 1854 metrics: {'loss': 0.5543, 'grad_norm': 6.5, 'learning_rate': 4.6625039785456113e-05, 'epoch': 2.522448979591837}\n",
            "\n",
            ">>> Step 1855 metrics: {'loss': 0.4756, 'grad_norm': 7.09375, 'learning_rate': 4.662146116158463e-05, 'epoch': 2.5238095238095237}\n",
            "\n",
            ">>> Step 1856 metrics: {'loss': 0.2428, 'grad_norm': 5.1875, 'learning_rate': 4.661788077891589e-05, 'epoch': 2.5251700680272107}\n",
            "\n",
            ">>> Step 1857 metrics: {'loss': 0.7594, 'grad_norm': 8.0, 'learning_rate': 4.661429863774114e-05, 'epoch': 2.526530612244898}\n",
            "\n",
            ">>> Step 1858 metrics: {'loss': 0.2438, 'grad_norm': 5.75, 'learning_rate': 4.661071473835177e-05, 'epoch': 2.527891156462585}\n",
            "\n",
            ">>> Step 1859 metrics: {'loss': 0.7523, 'grad_norm': 7.71875, 'learning_rate': 4.660712908103931e-05, 'epoch': 2.5292517006802724}\n",
            "\n",
            ">>> Step 1860 metrics: {'loss': 0.3718, 'grad_norm': 6.03125, 'learning_rate': 4.660354166609545e-05, 'epoch': 2.5306122448979593}\n",
            "\n",
            ">>> Step 1861 metrics: {'loss': 0.3464, 'grad_norm': 6.0, 'learning_rate': 4.6599952493811985e-05, 'epoch': 2.5319727891156463}\n",
            "\n",
            ">>> Step 1862 metrics: {'loss': 0.3119, 'grad_norm': 5.8125, 'learning_rate': 4.659636156448088e-05, 'epoch': 2.533333333333333}\n",
            "\n",
            ">>> Step 1863 metrics: {'loss': 0.2488, 'grad_norm': 5.71875, 'learning_rate': 4.659276887839425e-05, 'epoch': 2.53469387755102}\n",
            "\n",
            ">>> Step 1864 metrics: {'loss': 0.2423, 'grad_norm': 6.125, 'learning_rate': 4.658917443584434e-05, 'epoch': 2.5360544217687075}\n",
            "\n",
            ">>> Step 1865 metrics: {'loss': 0.4499, 'grad_norm': 7.25, 'learning_rate': 4.6585578237123536e-05, 'epoch': 2.5374149659863945}\n",
            "\n",
            ">>> Step 1866 metrics: {'loss': 0.2154, 'grad_norm': 4.875, 'learning_rate': 4.658198028252437e-05, 'epoch': 2.538775510204082}\n",
            "\n",
            ">>> Step 1867 metrics: {'loss': 0.5426, 'grad_norm': 7.1875, 'learning_rate': 4.657838057233952e-05, 'epoch': 2.540136054421769}\n",
            "\n",
            ">>> Step 1868 metrics: {'loss': 0.3963, 'grad_norm': 6.34375, 'learning_rate': 4.65747791068618e-05, 'epoch': 2.5414965986394558}\n",
            "\n",
            ">>> Step 1869 metrics: {'loss': 0.6527, 'grad_norm': 10.8125, 'learning_rate': 4.657117588638418e-05, 'epoch': 2.5428571428571427}\n",
            "\n",
            ">>> Step 1870 metrics: {'loss': 0.7307, 'grad_norm': 9.125, 'learning_rate': 4.6567570911199754e-05, 'epoch': 2.54421768707483}\n",
            "\n",
            ">>> Step 1871 metrics: {'loss': 0.5842, 'grad_norm': 7.3125, 'learning_rate': 4.6563964181601775e-05, 'epoch': 2.545578231292517}\n",
            "\n",
            ">>> Step 1872 metrics: {'loss': 0.3793, 'grad_norm': 6.46875, 'learning_rate': 4.656035569788363e-05, 'epoch': 2.546938775510204}\n",
            "\n",
            ">>> Step 1873 metrics: {'loss': 0.4662, 'grad_norm': 5.875, 'learning_rate': 4.655674546033886e-05, 'epoch': 2.5482993197278914}\n",
            "\n",
            ">>> Step 1874 metrics: {'loss': 0.911, 'grad_norm': 9.875, 'learning_rate': 4.655313346926112e-05, 'epoch': 2.5496598639455783}\n",
            "\n",
            ">>> Step 1875 metrics: {'loss': 1.0733, 'grad_norm': 9.0, 'learning_rate': 4.6549519724944245e-05, 'epoch': 2.5510204081632653}\n",
            "\n",
            ">>> Step 1876 metrics: {'loss': 0.8607, 'grad_norm': 8.75, 'learning_rate': 4.654590422768219e-05, 'epoch': 2.552380952380952}\n",
            "\n",
            ">>> Step 1877 metrics: {'loss': 0.3534, 'grad_norm': 6.6875, 'learning_rate': 4.654228697776905e-05, 'epoch': 2.5537414965986396}\n",
            "\n",
            ">>> Step 1878 metrics: {'loss': 0.7494, 'grad_norm': 7.84375, 'learning_rate': 4.653866797549908e-05, 'epoch': 2.5551020408163265}\n",
            "\n",
            ">>> Step 1879 metrics: {'loss': 0.7851, 'grad_norm': 7.375, 'learning_rate': 4.653504722116666e-05, 'epoch': 2.5564625850340135}\n",
            "\n",
            ">>> Step 1880 metrics: {'loss': 0.6384, 'grad_norm': 5.84375, 'learning_rate': 4.653142471506633e-05, 'epoch': 2.557823129251701}\n",
            "\n",
            ">>> Step 1881 metrics: {'loss': 0.6732, 'grad_norm': 7.84375, 'learning_rate': 4.652780045749275e-05, 'epoch': 2.559183673469388}\n",
            "\n",
            ">>> Step 1882 metrics: {'loss': 0.2327, 'grad_norm': 5.09375, 'learning_rate': 4.652417444874075e-05, 'epoch': 2.5605442176870747}\n",
            "\n",
            ">>> Step 1883 metrics: {'loss': 0.2746, 'grad_norm': 5.375, 'learning_rate': 4.652054668910527e-05, 'epoch': 2.5619047619047617}\n",
            "\n",
            ">>> Step 1884 metrics: {'loss': 0.3219, 'grad_norm': 5.5625, 'learning_rate': 4.651691717888143e-05, 'epoch': 2.563265306122449}\n",
            "\n",
            ">>> Step 1885 metrics: {'loss': 0.3753, 'grad_norm': 6.125, 'learning_rate': 4.651328591836445e-05, 'epoch': 2.564625850340136}\n",
            "\n",
            ">>> Step 1886 metrics: {'loss': 0.9455, 'grad_norm': 7.1875, 'learning_rate': 4.650965290784973e-05, 'epoch': 2.5659863945578234}\n",
            "\n",
            ">>> Step 1887 metrics: {'loss': 0.2969, 'grad_norm': 6.09375, 'learning_rate': 4.65060181476328e-05, 'epoch': 2.5673469387755103}\n",
            "\n",
            ">>> Step 1888 metrics: {'loss': 0.2452, 'grad_norm': 5.875, 'learning_rate': 4.6502381638009324e-05, 'epoch': 2.5687074829931973}\n",
            "\n",
            ">>> Step 1889 metrics: {'loss': 0.5982, 'grad_norm': 7.28125, 'learning_rate': 4.64987433792751e-05, 'epoch': 2.5700680272108842}\n",
            "\n",
            ">>> Step 1890 metrics: {'loss': 0.2168, 'grad_norm': 5.3125, 'learning_rate': 4.649510337172611e-05, 'epoch': 2.571428571428571}\n",
            "\n",
            ">>> Step 1891 metrics: {'loss': 0.5508, 'grad_norm': 7.0625, 'learning_rate': 4.649146161565843e-05, 'epoch': 2.5727891156462586}\n",
            "\n",
            ">>> Step 1892 metrics: {'loss': 0.8108, 'grad_norm': 8.625, 'learning_rate': 4.648781811136831e-05, 'epoch': 2.5741496598639455}\n",
            "\n",
            ">>> Step 1893 metrics: {'loss': 0.5886, 'grad_norm': 8.375, 'learning_rate': 4.648417285915212e-05, 'epoch': 2.575510204081633}\n",
            "\n",
            ">>> Step 1894 metrics: {'loss': 0.4455, 'grad_norm': 7.0, 'learning_rate': 4.648052585930638e-05, 'epoch': 2.57687074829932}\n",
            "\n",
            ">>> Step 1895 metrics: {'loss': 0.5402, 'grad_norm': 7.59375, 'learning_rate': 4.647687711212777e-05, 'epoch': 2.578231292517007}\n",
            "\n",
            ">>> Step 1896 metrics: {'loss': 0.4404, 'grad_norm': 6.90625, 'learning_rate': 4.647322661791309e-05, 'epoch': 2.5795918367346937}\n",
            "\n",
            ">>> Step 1897 metrics: {'loss': 0.4656, 'grad_norm': 6.71875, 'learning_rate': 4.646957437695929e-05, 'epoch': 2.580952380952381}\n",
            "\n",
            ">>> Step 1898 metrics: {'loss': 0.6774, 'grad_norm': 7.5625, 'learning_rate': 4.646592038956346e-05, 'epoch': 2.582312925170068}\n",
            "\n",
            ">>> Step 1899 metrics: {'loss': 0.727, 'grad_norm': 9.0, 'learning_rate': 4.6462264656022836e-05, 'epoch': 2.583673469387755}\n",
            "\n",
            ">>> Step 1900 metrics: {'loss': 0.6065, 'grad_norm': 8.1875, 'learning_rate': 4.645860717663479e-05, 'epoch': 2.5850340136054424}\n",
            "\n",
            ">>> Step 1901 metrics: {'loss': 0.6114, 'grad_norm': 7.625, 'learning_rate': 4.645494795169685e-05, 'epoch': 2.5863945578231293}\n",
            "\n",
            ">>> Step 1902 metrics: {'loss': 0.2744, 'grad_norm': 6.125, 'learning_rate': 4.645128698150666e-05, 'epoch': 2.5877551020408163}\n",
            "\n",
            ">>> Step 1903 metrics: {'loss': 0.8752, 'grad_norm': 9.125, 'learning_rate': 4.6447624266362024e-05, 'epoch': 2.589115646258503}\n",
            "\n",
            ">>> Step 1904 metrics: {'loss': 0.4892, 'grad_norm': 5.6875, 'learning_rate': 4.64439598065609e-05, 'epoch': 2.5904761904761906}\n",
            "\n",
            ">>> Step 1905 metrics: {'loss': 0.4341, 'grad_norm': 7.21875, 'learning_rate': 4.644029360240135e-05, 'epoch': 2.5918367346938775}\n",
            "\n",
            ">>> Step 1906 metrics: {'loss': 0.4086, 'grad_norm': 6.15625, 'learning_rate': 4.6436625654181635e-05, 'epoch': 2.5931972789115645}\n",
            "\n",
            ">>> Step 1907 metrics: {'loss': 0.5844, 'grad_norm': 8.375, 'learning_rate': 4.6432955962200085e-05, 'epoch': 2.594557823129252}\n",
            "\n",
            ">>> Step 1908 metrics: {'loss': 0.4398, 'grad_norm': 6.15625, 'learning_rate': 4.642928452675524e-05, 'epoch': 2.595918367346939}\n",
            "\n",
            ">>> Step 1909 metrics: {'loss': 0.5857, 'grad_norm': 6.625, 'learning_rate': 4.642561134814574e-05, 'epoch': 2.5972789115646258}\n",
            "\n",
            ">>> Step 1910 metrics: {'loss': 0.4899, 'grad_norm': 5.5, 'learning_rate': 4.6421936426670386e-05, 'epoch': 2.5986394557823127}\n",
            "\n",
            ">>> Step 1911 metrics: {'loss': 0.2176, 'grad_norm': 6.8125, 'learning_rate': 4.641825976262811e-05, 'epoch': 2.6}\n",
            "\n",
            ">>> Step 1912 metrics: {'loss': 0.6194, 'grad_norm': 7.5625, 'learning_rate': 4.6414581356317985e-05, 'epoch': 2.601360544217687}\n",
            "\n",
            ">>> Step 1913 metrics: {'loss': 0.2577, 'grad_norm': 5.0, 'learning_rate': 4.641090120803925e-05, 'epoch': 2.6027210884353744}\n",
            "\n",
            ">>> Step 1914 metrics: {'loss': 0.224, 'grad_norm': 4.28125, 'learning_rate': 4.640721931809125e-05, 'epoch': 2.6040816326530614}\n",
            "\n",
            ">>> Step 1915 metrics: {'loss': 0.238, 'grad_norm': 5.1875, 'learning_rate': 4.640353568677348e-05, 'epoch': 2.6054421768707483}\n",
            "\n",
            ">>> Step 1916 metrics: {'loss': 0.6061, 'grad_norm': 7.0, 'learning_rate': 4.639985031438561e-05, 'epoch': 2.6068027210884352}\n",
            "\n",
            ">>> Step 1917 metrics: {'loss': 0.413, 'grad_norm': 7.1875, 'learning_rate': 4.63961632012274e-05, 'epoch': 2.608163265306122}\n",
            "\n",
            ">>> Step 1918 metrics: {'loss': 0.8362, 'grad_norm': 9.9375, 'learning_rate': 4.6392474347598803e-05, 'epoch': 2.6095238095238096}\n",
            "\n",
            ">>> Step 1919 metrics: {'loss': 0.2768, 'grad_norm': 6.0, 'learning_rate': 4.638878375379987e-05, 'epoch': 2.6108843537414965}\n",
            "\n",
            ">>> Step 1920 metrics: {'loss': 0.4703, 'grad_norm': 6.90625, 'learning_rate': 4.638509142013082e-05, 'epoch': 2.612244897959184}\n",
            "\n",
            ">>> Step 1921 metrics: {'loss': 0.3376, 'grad_norm': 6.96875, 'learning_rate': 4.638139734689201e-05, 'epoch': 2.613605442176871}\n",
            "\n",
            ">>> Step 1922 metrics: {'loss': 0.2818, 'grad_norm': 6.84375, 'learning_rate': 4.637770153438393e-05, 'epoch': 2.614965986394558}\n",
            "\n",
            ">>> Step 1923 metrics: {'loss': 0.9941, 'grad_norm': 9.5625, 'learning_rate': 4.637400398290721e-05, 'epoch': 2.6163265306122447}\n",
            "\n",
            ">>> Step 1924 metrics: {'loss': 0.6366, 'grad_norm': 8.375, 'learning_rate': 4.6370304692762636e-05, 'epoch': 2.617687074829932}\n",
            "\n",
            ">>> Step 1925 metrics: {'loss': 0.7395, 'grad_norm': 8.3125, 'learning_rate': 4.636660366425112e-05, 'epoch': 2.619047619047619}\n",
            "\n",
            ">>> Step 1926 metrics: {'loss': 0.4107, 'grad_norm': 7.21875, 'learning_rate': 4.636290089767373e-05, 'epoch': 2.620408163265306}\n",
            "\n",
            ">>> Step 1927 metrics: {'loss': 0.2667, 'grad_norm': 6.34375, 'learning_rate': 4.635919639333166e-05, 'epoch': 2.6217687074829934}\n",
            "\n",
            ">>> Step 1928 metrics: {'loss': 0.5159, 'grad_norm': 6.75, 'learning_rate': 4.635549015152625e-05, 'epoch': 2.6231292517006803}\n",
            "\n",
            ">>> Step 1929 metrics: {'loss': 0.2948, 'grad_norm': 6.75, 'learning_rate': 4.6351782172559e-05, 'epoch': 2.6244897959183673}\n",
            "\n",
            ">>> Step 1930 metrics: {'loss': 0.7293, 'grad_norm': 8.1875, 'learning_rate': 4.6348072456731516e-05, 'epoch': 2.6258503401360542}\n",
            "\n",
            ">>> Step 1931 metrics: {'loss': 0.5165, 'grad_norm': 6.5625, 'learning_rate': 4.634436100434558e-05, 'epoch': 2.6272108843537416}\n",
            "\n",
            ">>> Step 1932 metrics: {'loss': 0.6537, 'grad_norm': 8.4375, 'learning_rate': 4.63406478157031e-05, 'epoch': 2.6285714285714286}\n",
            "\n",
            ">>> Step 1933 metrics: {'loss': 0.7137, 'grad_norm': 7.375, 'learning_rate': 4.6336932891106105e-05, 'epoch': 2.6299319727891155}\n",
            "\n",
            ">>> Step 1934 metrics: {'loss': 0.6659, 'grad_norm': 6.03125, 'learning_rate': 4.6333216230856806e-05, 'epoch': 2.631292517006803}\n",
            "\n",
            ">>> Step 1935 metrics: {'loss': 0.8446, 'grad_norm': 7.125, 'learning_rate': 4.632949783525753e-05, 'epoch': 2.63265306122449}\n",
            "\n",
            ">>> Step 1936 metrics: {'loss': 0.472, 'grad_norm': 6.25, 'learning_rate': 4.632577770461075e-05, 'epoch': 2.6340136054421768}\n",
            "\n",
            ">>> Step 1937 metrics: {'loss': 0.4067, 'grad_norm': 6.5625, 'learning_rate': 4.632205583921907e-05, 'epoch': 2.6353741496598637}\n",
            "\n",
            ">>> Step 1938 metrics: {'loss': 0.4166, 'grad_norm': 5.875, 'learning_rate': 4.6318332239385254e-05, 'epoch': 2.636734693877551}\n",
            "\n",
            ">>> Step 1939 metrics: {'loss': 0.3198, 'grad_norm': 6.09375, 'learning_rate': 4.631460690541221e-05, 'epoch': 2.638095238095238}\n",
            "\n",
            ">>> Step 1940 metrics: {'loss': 0.5161, 'grad_norm': 6.6875, 'learning_rate': 4.631087983760295e-05, 'epoch': 2.6394557823129254}\n",
            "\n",
            ">>> Step 1941 metrics: {'loss': 0.828, 'grad_norm': 7.1875, 'learning_rate': 4.630715103626066e-05, 'epoch': 2.6408163265306124}\n",
            "\n",
            ">>> Step 1942 metrics: {'loss': 0.3545, 'grad_norm': 6.90625, 'learning_rate': 4.630342050168868e-05, 'epoch': 2.6421768707482993}\n",
            "\n",
            ">>> Step 1943 metrics: {'loss': 0.9317, 'grad_norm': 7.6875, 'learning_rate': 4.629968823419044e-05, 'epoch': 2.6435374149659863}\n",
            "\n",
            ">>> Step 1944 metrics: {'loss': 0.5423, 'grad_norm': 7.0, 'learning_rate': 4.629595423406956e-05, 'epoch': 2.644897959183673}\n",
            "\n",
            ">>> Step 1945 metrics: {'loss': 0.485, 'grad_norm': 7.34375, 'learning_rate': 4.629221850162978e-05, 'epoch': 2.6462585034013606}\n",
            "\n",
            ">>> Step 1946 metrics: {'loss': 0.3837, 'grad_norm': 6.625, 'learning_rate': 4.628848103717498e-05, 'epoch': 2.6476190476190475}\n",
            "\n",
            ">>> Step 1947 metrics: {'loss': 0.3888, 'grad_norm': 5.8125, 'learning_rate': 4.628474184100918e-05, 'epoch': 2.648979591836735}\n",
            "\n",
            ">>> Step 1948 metrics: {'loss': 0.452, 'grad_norm': 6.875, 'learning_rate': 4.6281000913436555e-05, 'epoch': 2.650340136054422}\n",
            "\n",
            ">>> Step 1949 metrics: {'loss': 0.6896, 'grad_norm': 6.78125, 'learning_rate': 4.6277258254761405e-05, 'epoch': 2.651700680272109}\n",
            "\n",
            ">>> Step 1950 metrics: {'loss': 0.5051, 'grad_norm': 6.5, 'learning_rate': 4.627351386528817e-05, 'epoch': 2.6530612244897958}\n",
            "\n",
            ">>> Step 1951 metrics: {'loss': 0.3322, 'grad_norm': 6.0625, 'learning_rate': 4.626976774532146e-05, 'epoch': 2.654421768707483}\n",
            "\n",
            ">>> Step 1952 metrics: {'loss': 0.3254, 'grad_norm': 6.4375, 'learning_rate': 4.6266019895165966e-05, 'epoch': 2.65578231292517}\n",
            "\n",
            ">>> Step 1953 metrics: {'loss': 0.5243, 'grad_norm': 6.6875, 'learning_rate': 4.626227031512659e-05, 'epoch': 2.657142857142857}\n",
            "\n",
            ">>> Step 1954 metrics: {'loss': 0.4649, 'grad_norm': 6.625, 'learning_rate': 4.6258519005508326e-05, 'epoch': 2.6585034013605444}\n",
            "\n",
            ">>> Step 1955 metrics: {'loss': 0.5871, 'grad_norm': 6.875, 'learning_rate': 4.6254765966616334e-05, 'epoch': 2.6598639455782314}\n",
            "\n",
            ">>> Step 1956 metrics: {'loss': 0.5269, 'grad_norm': 6.84375, 'learning_rate': 4.6251011198755886e-05, 'epoch': 2.6612244897959183}\n",
            "\n",
            ">>> Step 1957 metrics: {'loss': 0.4257, 'grad_norm': 6.25, 'learning_rate': 4.6247254702232436e-05, 'epoch': 2.6625850340136052}\n",
            "\n",
            ">>> Step 1958 metrics: {'loss': 0.6509, 'grad_norm': 6.46875, 'learning_rate': 4.624349647735153e-05, 'epoch': 2.6639455782312926}\n",
            "\n",
            ">>> Step 1959 metrics: {'loss': 0.4123, 'grad_norm': 7.40625, 'learning_rate': 4.623973652441892e-05, 'epoch': 2.6653061224489796}\n",
            "\n",
            ">>> Step 1960 metrics: {'loss': 0.8533, 'grad_norm': 9.5625, 'learning_rate': 4.623597484374041e-05, 'epoch': 2.6666666666666665}\n",
            "\n",
            ">>> Step 1961 metrics: {'loss': 0.2535, 'grad_norm': 5.75, 'learning_rate': 4.623221143562204e-05, 'epoch': 2.668027210884354}\n",
            "\n",
            ">>> Step 1962 metrics: {'loss': 0.592, 'grad_norm': 7.375, 'learning_rate': 4.622844630036992e-05, 'epoch': 2.669387755102041}\n",
            "\n",
            ">>> Step 1963 metrics: {'loss': 0.4363, 'grad_norm': 7.03125, 'learning_rate': 4.622467943829033e-05, 'epoch': 2.670748299319728}\n",
            "\n",
            ">>> Step 1964 metrics: {'loss': 0.4388, 'grad_norm': 6.4375, 'learning_rate': 4.6220910849689673e-05, 'epoch': 2.6721088435374147}\n",
            "\n",
            ">>> Step 1965 metrics: {'loss': 0.5315, 'grad_norm': 7.40625, 'learning_rate': 4.6217140534874524e-05, 'epoch': 2.673469387755102}\n",
            "\n",
            ">>> Step 1966 metrics: {'loss': 0.7198, 'grad_norm': 7.84375, 'learning_rate': 4.6213368494151574e-05, 'epoch': 2.674829931972789}\n",
            "\n",
            ">>> Step 1967 metrics: {'loss': 0.8331, 'grad_norm': 7.875, 'learning_rate': 4.620959472782765e-05, 'epoch': 2.6761904761904765}\n",
            "\n",
            ">>> Step 1968 metrics: {'loss': 0.3304, 'grad_norm': 5.5, 'learning_rate': 4.6205819236209736e-05, 'epoch': 2.6775510204081634}\n",
            "\n",
            ">>> Step 1969 metrics: {'loss': 0.4207, 'grad_norm': 6.09375, 'learning_rate': 4.6202042019604954e-05, 'epoch': 2.6789115646258503}\n",
            "\n",
            ">>> Step 1970 metrics: {'loss': 0.7274, 'grad_norm': 9.0, 'learning_rate': 4.6198263078320556e-05, 'epoch': 2.6802721088435373}\n",
            "\n",
            ">>> Step 1971 metrics: {'loss': 0.3419, 'grad_norm': 6.21875, 'learning_rate': 4.619448241266393e-05, 'epoch': 2.6816326530612242}\n",
            "\n",
            ">>> Step 1972 metrics: {'loss': 0.5358, 'grad_norm': 6.78125, 'learning_rate': 4.619070002294264e-05, 'epoch': 2.6829931972789116}\n",
            "\n",
            ">>> Step 1973 metrics: {'loss': 0.3256, 'grad_norm': 5.8125, 'learning_rate': 4.6186915909464335e-05, 'epoch': 2.6843537414965986}\n",
            "\n",
            ">>> Step 1974 metrics: {'loss': 0.3329, 'grad_norm': 6.40625, 'learning_rate': 4.6183130072536856e-05, 'epoch': 2.685714285714286}\n",
            "\n",
            ">>> Step 1975 metrics: {'loss': 0.4666, 'grad_norm': 8.1875, 'learning_rate': 4.617934251246815e-05, 'epoch': 2.687074829931973}\n",
            "\n",
            ">>> Step 1976 metrics: {'loss': 0.6683, 'grad_norm': 6.5625, 'learning_rate': 4.617555322956632e-05, 'epoch': 2.68843537414966}\n",
            "\n",
            ">>> Step 1977 metrics: {'loss': 0.5898, 'grad_norm': 7.40625, 'learning_rate': 4.61717622241396e-05, 'epoch': 2.6897959183673468}\n",
            "\n",
            ">>> Step 1978 metrics: {'loss': 0.3647, 'grad_norm': 7.5, 'learning_rate': 4.616796949649638e-05, 'epoch': 2.691156462585034}\n",
            "\n",
            ">>> Step 1979 metrics: {'loss': 0.7656, 'grad_norm': 7.125, 'learning_rate': 4.616417504694518e-05, 'epoch': 2.692517006802721}\n",
            "\n",
            ">>> Step 1980 metrics: {'loss': 0.2203, 'grad_norm': 5.21875, 'learning_rate': 4.616037887579464e-05, 'epoch': 2.693877551020408}\n",
            "\n",
            ">>> Step 1981 metrics: {'loss': 0.4138, 'grad_norm': 7.1875, 'learning_rate': 4.615658098335357e-05, 'epoch': 2.6952380952380954}\n",
            "\n",
            ">>> Step 1982 metrics: {'loss': 0.2333, 'grad_norm': 5.8125, 'learning_rate': 4.615278136993092e-05, 'epoch': 2.6965986394557824}\n",
            "\n",
            ">>> Step 1983 metrics: {'loss': 0.3997, 'grad_norm': 6.5625, 'learning_rate': 4.614898003583576e-05, 'epoch': 2.6979591836734693}\n",
            "\n",
            ">>> Step 1984 metrics: {'loss': 0.5569, 'grad_norm': 6.90625, 'learning_rate': 4.614517698137731e-05, 'epoch': 2.6993197278911563}\n",
            "\n",
            ">>> Step 1985 metrics: {'loss': 0.5574, 'grad_norm': 7.0625, 'learning_rate': 4.614137220686493e-05, 'epoch': 2.7006802721088436}\n",
            "\n",
            ">>> Step 1986 metrics: {'loss': 0.6664, 'grad_norm': 6.28125, 'learning_rate': 4.613756571260811e-05, 'epoch': 2.7020408163265306}\n",
            "\n",
            ">>> Step 1987 metrics: {'loss': 0.4019, 'grad_norm': 6.3125, 'learning_rate': 4.6133757498916505e-05, 'epoch': 2.7034013605442175}\n",
            "\n",
            ">>> Step 1988 metrics: {'loss': 0.822, 'grad_norm': 7.46875, 'learning_rate': 4.612994756609989e-05, 'epoch': 2.704761904761905}\n",
            "\n",
            ">>> Step 1989 metrics: {'loss': 0.5775, 'grad_norm': 7.0625, 'learning_rate': 4.612613591446818e-05, 'epoch': 2.706122448979592}\n",
            "\n",
            ">>> Step 1990 metrics: {'loss': 0.4615, 'grad_norm': 6.46875, 'learning_rate': 4.612232254433143e-05, 'epoch': 2.707482993197279}\n",
            "\n",
            ">>> Step 1991 metrics: {'loss': 0.3772, 'grad_norm': 6.53125, 'learning_rate': 4.611850745599985e-05, 'epoch': 2.7088435374149658}\n",
            "\n",
            ">>> Step 1992 metrics: {'loss': 0.7254, 'grad_norm': 6.9375, 'learning_rate': 4.611469064978377e-05, 'epoch': 2.710204081632653}\n",
            "\n",
            ">>> Step 1993 metrics: {'loss': 0.3355, 'grad_norm': 5.90625, 'learning_rate': 4.611087212599367e-05, 'epoch': 2.71156462585034}\n",
            "\n",
            ">>> Step 1994 metrics: {'loss': 1.0191, 'grad_norm': 10.375, 'learning_rate': 4.6107051884940156e-05, 'epoch': 2.7129251700680275}\n",
            "\n",
            ">>> Step 1995 metrics: {'loss': 0.2069, 'grad_norm': 4.5625, 'learning_rate': 4.610322992693401e-05, 'epoch': 2.7142857142857144}\n",
            "\n",
            ">>> Step 1996 metrics: {'loss': 0.4425, 'grad_norm': 6.71875, 'learning_rate': 4.609940625228611e-05, 'epoch': 2.7156462585034014}\n",
            "\n",
            ">>> Step 1997 metrics: {'loss': 0.6077, 'grad_norm': 7.375, 'learning_rate': 4.60955808613075e-05, 'epoch': 2.7170068027210883}\n",
            "\n",
            ">>> Step 1998 metrics: {'loss': 0.5412, 'grad_norm': 7.625, 'learning_rate': 4.609175375430935e-05, 'epoch': 2.7183673469387752}\n",
            "\n",
            ">>> Step 1999 metrics: {'loss': 0.4765, 'grad_norm': 6.40625, 'learning_rate': 4.608792493160299e-05, 'epoch': 2.7197278911564626}\n",
            "\n",
            ">>> Step 2000 metrics: {'loss': 0.3315, 'grad_norm': 6.125, 'learning_rate': 4.608409439349986e-05, 'epoch': 2.7210884353741496}\n",
            "\n",
            ">>> Step 2001 metrics: {'loss': 0.3892, 'grad_norm': 6.15625, 'learning_rate': 4.6080262140311566e-05, 'epoch': 2.722448979591837}\n",
            "\n",
            ">>> Step 2002 metrics: {'loss': 0.7342, 'grad_norm': 8.0625, 'learning_rate': 4.607642817234983e-05, 'epoch': 2.723809523809524}\n",
            "\n",
            ">>> Step 2003 metrics: {'loss': 1.022, 'grad_norm': 11.875, 'learning_rate': 4.607259248992654e-05, 'epoch': 2.725170068027211}\n",
            "\n",
            ">>> Step 2004 metrics: {'loss': 0.2271, 'grad_norm': 5.625, 'learning_rate': 4.60687550933537e-05, 'epoch': 2.726530612244898}\n",
            "\n",
            ">>> Step 2005 metrics: {'loss': 0.1099, 'grad_norm': 4.78125, 'learning_rate': 4.606491598294347e-05, 'epoch': 2.727891156462585}\n",
            "\n",
            ">>> Step 2006 metrics: {'loss': 0.3448, 'grad_norm': 5.5625, 'learning_rate': 4.606107515900814e-05, 'epoch': 2.729251700680272}\n",
            "\n",
            ">>> Step 2007 metrics: {'loss': 0.3829, 'grad_norm': 6.15625, 'learning_rate': 4.605723262186014e-05, 'epoch': 2.730612244897959}\n",
            "\n",
            ">>> Step 2008 metrics: {'loss': 0.434, 'grad_norm': 8.4375, 'learning_rate': 4.605338837181203e-05, 'epoch': 2.7319727891156464}\n",
            "\n",
            ">>> Step 2009 metrics: {'loss': 0.2727, 'grad_norm': 6.625, 'learning_rate': 4.6049542409176546e-05, 'epoch': 2.7333333333333334}\n",
            "\n",
            ">>> Step 2010 metrics: {'loss': 0.3134, 'grad_norm': 5.75, 'learning_rate': 4.604569473426652e-05, 'epoch': 2.7346938775510203}\n",
            "\n",
            ">>> Step 2011 metrics: {'loss': 0.885, 'grad_norm': 15.375, 'learning_rate': 4.6041845347394944e-05, 'epoch': 2.7360544217687073}\n",
            "\n",
            ">>> Step 2012 metrics: {'loss': 0.4673, 'grad_norm': 5.6875, 'learning_rate': 4.603799424887495e-05, 'epoch': 2.7374149659863947}\n",
            "\n",
            ">>> Step 2013 metrics: {'loss': 0.2684, 'grad_norm': 8.375, 'learning_rate': 4.60341414390198e-05, 'epoch': 2.7387755102040816}\n",
            "\n",
            ">>> Step 2014 metrics: {'loss': 0.6473, 'grad_norm': 8.25, 'learning_rate': 4.6030286918142895e-05, 'epoch': 2.7401360544217686}\n",
            "\n",
            ">>> Step 2015 metrics: {'loss': 0.2983, 'grad_norm': 5.9375, 'learning_rate': 4.60264306865578e-05, 'epoch': 2.741496598639456}\n",
            "\n",
            ">>> Step 2016 metrics: {'loss': 0.4119, 'grad_norm': 6.0625, 'learning_rate': 4.6022572744578185e-05, 'epoch': 2.742857142857143}\n",
            "\n",
            ">>> Step 2017 metrics: {'loss': 0.8095, 'grad_norm': 8.3125, 'learning_rate': 4.601871309251788e-05, 'epoch': 2.74421768707483}\n",
            "\n",
            ">>> Step 2018 metrics: {'loss': 0.4918, 'grad_norm': 7.1875, 'learning_rate': 4.601485173069084e-05, 'epoch': 2.7455782312925168}\n",
            "\n",
            ">>> Step 2019 metrics: {'loss': 0.6054, 'grad_norm': 7.9375, 'learning_rate': 4.601098865941118e-05, 'epoch': 2.746938775510204}\n",
            "\n",
            ">>> Step 2020 metrics: {'loss': 0.6749, 'grad_norm': 7.34375, 'learning_rate': 4.600712387899314e-05, 'epoch': 2.748299319727891}\n",
            "\n",
            ">>> Step 2021 metrics: {'loss': 0.3919, 'grad_norm': 6.03125, 'learning_rate': 4.60032573897511e-05, 'epoch': 2.7496598639455785}\n",
            "\n",
            ">>> Step 2022 metrics: {'loss': 0.4176, 'grad_norm': 6.5, 'learning_rate': 4.599938919199956e-05, 'epoch': 2.7510204081632654}\n",
            "\n",
            ">>> Step 2023 metrics: {'loss': 0.3985, 'grad_norm': 7.40625, 'learning_rate': 4.59955192860532e-05, 'epoch': 2.7523809523809524}\n",
            "\n",
            ">>> Step 2024 metrics: {'loss': 0.4362, 'grad_norm': 6.625, 'learning_rate': 4.599164767222682e-05, 'epoch': 2.7537414965986393}\n",
            "\n",
            ">>> Step 2025 metrics: {'loss': 0.3972, 'grad_norm': 6.15625, 'learning_rate': 4.598777435083533e-05, 'epoch': 2.7551020408163263}\n",
            "\n",
            ">>> Step 2026 metrics: {'loss': 0.4707, 'grad_norm': 6.0625, 'learning_rate': 4.598389932219384e-05, 'epoch': 2.7564625850340136}\n",
            "\n",
            ">>> Step 2027 metrics: {'loss': 0.5666, 'grad_norm': 6.09375, 'learning_rate': 4.5980022586617544e-05, 'epoch': 2.7578231292517006}\n",
            "\n",
            ">>> Step 2028 metrics: {'loss': 0.1814, 'grad_norm': 4.125, 'learning_rate': 4.597614414442179e-05, 'epoch': 2.759183673469388}\n",
            "\n",
            ">>> Step 2029 metrics: {'loss': 0.7804, 'grad_norm': 7.125, 'learning_rate': 4.597226399592208e-05, 'epoch': 2.760544217687075}\n",
            "\n",
            ">>> Step 2030 metrics: {'loss': 0.4164, 'grad_norm': 6.03125, 'learning_rate': 4.596838214143405e-05, 'epoch': 2.761904761904762}\n",
            "\n",
            ">>> Step 2031 metrics: {'loss': 0.7148, 'grad_norm': 8.6875, 'learning_rate': 4.596449858127345e-05, 'epoch': 2.763265306122449}\n",
            "\n",
            ">>> Step 2032 metrics: {'loss': 0.7609, 'grad_norm': 7.65625, 'learning_rate': 4.596061331575621e-05, 'epoch': 2.764625850340136}\n",
            "\n",
            ">>> Step 2033 metrics: {'loss': 0.4096, 'grad_norm': 7.09375, 'learning_rate': 4.595672634519836e-05, 'epoch': 2.765986394557823}\n",
            "\n",
            ">>> Step 2034 metrics: {'loss': 0.4644, 'grad_norm': 7.0625, 'learning_rate': 4.59528376699161e-05, 'epoch': 2.76734693877551}\n",
            "\n",
            ">>> Step 2035 metrics: {'loss': 0.8842, 'grad_norm': 7.34375, 'learning_rate': 4.594894729022574e-05, 'epoch': 2.7687074829931975}\n",
            "\n",
            ">>> Step 2036 metrics: {'loss': 0.2361, 'grad_norm': 6.375, 'learning_rate': 4.5945055206443754e-05, 'epoch': 2.7700680272108844}\n",
            "\n",
            ">>> Step 2037 metrics: {'loss': 0.6195, 'grad_norm': 7.34375, 'learning_rate': 4.5941161418886734e-05, 'epoch': 2.7714285714285714}\n",
            "\n",
            ">>> Step 2038 metrics: {'loss': 0.3648, 'grad_norm': 6.28125, 'learning_rate': 4.593726592787143e-05, 'epoch': 2.7727891156462583}\n",
            "\n",
            ">>> Step 2039 metrics: {'loss': 0.4189, 'grad_norm': 6.5625, 'learning_rate': 4.593336873371471e-05, 'epoch': 2.7741496598639457}\n",
            "\n",
            ">>> Step 2040 metrics: {'loss': 0.4986, 'grad_norm': 8.0625, 'learning_rate': 4.59294698367336e-05, 'epoch': 2.7755102040816326}\n",
            "\n",
            ">>> Step 2041 metrics: {'loss': 0.995, 'grad_norm': 7.8125, 'learning_rate': 4.592556923724525e-05, 'epoch': 2.7768707482993196}\n",
            "\n",
            ">>> Step 2042 metrics: {'loss': 0.3567, 'grad_norm': 6.21875, 'learning_rate': 4.592166693556696e-05, 'epoch': 2.778231292517007}\n",
            "\n",
            ">>> Step 2043 metrics: {'loss': 0.5551, 'grad_norm': 7.59375, 'learning_rate': 4.5917762932016155e-05, 'epoch': 2.779591836734694}\n",
            "\n",
            ">>> Step 2044 metrics: {'loss': 0.4567, 'grad_norm': 6.625, 'learning_rate': 4.591385722691041e-05, 'epoch': 2.780952380952381}\n",
            "\n",
            ">>> Step 2045 metrics: {'loss': 0.1584, 'grad_norm': 4.125, 'learning_rate': 4.5909949820567436e-05, 'epoch': 2.782312925170068}\n",
            "\n",
            ">>> Step 2046 metrics: {'loss': 0.3579, 'grad_norm': 7.25, 'learning_rate': 4.590604071330508e-05, 'epoch': 2.783673469387755}\n",
            "\n",
            ">>> Step 2047 metrics: {'loss': 0.618, 'grad_norm': 8.0, 'learning_rate': 4.5902129905441325e-05, 'epoch': 2.785034013605442}\n",
            "\n",
            ">>> Step 2048 metrics: {'loss': 0.3475, 'grad_norm': 6.0625, 'learning_rate': 4.5898217397294306e-05, 'epoch': 2.7863945578231295}\n",
            "\n",
            ">>> Step 2049 metrics: {'loss': 0.5511, 'grad_norm': 7.625, 'learning_rate': 4.5894303189182275e-05, 'epoch': 2.7877551020408164}\n",
            "\n",
            ">>> Step 2050 metrics: {'loss': 0.3546, 'grad_norm': 5.40625, 'learning_rate': 4.5890387281423635e-05, 'epoch': 2.7891156462585034}\n",
            "\n",
            ">>> Step 2051 metrics: {'loss': 0.4313, 'grad_norm': 5.65625, 'learning_rate': 4.5886469674336926e-05, 'epoch': 2.7904761904761903}\n",
            "\n",
            ">>> Step 2052 metrics: {'loss': 0.2245, 'grad_norm': 5.28125, 'learning_rate': 4.588255036824083e-05, 'epoch': 2.7918367346938773}\n",
            "\n",
            ">>> Step 2053 metrics: {'loss': 0.4791, 'grad_norm': 6.09375, 'learning_rate': 4.587862936345416e-05, 'epoch': 2.7931972789115647}\n",
            "\n",
            ">>> Step 2054 metrics: {'loss': 0.4484, 'grad_norm': 6.59375, 'learning_rate': 4.587470666029586e-05, 'epoch': 2.7945578231292516}\n",
            "\n",
            ">>> Step 2055 metrics: {'loss': 0.8727, 'grad_norm': 10.8125, 'learning_rate': 4.587078225908504e-05, 'epoch': 2.795918367346939}\n",
            "\n",
            ">>> Step 2056 metrics: {'loss': 1.0176, 'grad_norm': 8.8125, 'learning_rate': 4.586685616014092e-05, 'epoch': 2.797278911564626}\n",
            "\n",
            ">>> Step 2057 metrics: {'loss': 0.2567, 'grad_norm': 4.71875, 'learning_rate': 4.586292836378287e-05, 'epoch': 2.798639455782313}\n",
            "\n",
            ">>> Step 2058 metrics: {'loss': 0.2089, 'grad_norm': 4.75, 'learning_rate': 4.58589988703304e-05, 'epoch': 2.8}\n",
            "\n",
            ">>> Step 2059 metrics: {'loss': 0.4215, 'grad_norm': 6.3125, 'learning_rate': 4.585506768010315e-05, 'epoch': 2.801360544217687}\n",
            "\n",
            ">>> Step 2060 metrics: {'loss': 0.702, 'grad_norm': 6.375, 'learning_rate': 4.58511347934209e-05, 'epoch': 2.802721088435374}\n",
            "\n",
            ">>> Step 2061 metrics: {'loss': 0.3782, 'grad_norm': 5.9375, 'learning_rate': 4.584720021060358e-05, 'epoch': 2.804081632653061}\n",
            "\n",
            ">>> Step 2062 metrics: {'loss': 0.6067, 'grad_norm': 7.15625, 'learning_rate': 4.584326393197124e-05, 'epoch': 2.8054421768707485}\n",
            "\n",
            ">>> Step 2063 metrics: {'loss': 0.3903, 'grad_norm': 6.4375, 'learning_rate': 4.583932595784408e-05, 'epoch': 2.8068027210884354}\n",
            "\n",
            ">>> Step 2064 metrics: {'loss': 0.3393, 'grad_norm': 5.375, 'learning_rate': 4.583538628854244e-05, 'epoch': 2.8081632653061224}\n",
            "\n",
            ">>> Step 2065 metrics: {'loss': 0.6734, 'grad_norm': 7.78125, 'learning_rate': 4.583144492438677e-05, 'epoch': 2.8095238095238093}\n",
            "\n",
            ">>> Step 2066 metrics: {'loss': 0.7265, 'grad_norm': 7.25, 'learning_rate': 4.582750186569771e-05, 'epoch': 2.8108843537414967}\n",
            "\n",
            ">>> Step 2067 metrics: {'loss': 0.2979, 'grad_norm': 6.21875, 'learning_rate': 4.582355711279599e-05, 'epoch': 2.8122448979591836}\n",
            "\n",
            ">>> Step 2068 metrics: {'loss': 0.3044, 'grad_norm': 6.1875, 'learning_rate': 4.58196106660025e-05, 'epoch': 2.8136054421768706}\n",
            "\n",
            ">>> Step 2069 metrics: {'loss': 0.3381, 'grad_norm': 6.46875, 'learning_rate': 4.581566252563827e-05, 'epoch': 2.814965986394558}\n",
            "\n",
            ">>> Step 2070 metrics: {'loss': 0.5313, 'grad_norm': 7.34375, 'learning_rate': 4.581171269202445e-05, 'epoch': 2.816326530612245}\n",
            "\n",
            ">>> Step 2071 metrics: {'loss': 0.661, 'grad_norm': 6.75, 'learning_rate': 4.5807761165482346e-05, 'epoch': 2.817687074829932}\n",
            "\n",
            ">>> Step 2072 metrics: {'loss': 0.6701, 'grad_norm': 8.0625, 'learning_rate': 4.5803807946333394e-05, 'epoch': 2.819047619047619}\n",
            "\n",
            ">>> Step 2073 metrics: {'loss': 0.4809, 'grad_norm': 7.375, 'learning_rate': 4.579985303489917e-05, 'epoch': 2.820408163265306}\n",
            "\n",
            ">>> Step 2074 metrics: {'loss': 0.8683, 'grad_norm': 8.375, 'learning_rate': 4.579589643150138e-05, 'epoch': 2.821768707482993}\n",
            "\n",
            ">>> Step 2075 metrics: {'loss': 0.2005, 'grad_norm': 4.65625, 'learning_rate': 4.5791938136461875e-05, 'epoch': 2.8231292517006805}\n",
            "\n",
            ">>> Step 2076 metrics: {'loss': 0.3509, 'grad_norm': 6.59375, 'learning_rate': 4.578797815010265e-05, 'epoch': 2.8244897959183675}\n",
            "\n",
            ">>> Step 2077 metrics: {'loss': 0.9524, 'grad_norm': 8.25, 'learning_rate': 4.578401647274583e-05, 'epoch': 2.8258503401360544}\n",
            "\n",
            ">>> Step 2078 metrics: {'loss': 0.4341, 'grad_norm': 7.71875, 'learning_rate': 4.578005310471366e-05, 'epoch': 2.8272108843537413}\n",
            "\n",
            ">>> Step 2079 metrics: {'loss': 0.5282, 'grad_norm': 6.5, 'learning_rate': 4.5776088046328566e-05, 'epoch': 2.8285714285714287}\n",
            "\n",
            ">>> Step 2080 metrics: {'loss': 0.8368, 'grad_norm': 7.1875, 'learning_rate': 4.577212129791306e-05, 'epoch': 2.8299319727891157}\n",
            "\n",
            ">>> Step 2081 metrics: {'loss': 0.6245, 'grad_norm': 7.46875, 'learning_rate': 4.5768152859789835e-05, 'epoch': 2.8312925170068026}\n",
            "\n",
            ">>> Step 2082 metrics: {'loss': 0.3988, 'grad_norm': 6.125, 'learning_rate': 4.57641827322817e-05, 'epoch': 2.83265306122449}\n",
            "\n",
            ">>> Step 2083 metrics: {'loss': 0.5539, 'grad_norm': 7.5, 'learning_rate': 4.576021091571161e-05, 'epoch': 2.834013605442177}\n",
            "\n",
            ">>> Step 2084 metrics: {'loss': 0.4761, 'grad_norm': 6.84375, 'learning_rate': 4.575623741040264e-05, 'epoch': 2.835374149659864}\n",
            "\n",
            ">>> Step 2085 metrics: {'loss': 0.43, 'grad_norm': 6.375, 'learning_rate': 4.575226221667801e-05, 'epoch': 2.836734693877551}\n",
            "\n",
            ">>> Step 2086 metrics: {'loss': 0.3462, 'grad_norm': 5.9375, 'learning_rate': 4.57482853348611e-05, 'epoch': 2.8380952380952382}\n",
            "\n",
            ">>> Step 2087 metrics: {'loss': 0.1596, 'grad_norm': 4.40625, 'learning_rate': 4.57443067652754e-05, 'epoch': 2.839455782312925}\n",
            "\n",
            ">>> Step 2088 metrics: {'loss': 0.8766, 'grad_norm': 7.8125, 'learning_rate': 4.5740326508244553e-05, 'epoch': 2.840816326530612}\n",
            "\n",
            ">>> Step 2089 metrics: {'loss': 0.8038, 'grad_norm': 7.0, 'learning_rate': 4.573634456409232e-05, 'epoch': 2.8421768707482995}\n",
            "\n",
            ">>> Step 2090 metrics: {'loss': 0.3005, 'grad_norm': 6.625, 'learning_rate': 4.573236093314263e-05, 'epoch': 2.8435374149659864}\n",
            "\n",
            ">>> Step 2091 metrics: {'loss': 0.2998, 'grad_norm': 7.0, 'learning_rate': 4.572837561571952e-05, 'epoch': 2.8448979591836734}\n",
            "\n",
            ">>> Step 2092 metrics: {'loss': 0.2932, 'grad_norm': 6.21875, 'learning_rate': 4.5724388612147166e-05, 'epoch': 2.8462585034013603}\n",
            "\n",
            ">>> Step 2093 metrics: {'loss': 0.5033, 'grad_norm': 7.71875, 'learning_rate': 4.5720399922749914e-05, 'epoch': 2.8476190476190477}\n",
            "\n",
            ">>> Step 2094 metrics: {'loss': 0.7311, 'grad_norm': 7.3125, 'learning_rate': 4.57164095478522e-05, 'epoch': 2.8489795918367347}\n",
            "\n",
            ">>> Step 2095 metrics: {'loss': 0.3843, 'grad_norm': 6.0625, 'learning_rate': 4.571241748777863e-05, 'epoch': 2.8503401360544216}\n",
            "\n",
            ">>> Step 2096 metrics: {'loss': 0.3986, 'grad_norm': 6.625, 'learning_rate': 4.570842374285395e-05, 'epoch': 2.851700680272109}\n",
            "\n",
            ">>> Step 2097 metrics: {'loss': 0.2188, 'grad_norm': 5.0625, 'learning_rate': 4.570442831340303e-05, 'epoch': 2.853061224489796}\n",
            "\n",
            ">>> Step 2098 metrics: {'loss': 0.4509, 'grad_norm': 5.5625, 'learning_rate': 4.570043119975086e-05, 'epoch': 2.854421768707483}\n",
            "\n",
            ">>> Step 2099 metrics: {'loss': 0.2229, 'grad_norm': 5.4375, 'learning_rate': 4.56964324022226e-05, 'epoch': 2.85578231292517}\n",
            "\n",
            ">>> Step 2100 metrics: {'loss': 0.3186, 'grad_norm': 6.1875, 'learning_rate': 4.569243192114352e-05, 'epoch': 2.857142857142857}\n",
            "\n",
            ">>> Step 2101 metrics: {'loss': 0.6098, 'grad_norm': 7.625, 'learning_rate': 4.568842975683904e-05, 'epoch': 2.858503401360544}\n",
            "\n",
            ">>> Step 2102 metrics: {'loss': 0.3874, 'grad_norm': 6.8125, 'learning_rate': 4.568442590963474e-05, 'epoch': 2.8598639455782315}\n",
            "\n",
            ">>> Step 2103 metrics: {'loss': 0.788, 'grad_norm': 7.0, 'learning_rate': 4.568042037985628e-05, 'epoch': 2.8612244897959185}\n",
            "\n",
            ">>> Step 2104 metrics: {'loss': 0.1834, 'grad_norm': 5.75, 'learning_rate': 4.567641316782951e-05, 'epoch': 2.8625850340136054}\n",
            "\n",
            ">>> Step 2105 metrics: {'loss': 0.357, 'grad_norm': 6.59375, 'learning_rate': 4.5672404273880396e-05, 'epoch': 2.8639455782312924}\n",
            "\n",
            ">>> Step 2106 metrics: {'loss': 0.4575, 'grad_norm': 6.9375, 'learning_rate': 4.566839369833503e-05, 'epoch': 2.8653061224489798}\n",
            "\n",
            ">>> Step 2107 metrics: {'loss': 0.5693, 'grad_norm': 6.78125, 'learning_rate': 4.566438144151967e-05, 'epoch': 2.8666666666666667}\n",
            "\n",
            ">>> Step 2108 metrics: {'loss': 0.4425, 'grad_norm': 6.5, 'learning_rate': 4.5660367503760666e-05, 'epoch': 2.8680272108843536}\n",
            "\n",
            ">>> Step 2109 metrics: {'loss': 0.4462, 'grad_norm': 7.125, 'learning_rate': 4.565635188538455e-05, 'epoch': 2.869387755102041}\n",
            "\n",
            ">>> Step 2110 metrics: {'loss': 0.2622, 'grad_norm': 6.4375, 'learning_rate': 4.5652334586717974e-05, 'epoch': 2.870748299319728}\n",
            "\n",
            ">>> Step 2111 metrics: {'loss': 0.5095, 'grad_norm': 7.09375, 'learning_rate': 4.564831560808772e-05, 'epoch': 2.872108843537415}\n",
            "\n",
            ">>> Step 2112 metrics: {'loss': 0.4285, 'grad_norm': 5.84375, 'learning_rate': 4.5644294949820724e-05, 'epoch': 2.873469387755102}\n",
            "\n",
            ">>> Step 2113 metrics: {'loss': 0.4513, 'grad_norm': 5.5625, 'learning_rate': 4.5640272612244014e-05, 'epoch': 2.8748299319727892}\n",
            "\n",
            ">>> Step 2114 metrics: {'loss': 0.4133, 'grad_norm': 6.375, 'learning_rate': 4.563624859568482e-05, 'epoch': 2.876190476190476}\n",
            "\n",
            ">>> Step 2115 metrics: {'loss': 0.7907, 'grad_norm': 7.3125, 'learning_rate': 4.563222290047046e-05, 'epoch': 2.877551020408163}\n",
            "\n",
            ">>> Step 2116 metrics: {'loss': 0.3107, 'grad_norm': 6.0, 'learning_rate': 4.5628195526928406e-05, 'epoch': 2.8789115646258505}\n",
            "\n",
            ">>> Step 2117 metrics: {'loss': 0.7595, 'grad_norm': 7.6875, 'learning_rate': 4.5624166475386274e-05, 'epoch': 2.8802721088435375}\n",
            "\n",
            ">>> Step 2118 metrics: {'loss': 0.9139, 'grad_norm': 9.3125, 'learning_rate': 4.562013574617179e-05, 'epoch': 2.8816326530612244}\n",
            "\n",
            ">>> Step 2119 metrics: {'loss': 0.4239, 'grad_norm': 11.5, 'learning_rate': 4.5616103339612846e-05, 'epoch': 2.8829931972789113}\n",
            "\n",
            ">>> Step 2120 metrics: {'loss': 0.5716, 'grad_norm': 7.0625, 'learning_rate': 4.561206925603746e-05, 'epoch': 2.8843537414965987}\n",
            "\n",
            ">>> Step 2121 metrics: {'loss': 0.8619, 'grad_norm': 10.8125, 'learning_rate': 4.560803349577377e-05, 'epoch': 2.8857142857142857}\n",
            "\n",
            ">>> Step 2122 metrics: {'loss': 0.2264, 'grad_norm': 5.375, 'learning_rate': 4.560399605915008e-05, 'epoch': 2.887074829931973}\n",
            "\n",
            ">>> Step 2123 metrics: {'loss': 0.7954, 'grad_norm': 11.1875, 'learning_rate': 4.5599956946494805e-05, 'epoch': 2.88843537414966}\n",
            "\n",
            ">>> Step 2124 metrics: {'loss': 0.4849, 'grad_norm': 6.78125, 'learning_rate': 4.559591615813651e-05, 'epoch': 2.889795918367347}\n",
            "\n",
            ">>> Step 2125 metrics: {'loss': 0.4071, 'grad_norm': 7.125, 'learning_rate': 4.55918736944039e-05, 'epoch': 2.891156462585034}\n",
            "\n",
            ">>> Step 2126 metrics: {'loss': 0.2662, 'grad_norm': 5.28125, 'learning_rate': 4.5587829555625794e-05, 'epoch': 2.892517006802721}\n",
            "\n",
            ">>> Step 2127 metrics: {'loss': 0.3144, 'grad_norm': 6.0, 'learning_rate': 4.558378374213118e-05, 'epoch': 2.8938775510204082}\n",
            "\n",
            ">>> Step 2128 metrics: {'loss': 0.3372, 'grad_norm': 5.84375, 'learning_rate': 4.557973625424915e-05, 'epoch': 2.895238095238095}\n",
            "\n",
            ">>> Step 2129 metrics: {'loss': 0.5536, 'grad_norm': 6.78125, 'learning_rate': 4.5575687092308955e-05, 'epoch': 2.8965986394557826}\n",
            "\n",
            ">>> Step 2130 metrics: {'loss': 0.4072, 'grad_norm': 6.65625, 'learning_rate': 4.557163625663997e-05, 'epoch': 2.8979591836734695}\n",
            "\n",
            ">>> Step 2131 metrics: {'loss': 0.4454, 'grad_norm': 7.3125, 'learning_rate': 4.556758374757172e-05, 'epoch': 2.8993197278911564}\n",
            "\n",
            ">>> Step 2132 metrics: {'loss': 0.4548, 'grad_norm': 6.46875, 'learning_rate': 4.556352956543384e-05, 'epoch': 2.9006802721088434}\n",
            "\n",
            ">>> Step 2133 metrics: {'loss': 0.4609, 'grad_norm': 6.40625, 'learning_rate': 4.555947371055612e-05, 'epoch': 2.9020408163265308}\n",
            "\n",
            ">>> Step 2134 metrics: {'loss': 0.4471, 'grad_norm': 6.90625, 'learning_rate': 4.55554161832685e-05, 'epoch': 2.9034013605442177}\n",
            "\n",
            ">>> Step 2135 metrics: {'loss': 0.28, 'grad_norm': 5.78125, 'learning_rate': 4.5551356983901015e-05, 'epoch': 2.9047619047619047}\n",
            "\n",
            ">>> Step 2136 metrics: {'loss': 0.551, 'grad_norm': 7.03125, 'learning_rate': 4.554729611278388e-05, 'epoch': 2.906122448979592}\n",
            "\n",
            ">>> Step 2137 metrics: {'loss': 0.852, 'grad_norm': 7.75, 'learning_rate': 4.554323357024742e-05, 'epoch': 2.907482993197279}\n",
            "\n",
            ">>> Step 2138 metrics: {'loss': 0.5165, 'grad_norm': 6.78125, 'learning_rate': 4.55391693566221e-05, 'epoch': 2.908843537414966}\n",
            "\n",
            ">>> Step 2139 metrics: {'loss': 0.4697, 'grad_norm': 7.125, 'learning_rate': 4.553510347223853e-05, 'epoch': 2.910204081632653}\n",
            "\n",
            ">>> Step 2140 metrics: {'loss': 0.578, 'grad_norm': 7.5, 'learning_rate': 4.553103591742745e-05, 'epoch': 2.9115646258503403}\n",
            "\n",
            ">>> Step 2141 metrics: {'loss': 0.4992, 'grad_norm': 6.5, 'learning_rate': 4.5526966692519725e-05, 'epoch': 2.912925170068027}\n",
            "\n",
            ">>> Step 2142 metrics: {'loss': 0.6458, 'grad_norm': 7.0625, 'learning_rate': 4.5522895797846365e-05, 'epoch': 2.914285714285714}\n",
            "\n",
            ">>> Step 2143 metrics: {'loss': 0.3088, 'grad_norm': 5.90625, 'learning_rate': 4.551882323373853e-05, 'epoch': 2.9156462585034015}\n",
            "\n",
            ">>> Step 2144 metrics: {'loss': 0.7334, 'grad_norm': 6.84375, 'learning_rate': 4.551474900052749e-05, 'epoch': 2.9170068027210885}\n",
            "\n",
            ">>> Step 2145 metrics: {'loss': 0.3749, 'grad_norm': 6.28125, 'learning_rate': 4.551067309854468e-05, 'epoch': 2.9183673469387754}\n",
            "\n",
            ">>> Step 2146 metrics: {'loss': 0.4246, 'grad_norm': 6.40625, 'learning_rate': 4.550659552812163e-05, 'epoch': 2.9197278911564624}\n",
            "\n",
            ">>> Step 2147 metrics: {'loss': 0.3509, 'grad_norm': 6.0, 'learning_rate': 4.550251628959005e-05, 'epoch': 2.9210884353741497}\n",
            "\n",
            ">>> Step 2148 metrics: {'loss': 0.7997, 'grad_norm': 7.3125, 'learning_rate': 4.5498435383281765e-05, 'epoch': 2.9224489795918367}\n",
            "\n",
            ">>> Step 2149 metrics: {'loss': 0.6939, 'grad_norm': 7.1875, 'learning_rate': 4.549435280952872e-05, 'epoch': 2.923809523809524}\n",
            "\n",
            ">>> Step 2150 metrics: {'loss': 0.4363, 'grad_norm': 6.34375, 'learning_rate': 4.549026856866302e-05, 'epoch': 2.925170068027211}\n",
            "\n",
            ">>> Step 2151 metrics: {'loss': 0.273, 'grad_norm': 7.90625, 'learning_rate': 4.5486182661016905e-05, 'epoch': 2.926530612244898}\n",
            "\n",
            ">>> Step 2152 metrics: {'loss': 0.1643, 'grad_norm': 6.28125, 'learning_rate': 4.548209508692274e-05, 'epoch': 2.927891156462585}\n",
            "\n",
            ">>> Step 2153 metrics: {'loss': 0.8405, 'grad_norm': 9.875, 'learning_rate': 4.547800584671302e-05, 'epoch': 2.929251700680272}\n",
            "\n",
            ">>> Step 2154 metrics: {'loss': 0.5119, 'grad_norm': 6.4375, 'learning_rate': 4.54739149407204e-05, 'epoch': 2.9306122448979592}\n",
            "\n",
            ">>> Step 2155 metrics: {'loss': 0.1632, 'grad_norm': 6.625, 'learning_rate': 4.546982236927764e-05, 'epoch': 2.931972789115646}\n",
            "\n",
            ">>> Step 2156 metrics: {'loss': 0.709, 'grad_norm': 6.78125, 'learning_rate': 4.5465728132717646e-05, 'epoch': 2.9333333333333336}\n",
            "\n",
            ">>> Step 2157 metrics: {'loss': 0.3196, 'grad_norm': 6.5625, 'learning_rate': 4.546163223137347e-05, 'epoch': 2.9346938775510205}\n",
            "\n",
            ">>> Step 2158 metrics: {'loss': 0.6713, 'grad_norm': 7.65625, 'learning_rate': 4.545753466557831e-05, 'epoch': 2.9360544217687075}\n",
            "\n",
            ">>> Step 2159 metrics: {'loss': 0.4516, 'grad_norm': 7.84375, 'learning_rate': 4.545343543566546e-05, 'epoch': 2.9374149659863944}\n",
            "\n",
            ">>> Step 2160 metrics: {'loss': 0.4745, 'grad_norm': 7.46875, 'learning_rate': 4.5449334541968385e-05, 'epoch': 2.938775510204082}\n",
            "\n",
            ">>> Step 2161 metrics: {'loss': 0.4667, 'grad_norm': 6.6875, 'learning_rate': 4.5445231984820665e-05, 'epoch': 2.9401360544217687}\n",
            "\n",
            ">>> Step 2162 metrics: {'loss': 0.4767, 'grad_norm': 7.40625, 'learning_rate': 4.544112776455602e-05, 'epoch': 2.9414965986394557}\n",
            "\n",
            ">>> Step 2163 metrics: {'loss': 0.6411, 'grad_norm': 6.28125, 'learning_rate': 4.543702188150831e-05, 'epoch': 2.942857142857143}\n",
            "\n",
            ">>> Step 2164 metrics: {'loss': 0.3257, 'grad_norm': 6.0, 'learning_rate': 4.543291433601154e-05, 'epoch': 2.94421768707483}\n",
            "\n",
            ">>> Step 2165 metrics: {'loss': 0.3038, 'grad_norm': 5.96875, 'learning_rate': 4.542880512839982e-05, 'epoch': 2.945578231292517}\n",
            "\n",
            ">>> Step 2166 metrics: {'loss': 0.355, 'grad_norm': 6.71875, 'learning_rate': 4.5424694259007425e-05, 'epoch': 2.946938775510204}\n",
            "\n",
            ">>> Step 2167 metrics: {'loss': 0.3713, 'grad_norm': 5.875, 'learning_rate': 4.5420581728168744e-05, 'epoch': 2.9482993197278913}\n",
            "\n",
            ">>> Step 2168 metrics: {'loss': 0.5157, 'grad_norm': 8.625, 'learning_rate': 4.541646753621832e-05, 'epoch': 2.949659863945578}\n",
            "\n",
            ">>> Step 2169 metrics: {'loss': 0.4101, 'grad_norm': 6.125, 'learning_rate': 4.5412351683490826e-05, 'epoch': 2.951020408163265}\n",
            "\n",
            ">>> Step 2170 metrics: {'loss': 0.4663, 'grad_norm': 7.25, 'learning_rate': 4.540823417032105e-05, 'epoch': 2.9523809523809526}\n",
            "\n",
            ">>> Step 2171 metrics: {'loss': 0.647, 'grad_norm': 7.625, 'learning_rate': 4.5404114997043945e-05, 'epoch': 2.9537414965986395}\n",
            "\n",
            ">>> Step 2172 metrics: {'loss': 0.5655, 'grad_norm': 7.0625, 'learning_rate': 4.539999416399457e-05, 'epoch': 2.9551020408163264}\n",
            "\n",
            ">>> Step 2173 metrics: {'loss': 0.6331, 'grad_norm': 7.03125, 'learning_rate': 4.539587167150815e-05, 'epoch': 2.9564625850340134}\n",
            "\n",
            ">>> Step 2174 metrics: {'loss': 0.6676, 'grad_norm': 6.78125, 'learning_rate': 4.539174751992003e-05, 'epoch': 2.9578231292517008}\n",
            "\n",
            ">>> Step 2175 metrics: {'loss': 0.471, 'grad_norm': 7.40625, 'learning_rate': 4.538762170956566e-05, 'epoch': 2.9591836734693877}\n",
            "\n",
            ">>> Step 2176 metrics: {'loss': 0.4237, 'grad_norm': 6.78125, 'learning_rate': 4.5383494240780696e-05, 'epoch': 2.960544217687075}\n",
            "\n",
            ">>> Step 2177 metrics: {'loss': 0.4069, 'grad_norm': 8.8125, 'learning_rate': 4.5379365113900865e-05, 'epoch': 2.961904761904762}\n",
            "\n",
            ">>> Step 2178 metrics: {'loss': 0.8344, 'grad_norm': 9.4375, 'learning_rate': 4.537523432926205e-05, 'epoch': 2.963265306122449}\n",
            "\n",
            ">>> Step 2179 metrics: {'loss': 0.6484, 'grad_norm': 7.75, 'learning_rate': 4.5371101887200265e-05, 'epoch': 2.964625850340136}\n",
            "\n",
            ">>> Step 2180 metrics: {'loss': 0.6309, 'grad_norm': 7.09375, 'learning_rate': 4.5366967788051676e-05, 'epoch': 2.965986394557823}\n",
            "\n",
            ">>> Step 2181 metrics: {'loss': 0.1813, 'grad_norm': 4.96875, 'learning_rate': 4.536283203215256e-05, 'epoch': 2.9673469387755103}\n",
            "\n",
            ">>> Step 2182 metrics: {'loss': 0.7361, 'grad_norm': 7.34375, 'learning_rate': 4.535869461983935e-05, 'epoch': 2.968707482993197}\n",
            "\n",
            ">>> Step 2183 metrics: {'loss': 0.7302, 'grad_norm': 11.3125, 'learning_rate': 4.5354555551448594e-05, 'epoch': 2.9700680272108846}\n",
            "\n",
            ">>> Step 2184 metrics: {'loss': 0.1755, 'grad_norm': 6.28125, 'learning_rate': 4.535041482731699e-05, 'epoch': 2.9714285714285715}\n",
            "\n",
            ">>> Step 2185 metrics: {'loss': 0.419, 'grad_norm': 7.59375, 'learning_rate': 4.534627244778138e-05, 'epoch': 2.9727891156462585}\n",
            "\n",
            ">>> Step 2186 metrics: {'loss': 0.3934, 'grad_norm': 5.875, 'learning_rate': 4.53421284131787e-05, 'epoch': 2.9741496598639454}\n",
            "\n",
            ">>> Step 2187 metrics: {'loss': 0.2556, 'grad_norm': 5.0, 'learning_rate': 4.533798272384605e-05, 'epoch': 2.975510204081633}\n",
            "\n",
            ">>> Step 2188 metrics: {'loss': 0.3013, 'grad_norm': 5.34375, 'learning_rate': 4.533383538012067e-05, 'epoch': 2.9768707482993197}\n",
            "\n",
            ">>> Step 2189 metrics: {'loss': 0.3512, 'grad_norm': 5.84375, 'learning_rate': 4.532968638233993e-05, 'epoch': 2.9782312925170067}\n",
            "\n",
            ">>> Step 2190 metrics: {'loss': 0.4039, 'grad_norm': 6.0625, 'learning_rate': 4.532553573084132e-05, 'epoch': 2.979591836734694}\n",
            "\n",
            ">>> Step 2191 metrics: {'loss': 0.6053, 'grad_norm': 6.78125, 'learning_rate': 4.532138342596248e-05, 'epoch': 2.980952380952381}\n",
            "\n",
            ">>> Step 2192 metrics: {'loss': 0.9508, 'grad_norm': 7.28125, 'learning_rate': 4.5317229468041174e-05, 'epoch': 2.982312925170068}\n",
            "\n",
            ">>> Step 2193 metrics: {'loss': 0.4613, 'grad_norm': 6.40625, 'learning_rate': 4.531307385741531e-05, 'epoch': 2.983673469387755}\n",
            "\n",
            ">>> Step 2194 metrics: {'loss': 0.4633, 'grad_norm': 6.5625, 'learning_rate': 4.530891659442292e-05, 'epoch': 2.9850340136054423}\n",
            "\n",
            ">>> Step 2195 metrics: {'loss': 0.2319, 'grad_norm': 5.625, 'learning_rate': 4.530475767940219e-05, 'epoch': 2.9863945578231292}\n",
            "\n",
            ">>> Step 2196 metrics: {'loss': 0.6022, 'grad_norm': 7.28125, 'learning_rate': 4.5300597112691414e-05, 'epoch': 2.987755102040816}\n",
            "\n",
            ">>> Step 2197 metrics: {'loss': 0.7678, 'grad_norm': 7.65625, 'learning_rate': 4.529643489462904e-05, 'epoch': 2.9891156462585036}\n",
            "\n",
            ">>> Step 2198 metrics: {'loss': 0.4477, 'grad_norm': 6.5625, 'learning_rate': 4.529227102555364e-05, 'epoch': 2.9904761904761905}\n",
            "\n",
            ">>> Step 2199 metrics: {'loss': 0.6893, 'grad_norm': 6.71875, 'learning_rate': 4.528810550580393e-05, 'epoch': 2.9918367346938775}\n",
            "\n",
            ">>> Step 2200 metrics: {'loss': 0.4379, 'grad_norm': 7.0625, 'learning_rate': 4.5283938335718736e-05, 'epoch': 2.9931972789115644}\n",
            "\n",
            ">>> Step 2201 metrics: {'loss': 0.6414, 'grad_norm': 9.625, 'learning_rate': 4.5279769515637055e-05, 'epoch': 2.994557823129252}\n",
            "\n",
            ">>> Step 2202 metrics: {'loss': 0.8642, 'grad_norm': 8.125, 'learning_rate': 4.5275599045898e-05, 'epoch': 2.9959183673469387}\n",
            "\n",
            ">>> Step 2203 metrics: {'loss': 1.4433, 'grad_norm': 15.6875, 'learning_rate': 4.52714269268408e-05, 'epoch': 2.997278911564626}\n",
            "\n",
            ">>> Step 2204 metrics: {'loss': 0.2687, 'grad_norm': 6.4375, 'learning_rate': 4.526725315880486e-05, 'epoch': 2.998639455782313}\n",
            "\n",
            ">>> Step 2205 metrics: {'loss': 0.3896, 'grad_norm': 6.125, 'learning_rate': 4.526307774212967e-05, 'epoch': 3.0}\n",
            "\n",
            ">>> Step 2206 metrics: {'loss': 0.391, 'grad_norm': 6.96875, 'learning_rate': 4.52589006771549e-05, 'epoch': 3.001360544217687}\n",
            "\n",
            ">>> Step 2207 metrics: {'loss': 0.2339, 'grad_norm': 5.4375, 'learning_rate': 4.5254721964220324e-05, 'epoch': 3.0027210884353743}\n",
            "\n",
            ">>> Step 2208 metrics: {'loss': 0.1054, 'grad_norm': 3.734375, 'learning_rate': 4.525054160366586e-05, 'epoch': 3.0040816326530613}\n",
            "\n",
            ">>> Step 2209 metrics: {'loss': 0.2214, 'grad_norm': 5.34375, 'learning_rate': 4.524635959583156e-05, 'epoch': 3.005442176870748}\n",
            "\n",
            ">>> Step 2210 metrics: {'loss': 0.2255, 'grad_norm': 5.0625, 'learning_rate': 4.5242175941057606e-05, 'epoch': 3.006802721088435}\n",
            "\n",
            ">>> Step 2211 metrics: {'loss': 0.33, 'grad_norm': 6.09375, 'learning_rate': 4.5237990639684326e-05, 'epoch': 3.0081632653061225}\n",
            "\n",
            ">>> Step 2212 metrics: {'loss': 0.3799, 'grad_norm': 5.25, 'learning_rate': 4.523380369205217e-05, 'epoch': 3.0095238095238095}\n",
            "\n",
            ">>> Step 2213 metrics: {'loss': 0.1162, 'grad_norm': 5.15625, 'learning_rate': 4.522961509850171e-05, 'epoch': 3.0108843537414964}\n",
            "\n",
            ">>> Step 2214 metrics: {'loss': 0.4207, 'grad_norm': 9.0, 'learning_rate': 4.522542485937369e-05, 'epoch': 3.012244897959184}\n",
            "\n",
            ">>> Step 2215 metrics: {'loss': 0.2151, 'grad_norm': 6.46875, 'learning_rate': 4.522123297500895e-05, 'epoch': 3.0136054421768708}\n",
            "\n",
            ">>> Step 2216 metrics: {'loss': 0.2555, 'grad_norm': 8.25, 'learning_rate': 4.521703944574849e-05, 'epoch': 3.0149659863945577}\n",
            "\n",
            ">>> Step 2217 metrics: {'loss': 0.4416, 'grad_norm': 9.125, 'learning_rate': 4.521284427193342e-05, 'epoch': 3.016326530612245}\n",
            "\n",
            ">>> Step 2218 metrics: {'loss': 0.1679, 'grad_norm': 6.25, 'learning_rate': 4.5208647453904995e-05, 'epoch': 3.017687074829932}\n",
            "\n",
            ">>> Step 2219 metrics: {'loss': 0.2113, 'grad_norm': 5.53125, 'learning_rate': 4.520444899200462e-05, 'epoch': 3.019047619047619}\n",
            "\n",
            ">>> Step 2220 metrics: {'loss': 0.2069, 'grad_norm': 9.3125, 'learning_rate': 4.520024888657381e-05, 'epoch': 3.020408163265306}\n",
            "\n",
            ">>> Step 2221 metrics: {'loss': 0.6029, 'grad_norm': 11.0, 'learning_rate': 4.519604713795423e-05, 'epoch': 3.0217687074829933}\n",
            "\n",
            ">>> Step 2222 metrics: {'loss': 0.4301, 'grad_norm': 8.8125, 'learning_rate': 4.5191843746487664e-05, 'epoch': 3.0231292517006803}\n",
            "\n",
            ">>> Step 2223 metrics: {'loss': 0.3904, 'grad_norm': 8.125, 'learning_rate': 4.518763871251603e-05, 'epoch': 3.024489795918367}\n",
            "\n",
            ">>> Step 2224 metrics: {'loss': 0.1313, 'grad_norm': 6.40625, 'learning_rate': 4.518343203638139e-05, 'epoch': 3.0258503401360546}\n",
            "\n",
            ">>> Step 2225 metrics: {'loss': 0.5941, 'grad_norm': 8.0625, 'learning_rate': 4.517922371842595e-05, 'epoch': 3.0272108843537415}\n",
            "\n",
            ">>> Step 2226 metrics: {'loss': 0.2568, 'grad_norm': 5.6875, 'learning_rate': 4.517501375899203e-05, 'epoch': 3.0285714285714285}\n",
            "\n",
            ">>> Step 2227 metrics: {'loss': 0.3106, 'grad_norm': 5.96875, 'learning_rate': 4.517080215842207e-05, 'epoch': 3.029931972789116}\n",
            "\n",
            ">>> Step 2228 metrics: {'loss': 0.244, 'grad_norm': 8.125, 'learning_rate': 4.516658891705868e-05, 'epoch': 3.031292517006803}\n",
            "\n",
            ">>> Step 2229 metrics: {'loss': 0.1888, 'grad_norm': 5.71875, 'learning_rate': 4.516237403524459e-05, 'epoch': 3.0326530612244897}\n",
            "\n",
            ">>> Step 2230 metrics: {'loss': 0.2361, 'grad_norm': 6.625, 'learning_rate': 4.5158157513322654e-05, 'epoch': 3.0340136054421767}\n",
            "\n",
            ">>> Step 2231 metrics: {'loss': 0.2225, 'grad_norm': 6.3125, 'learning_rate': 4.5153939351635857e-05, 'epoch': 3.035374149659864}\n",
            "\n",
            ">>> Step 2232 metrics: {'loss': 0.3701, 'grad_norm': 6.53125, 'learning_rate': 4.5149719550527334e-05, 'epoch': 3.036734693877551}\n",
            "\n",
            ">>> Step 2233 metrics: {'loss': 0.3857, 'grad_norm': 8.25, 'learning_rate': 4.514549811034034e-05, 'epoch': 3.038095238095238}\n",
            "\n",
            ">>> Step 2234 metrics: {'loss': 0.2038, 'grad_norm': 7.125, 'learning_rate': 4.514127503141827e-05, 'epoch': 3.0394557823129253}\n",
            "\n",
            ">>> Step 2235 metrics: {'loss': 0.2368, 'grad_norm': 7.09375, 'learning_rate': 4.5137050314104654e-05, 'epoch': 3.0408163265306123}\n",
            "\n",
            ">>> Step 2236 metrics: {'loss': 0.0988, 'grad_norm': 6.03125, 'learning_rate': 4.513282395874314e-05, 'epoch': 3.0421768707482992}\n",
            "\n",
            ">>> Step 2237 metrics: {'loss': 0.0859, 'grad_norm': 4.8125, 'learning_rate': 4.5128595965677536e-05, 'epoch': 3.043537414965986}\n",
            "\n",
            ">>> Step 2238 metrics: {'loss': 0.1641, 'grad_norm': 5.5625, 'learning_rate': 4.512436633525176e-05, 'epoch': 3.0448979591836736}\n",
            "\n",
            ">>> Step 2239 metrics: {'loss': 0.0944, 'grad_norm': 4.34375, 'learning_rate': 4.512013506780987e-05, 'epoch': 3.0462585034013605}\n",
            "\n",
            ">>> Step 2240 metrics: {'loss': 0.4299, 'grad_norm': 7.75, 'learning_rate': 4.511590216369607e-05, 'epoch': 3.0476190476190474}\n",
            "\n",
            ">>> Step 2241 metrics: {'loss': 0.2813, 'grad_norm': 6.28125, 'learning_rate': 4.511166762325466e-05, 'epoch': 3.048979591836735}\n",
            "\n",
            ">>> Step 2242 metrics: {'loss': 0.1457, 'grad_norm': 5.21875, 'learning_rate': 4.510743144683012e-05, 'epoch': 3.050340136054422}\n",
            "\n",
            ">>> Step 2243 metrics: {'loss': 0.1422, 'grad_norm': 5.53125, 'learning_rate': 4.510319363476704e-05, 'epoch': 3.0517006802721087}\n",
            "\n",
            ">>> Step 2244 metrics: {'loss': 0.2339, 'grad_norm': 7.21875, 'learning_rate': 4.5098954187410136e-05, 'epoch': 3.053061224489796}\n",
            "\n",
            ">>> Step 2245 metrics: {'loss': 0.162, 'grad_norm': 6.03125, 'learning_rate': 4.509471310510428e-05, 'epoch': 3.054421768707483}\n",
            "\n",
            ">>> Step 2246 metrics: {'loss': 0.8481, 'grad_norm': 10.8125, 'learning_rate': 4.509047038819444e-05, 'epoch': 3.05578231292517}\n",
            "\n",
            ">>> Step 2247 metrics: {'loss': 0.2924, 'grad_norm': 7.4375, 'learning_rate': 4.508622603702576e-05, 'epoch': 3.057142857142857}\n",
            "\n",
            ">>> Step 2248 metrics: {'loss': 0.0569, 'grad_norm': 4.15625, 'learning_rate': 4.508198005194349e-05, 'epoch': 3.0585034013605443}\n",
            "\n",
            ">>> Step 2249 metrics: {'loss': 0.1151, 'grad_norm': 4.84375, 'learning_rate': 4.507773243329302e-05, 'epoch': 3.0598639455782313}\n",
            "\n",
            ">>> Step 2250 metrics: {'loss': 0.0553, 'grad_norm': 5.59375, 'learning_rate': 4.507348318141987e-05, 'epoch': 3.061224489795918}\n",
            "\n",
            ">>> Step 2251 metrics: {'loss': 0.3049, 'grad_norm': 6.4375, 'learning_rate': 4.506923229666969e-05, 'epoch': 3.0625850340136056}\n",
            "\n",
            ">>> Step 2252 metrics: {'loss': 0.1605, 'grad_norm': 6.03125, 'learning_rate': 4.5064979779388285e-05, 'epoch': 3.0639455782312925}\n",
            "\n",
            ">>> Step 2253 metrics: {'loss': 0.2507, 'grad_norm': 6.28125, 'learning_rate': 4.5060725629921566e-05, 'epoch': 3.0653061224489795}\n",
            "\n",
            ">>> Step 2254 metrics: {'loss': 0.4838, 'grad_norm': 6.9375, 'learning_rate': 4.505646984861558e-05, 'epoch': 3.066666666666667}\n",
            "\n",
            ">>> Step 2255 metrics: {'loss': 0.5246, 'grad_norm': 7.34375, 'learning_rate': 4.505221243581652e-05, 'epoch': 3.068027210884354}\n",
            "\n",
            ">>> Step 2256 metrics: {'loss': 0.4646, 'grad_norm': 7.34375, 'learning_rate': 4.504795339187071e-05, 'epoch': 3.0693877551020408}\n",
            "\n",
            ">>> Step 2257 metrics: {'loss': 0.176, 'grad_norm': 6.0625, 'learning_rate': 4.50436927171246e-05, 'epoch': 3.0707482993197277}\n",
            "\n",
            ">>> Step 2258 metrics: {'loss': 0.4289, 'grad_norm': 8.25, 'learning_rate': 4.503943041192476e-05, 'epoch': 3.072108843537415}\n",
            "\n",
            ">>> Step 2259 metrics: {'loss': 0.247, 'grad_norm': 6.40625, 'learning_rate': 4.503516647661793e-05, 'epoch': 3.073469387755102}\n",
            "\n",
            ">>> Step 2260 metrics: {'loss': 0.1381, 'grad_norm': 5.03125, 'learning_rate': 4.503090091155094e-05, 'epoch': 3.074829931972789}\n",
            "\n",
            ">>> Step 2261 metrics: {'loss': 0.8265, 'grad_norm': 9.6875, 'learning_rate': 4.502663371707079e-05, 'epoch': 3.0761904761904764}\n",
            "\n",
            ">>> Step 2262 metrics: {'loss': 0.1186, 'grad_norm': 5.84375, 'learning_rate': 4.502236489352458e-05, 'epoch': 3.0775510204081633}\n",
            "\n",
            ">>> Step 2263 metrics: {'loss': 0.6257, 'grad_norm': 6.84375, 'learning_rate': 4.501809444125956e-05, 'epoch': 3.0789115646258503}\n",
            "\n",
            ">>> Step 2264 metrics: {'loss': 0.3567, 'grad_norm': 7.25, 'learning_rate': 4.501382236062312e-05, 'epoch': 3.0802721088435376}\n",
            "\n",
            ">>> Step 2265 metrics: {'loss': 0.0795, 'grad_norm': 4.59375, 'learning_rate': 4.500954865196276e-05, 'epoch': 3.0816326530612246}\n",
            "\n",
            ">>> Step 2266 metrics: {'loss': 0.1612, 'grad_norm': 6.875, 'learning_rate': 4.500527331562613e-05, 'epoch': 3.0829931972789115}\n",
            "\n",
            ">>> Step 2267 metrics: {'loss': 0.5559, 'grad_norm': 8.0625, 'learning_rate': 4.500099635196101e-05, 'epoch': 3.0843537414965985}\n",
            "\n",
            ">>> Step 2268 metrics: {'loss': 0.4207, 'grad_norm': 7.34375, 'learning_rate': 4.49967177613153e-05, 'epoch': 3.085714285714286}\n",
            "\n",
            ">>> Step 2269 metrics: {'loss': 0.2029, 'grad_norm': 6.6875, 'learning_rate': 4.4992437544037045e-05, 'epoch': 3.087074829931973}\n",
            "\n",
            ">>> Step 2270 metrics: {'loss': 0.3645, 'grad_norm': 7.34375, 'learning_rate': 4.4988155700474436e-05, 'epoch': 3.0884353741496597}\n",
            "\n",
            ">>> Step 2271 metrics: {'loss': 0.3288, 'grad_norm': 7.96875, 'learning_rate': 4.498387223097576e-05, 'epoch': 3.089795918367347}\n",
            "\n",
            ">>> Step 2272 metrics: {'loss': 0.4484, 'grad_norm': 8.875, 'learning_rate': 4.497958713588946e-05, 'epoch': 3.091156462585034}\n",
            "\n",
            ">>> Step 2273 metrics: {'loss': 0.152, 'grad_norm': 5.5, 'learning_rate': 4.497530041556411e-05, 'epoch': 3.092517006802721}\n",
            "\n",
            ">>> Step 2274 metrics: {'loss': 0.2893, 'grad_norm': 6.75, 'learning_rate': 4.4971012070348403e-05, 'epoch': 3.093877551020408}\n",
            "\n",
            ">>> Step 2275 metrics: {'loss': 0.3034, 'grad_norm': 6.65625, 'learning_rate': 4.49667221005912e-05, 'epoch': 3.0952380952380953}\n",
            "\n",
            ">>> Step 2276 metrics: {'loss': 0.4106, 'grad_norm': 6.1875, 'learning_rate': 4.496243050664144e-05, 'epoch': 3.0965986394557823}\n",
            "\n",
            ">>> Step 2277 metrics: {'loss': 0.1032, 'grad_norm': 4.21875, 'learning_rate': 4.495813728884824e-05, 'epoch': 3.0979591836734692}\n",
            "\n",
            ">>> Step 2278 metrics: {'loss': 0.2503, 'grad_norm': 6.96875, 'learning_rate': 4.495384244756082e-05, 'epoch': 3.0993197278911566}\n",
            "\n",
            ">>> Step 2279 metrics: {'loss': 0.2929, 'grad_norm': 7.90625, 'learning_rate': 4.494954598312855e-05, 'epoch': 3.1006802721088436}\n",
            "\n",
            ">>> Step 2280 metrics: {'loss': 0.6941, 'grad_norm': 11.5, 'learning_rate': 4.4945247895900934e-05, 'epoch': 3.1020408163265305}\n",
            "\n",
            ">>> Step 2281 metrics: {'loss': 0.0757, 'grad_norm': 4.84375, 'learning_rate': 4.494094818622758e-05, 'epoch': 3.103401360544218}\n",
            "\n",
            ">>> Step 2282 metrics: {'loss': 0.2747, 'grad_norm': 6.96875, 'learning_rate': 4.493664685445827e-05, 'epoch': 3.104761904761905}\n",
            "\n",
            ">>> Step 2283 metrics: {'loss': 0.1165, 'grad_norm': 5.96875, 'learning_rate': 4.4932343900942875e-05, 'epoch': 3.1061224489795918}\n",
            "\n",
            ">>> Step 2284 metrics: {'loss': 0.2114, 'grad_norm': 6.03125, 'learning_rate': 4.492803932603143e-05, 'epoch': 3.1074829931972787}\n",
            "\n",
            ">>> Step 2285 metrics: {'loss': 0.1697, 'grad_norm': 6.6875, 'learning_rate': 4.4923733130074095e-05, 'epoch': 3.108843537414966}\n",
            "\n",
            ">>> Step 2286 metrics: {'loss': 0.3872, 'grad_norm': 7.34375, 'learning_rate': 4.491942531342114e-05, 'epoch': 3.110204081632653}\n",
            "\n",
            ">>> Step 2287 metrics: {'loss': 0.1441, 'grad_norm': 5.375, 'learning_rate': 4.4915115876423006e-05, 'epoch': 3.11156462585034}\n",
            "\n",
            ">>> Step 2288 metrics: {'loss': 0.1667, 'grad_norm': 5.59375, 'learning_rate': 4.491080481943023e-05, 'epoch': 3.1129251700680274}\n",
            "\n",
            ">>> Step 2289 metrics: {'loss': 0.3891, 'grad_norm': 7.90625, 'learning_rate': 4.49064921427935e-05, 'epoch': 3.1142857142857143}\n",
            "\n",
            ">>> Step 2290 metrics: {'loss': 0.3587, 'grad_norm': 7.15625, 'learning_rate': 4.4902177846863624e-05, 'epoch': 3.1156462585034013}\n",
            "\n",
            ">>> Step 2291 metrics: {'loss': 0.1144, 'grad_norm': 6.1875, 'learning_rate': 4.489786193199156e-05, 'epoch': 3.1170068027210887}\n",
            "\n",
            ">>> Step 2292 metrics: {'loss': 0.2986, 'grad_norm': 6.0, 'learning_rate': 4.489354439852838e-05, 'epoch': 3.1183673469387756}\n",
            "\n",
            ">>> Step 2293 metrics: {'loss': 0.2032, 'grad_norm': 7.03125, 'learning_rate': 4.4889225246825284e-05, 'epoch': 3.1197278911564625}\n",
            "\n",
            ">>> Step 2294 metrics: {'loss': 0.2841, 'grad_norm': 7.28125, 'learning_rate': 4.488490447723363e-05, 'epoch': 3.1210884353741495}\n",
            "\n",
            ">>> Step 2295 metrics: {'loss': 0.262, 'grad_norm': 7.1875, 'learning_rate': 4.488058209010488e-05, 'epoch': 3.122448979591837}\n",
            "\n",
            ">>> Step 2296 metrics: {'loss': 0.2978, 'grad_norm': 6.9375, 'learning_rate': 4.487625808579065e-05, 'epoch': 3.123809523809524}\n",
            "\n",
            ">>> Step 2297 metrics: {'loss': 0.1589, 'grad_norm': 4.84375, 'learning_rate': 4.4871932464642655e-05, 'epoch': 3.1251700680272108}\n",
            "\n",
            ">>> Step 2298 metrics: {'loss': 0.1112, 'grad_norm': 4.90625, 'learning_rate': 4.486760522701278e-05, 'epoch': 3.126530612244898}\n",
            "\n",
            ">>> Step 2299 metrics: {'loss': 0.0977, 'grad_norm': 7.40625, 'learning_rate': 4.486327637325302e-05, 'epoch': 3.127891156462585}\n",
            "\n",
            ">>> Step 2300 metrics: {'loss': 0.2395, 'grad_norm': 6.03125, 'learning_rate': 4.485894590371551e-05, 'epoch': 3.129251700680272}\n",
            "\n",
            ">>> Step 2301 metrics: {'loss': 0.2147, 'grad_norm': 5.78125, 'learning_rate': 4.48546138187525e-05, 'epoch': 3.130612244897959}\n",
            "\n",
            ">>> Step 2302 metrics: {'loss': 0.4148, 'grad_norm': 7.3125, 'learning_rate': 4.485028011871639e-05, 'epoch': 3.1319727891156464}\n",
            "\n",
            ">>> Step 2303 metrics: {'loss': 0.192, 'grad_norm': 6.53125, 'learning_rate': 4.484594480395971e-05, 'epoch': 3.1333333333333333}\n",
            "\n",
            ">>> Step 2304 metrics: {'loss': 0.3159, 'grad_norm': 6.71875, 'learning_rate': 4.48416078748351e-05, 'epoch': 3.1346938775510202}\n",
            "\n",
            ">>> Step 2305 metrics: {'loss': 0.2486, 'grad_norm': 7.75, 'learning_rate': 4.483726933169538e-05, 'epoch': 3.1360544217687076}\n",
            "\n",
            ">>> Step 2306 metrics: {'loss': 0.5396, 'grad_norm': 7.25, 'learning_rate': 4.4832929174893427e-05, 'epoch': 3.1374149659863946}\n",
            "\n",
            ">>> Step 2307 metrics: {'loss': 0.4525, 'grad_norm': 8.1875, 'learning_rate': 4.482858740478232e-05, 'epoch': 3.1387755102040815}\n",
            "\n",
            ">>> Step 2308 metrics: {'loss': 0.5314, 'grad_norm': 8.625, 'learning_rate': 4.4824244021715225e-05, 'epoch': 3.140136054421769}\n",
            "\n",
            ">>> Step 2309 metrics: {'loss': 0.1943, 'grad_norm': 5.84375, 'learning_rate': 4.481989902604546e-05, 'epoch': 3.141496598639456}\n",
            "\n",
            ">>> Step 2310 metrics: {'loss': 0.1623, 'grad_norm': 5.84375, 'learning_rate': 4.481555241812648e-05, 'epoch': 3.142857142857143}\n",
            "\n",
            ">>> Step 2311 metrics: {'loss': 0.1888, 'grad_norm': 5.28125, 'learning_rate': 4.481120419831184e-05, 'epoch': 3.1442176870748297}\n",
            "\n",
            ">>> Step 2312 metrics: {'loss': 0.2392, 'grad_norm': 6.8125, 'learning_rate': 4.480685436695525e-05, 'epoch': 3.145578231292517}\n",
            "\n",
            ">>> Step 2313 metrics: {'loss': 0.1791, 'grad_norm': 6.125, 'learning_rate': 4.480250292441056e-05, 'epoch': 3.146938775510204}\n",
            "\n",
            ">>> Step 2314 metrics: {'loss': 0.2694, 'grad_norm': 6.25, 'learning_rate': 4.479814987103173e-05, 'epoch': 3.148299319727891}\n",
            "\n",
            ">>> Step 2315 metrics: {'loss': 0.1477, 'grad_norm': 5.53125, 'learning_rate': 4.479379520717286e-05, 'epoch': 3.1496598639455784}\n",
            "\n",
            ">>> Step 2316 metrics: {'loss': 0.3072, 'grad_norm': 7.84375, 'learning_rate': 4.478943893318817e-05, 'epoch': 3.1510204081632653}\n",
            "\n",
            ">>> Step 2317 metrics: {'loss': 0.2067, 'grad_norm': 5.75, 'learning_rate': 4.478508104943204e-05, 'epoch': 3.1523809523809523}\n",
            "\n",
            ">>> Step 2318 metrics: {'loss': 0.2815, 'grad_norm': 7.34375, 'learning_rate': 4.4780721556258944e-05, 'epoch': 3.1537414965986397}\n",
            "\n",
            ">>> Step 2319 metrics: {'loss': 0.1462, 'grad_norm': 5.5625, 'learning_rate': 4.4776360454023515e-05, 'epoch': 3.1551020408163266}\n",
            "\n",
            ">>> Step 2320 metrics: {'loss': 0.2012, 'grad_norm': 6.21875, 'learning_rate': 4.4771997743080506e-05, 'epoch': 3.1564625850340136}\n",
            "\n",
            ">>> Step 2321 metrics: {'loss': 0.3239, 'grad_norm': 7.9375, 'learning_rate': 4.476763342378479e-05, 'epoch': 3.1578231292517005}\n",
            "\n",
            ">>> Step 2322 metrics: {'loss': 0.835, 'grad_norm': 10.8125, 'learning_rate': 4.4763267496491407e-05, 'epoch': 3.159183673469388}\n",
            "\n",
            ">>> Step 2323 metrics: {'loss': 0.4022, 'grad_norm': 8.5, 'learning_rate': 4.475889996155548e-05, 'epoch': 3.160544217687075}\n",
            "\n",
            ">>> Step 2324 metrics: {'loss': 0.3944, 'grad_norm': 6.84375, 'learning_rate': 4.4754530819332295e-05, 'epoch': 3.1619047619047618}\n",
            "\n",
            ">>> Step 2325 metrics: {'loss': 0.1533, 'grad_norm': 6.78125, 'learning_rate': 4.4750160070177264e-05, 'epoch': 3.163265306122449}\n",
            "\n",
            ">>> Step 2326 metrics: {'loss': 0.3511, 'grad_norm': 7.4375, 'learning_rate': 4.474578771444592e-05, 'epoch': 3.164625850340136}\n",
            "\n",
            ">>> Step 2327 metrics: {'loss': 0.1886, 'grad_norm': 6.1875, 'learning_rate': 4.4741413752493934e-05, 'epoch': 3.165986394557823}\n",
            "\n",
            ">>> Step 2328 metrics: {'loss': 0.374, 'grad_norm': 6.875, 'learning_rate': 4.4737038184677095e-05, 'epoch': 3.16734693877551}\n",
            "\n",
            ">>> Step 2329 metrics: {'loss': 0.4872, 'grad_norm': 7.53125, 'learning_rate': 4.473266101135135e-05, 'epoch': 3.1687074829931974}\n",
            "\n",
            ">>> Step 2330 metrics: {'loss': 0.492, 'grad_norm': 8.625, 'learning_rate': 4.4728282232872756e-05, 'epoch': 3.1700680272108843}\n",
            "\n",
            ">>> Step 2331 metrics: {'loss': 0.5067, 'grad_norm': 7.34375, 'learning_rate': 4.47239018495975e-05, 'epoch': 3.1714285714285713}\n",
            "\n",
            ">>> Step 2332 metrics: {'loss': 0.3521, 'grad_norm': 7.3125, 'learning_rate': 4.471951986188191e-05, 'epoch': 3.1727891156462587}\n",
            "\n",
            ">>> Step 2333 metrics: {'loss': 0.5808, 'grad_norm': 9.5625, 'learning_rate': 4.4715136270082425e-05, 'epoch': 3.1741496598639456}\n",
            "\n",
            ">>> Step 2334 metrics: {'loss': 0.2316, 'grad_norm': 6.96875, 'learning_rate': 4.471075107455565e-05, 'epoch': 3.1755102040816325}\n",
            "\n",
            ">>> Step 2335 metrics: {'loss': 0.2884, 'grad_norm': 8.25, 'learning_rate': 4.470636427565828e-05, 'epoch': 3.17687074829932}\n",
            "\n",
            ">>> Step 2336 metrics: {'loss': 0.3403, 'grad_norm': 7.5625, 'learning_rate': 4.470197587374716e-05, 'epoch': 3.178231292517007}\n",
            "\n",
            ">>> Step 2337 metrics: {'loss': 0.2388, 'grad_norm': 6.34375, 'learning_rate': 4.4697585869179284e-05, 'epoch': 3.179591836734694}\n",
            "\n",
            ">>> Step 2338 metrics: {'loss': 0.2583, 'grad_norm': 6.09375, 'learning_rate': 4.4693194262311736e-05, 'epoch': 3.1809523809523808}\n",
            "\n",
            ">>> Step 2339 metrics: {'loss': 0.6633, 'grad_norm': 7.8125, 'learning_rate': 4.468880105350175e-05, 'epoch': 3.182312925170068}\n",
            "\n",
            ">>> Step 2340 metrics: {'loss': 0.2197, 'grad_norm': 6.71875, 'learning_rate': 4.468440624310671e-05, 'epoch': 3.183673469387755}\n",
            "\n",
            ">>> Step 2341 metrics: {'loss': 0.1599, 'grad_norm': 5.0625, 'learning_rate': 4.4680009831484096e-05, 'epoch': 3.185034013605442}\n",
            "\n",
            ">>> Step 2342 metrics: {'loss': 0.2402, 'grad_norm': 6.59375, 'learning_rate': 4.467561181899155e-05, 'epoch': 3.1863945578231294}\n",
            "\n",
            ">>> Step 2343 metrics: {'loss': 0.5088, 'grad_norm': 9.625, 'learning_rate': 4.4671212205986805e-05, 'epoch': 3.1877551020408164}\n",
            "\n",
            ">>> Step 2344 metrics: {'loss': 0.279, 'grad_norm': 7.21875, 'learning_rate': 4.4666810992827766e-05, 'epoch': 3.1891156462585033}\n",
            "\n",
            ">>> Step 2345 metrics: {'loss': 0.2166, 'grad_norm': 6.625, 'learning_rate': 4.466240817987244e-05, 'epoch': 3.1904761904761907}\n",
            "\n",
            ">>> Step 2346 metrics: {'loss': 0.2265, 'grad_norm': 6.59375, 'learning_rate': 4.465800376747898e-05, 'epoch': 3.1918367346938776}\n",
            "\n",
            ">>> Step 2347 metrics: {'loss': 0.2007, 'grad_norm': 6.59375, 'learning_rate': 4.465359775600566e-05, 'epoch': 3.1931972789115646}\n",
            "\n",
            ">>> Step 2348 metrics: {'loss': 0.1522, 'grad_norm': 5.90625, 'learning_rate': 4.464919014581089e-05, 'epoch': 3.1945578231292515}\n",
            "\n",
            ">>> Step 2349 metrics: {'loss': 0.2752, 'grad_norm': 7.28125, 'learning_rate': 4.46447809372532e-05, 'epoch': 3.195918367346939}\n",
            "\n",
            ">>> Step 2350 metrics: {'loss': 0.224, 'grad_norm': 6.15625, 'learning_rate': 4.464037013069126e-05, 'epoch': 3.197278911564626}\n",
            "\n",
            ">>> Step 2351 metrics: {'loss': 0.3799, 'grad_norm': 8.5, 'learning_rate': 4.463595772648387e-05, 'epoch': 3.198639455782313}\n",
            "\n",
            ">>> Step 2352 metrics: {'loss': 0.3846, 'grad_norm': 7.53125, 'learning_rate': 4.463154372498995e-05, 'epoch': 3.2}\n",
            "\n",
            ">>> Step 2353 metrics: {'loss': 0.2902, 'grad_norm': 7.1875, 'learning_rate': 4.462712812656857e-05, 'epoch': 3.201360544217687}\n",
            "\n",
            ">>> Step 2354 metrics: {'loss': 0.4428, 'grad_norm': 6.9375, 'learning_rate': 4.462271093157891e-05, 'epoch': 3.202721088435374}\n",
            "\n",
            ">>> Step 2355 metrics: {'loss': 0.3797, 'grad_norm': 8.125, 'learning_rate': 4.461829214038028e-05, 'epoch': 3.204081632653061}\n",
            "\n",
            ">>> Step 2356 metrics: {'loss': 0.4745, 'grad_norm': 7.53125, 'learning_rate': 4.4613871753332134e-05, 'epoch': 3.2054421768707484}\n",
            "\n",
            ">>> Step 2357 metrics: {'loss': 0.0923, 'grad_norm': 5.25, 'learning_rate': 4.460944977079404e-05, 'epoch': 3.2068027210884353}\n",
            "\n",
            ">>> Step 2358 metrics: {'loss': 0.3063, 'grad_norm': 7.46875, 'learning_rate': 4.4605026193125716e-05, 'epoch': 3.2081632653061223}\n",
            "\n",
            ">>> Step 2359 metrics: {'loss': 0.2804, 'grad_norm': 6.53125, 'learning_rate': 4.460060102068699e-05, 'epoch': 3.2095238095238097}\n",
            "\n",
            ">>> Step 2360 metrics: {'loss': 0.3844, 'grad_norm': 6.75, 'learning_rate': 4.459617425383783e-05, 'epoch': 3.2108843537414966}\n",
            "\n",
            ">>> Step 2361 metrics: {'loss': 0.1759, 'grad_norm': 4.84375, 'learning_rate': 4.459174589293833e-05, 'epoch': 3.2122448979591836}\n",
            "\n",
            ">>> Step 2362 metrics: {'loss': 0.5005, 'grad_norm': 8.25, 'learning_rate': 4.458731593834871e-05, 'epoch': 3.213605442176871}\n",
            "\n",
            ">>> Step 2363 metrics: {'loss': 0.1522, 'grad_norm': 5.625, 'learning_rate': 4.458288439042933e-05, 'epoch': 3.214965986394558}\n",
            "\n",
            ">>> Step 2364 metrics: {'loss': 0.2318, 'grad_norm': 7.9375, 'learning_rate': 4.457845124954069e-05, 'epoch': 3.216326530612245}\n",
            "\n",
            ">>> Step 2365 metrics: {'loss': 0.454, 'grad_norm': 8.25, 'learning_rate': 4.457401651604337e-05, 'epoch': 3.2176870748299318}\n",
            "\n",
            ">>> Step 2366 metrics: {'loss': 0.189, 'grad_norm': 7.84375, 'learning_rate': 4.456958019029814e-05, 'epoch': 3.219047619047619}\n",
            "\n",
            ">>> Step 2367 metrics: {'loss': 0.2782, 'grad_norm': 6.46875, 'learning_rate': 4.456514227266586e-05, 'epoch': 3.220408163265306}\n",
            "\n",
            ">>> Step 2368 metrics: {'loss': 0.27, 'grad_norm': 6.5625, 'learning_rate': 4.456070276350754e-05, 'epoch': 3.221768707482993}\n",
            "\n",
            ">>> Step 2369 metrics: {'loss': 0.1696, 'grad_norm': 5.21875, 'learning_rate': 4.455626166318431e-05, 'epoch': 3.2231292517006804}\n",
            "\n",
            ">>> Step 2370 metrics: {'loss': 0.1416, 'grad_norm': 6.40625, 'learning_rate': 4.455181897205742e-05, 'epoch': 3.2244897959183674}\n",
            "\n",
            ">>> Step 2371 metrics: {'loss': 0.4282, 'grad_norm': 7.3125, 'learning_rate': 4.454737469048828e-05, 'epoch': 3.2258503401360543}\n",
            "\n",
            ">>> Step 2372 metrics: {'loss': 0.3295, 'grad_norm': 6.71875, 'learning_rate': 4.454292881883839e-05, 'epoch': 3.2272108843537417}\n",
            "\n",
            ">>> Step 2373 metrics: {'loss': 0.1533, 'grad_norm': 5.8125, 'learning_rate': 4.453848135746942e-05, 'epoch': 3.2285714285714286}\n",
            "\n",
            ">>> Step 2374 metrics: {'loss': 0.2197, 'grad_norm': 6.21875, 'learning_rate': 4.453403230674313e-05, 'epoch': 3.2299319727891156}\n",
            "\n",
            ">>> Step 2375 metrics: {'loss': 0.5143, 'grad_norm': 7.59375, 'learning_rate': 4.4529581667021434e-05, 'epoch': 3.2312925170068025}\n",
            "\n",
            ">>> Step 2376 metrics: {'loss': 0.1622, 'grad_norm': 5.84375, 'learning_rate': 4.452512943866638e-05, 'epoch': 3.23265306122449}\n",
            "\n",
            ">>> Step 2377 metrics: {'loss': 0.3908, 'grad_norm': 8.1875, 'learning_rate': 4.452067562204012e-05, 'epoch': 3.234013605442177}\n",
            "\n",
            ">>> Step 2378 metrics: {'loss': 0.4973, 'grad_norm': 7.75, 'learning_rate': 4.4516220217504956e-05, 'epoch': 3.235374149659864}\n",
            "\n",
            ">>> Step 2379 metrics: {'loss': 0.1488, 'grad_norm': 5.25, 'learning_rate': 4.4511763225423306e-05, 'epoch': 3.236734693877551}\n",
            "\n",
            ">>> Step 2380 metrics: {'loss': 0.2551, 'grad_norm': 7.09375, 'learning_rate': 4.450730464615774e-05, 'epoch': 3.238095238095238}\n",
            "\n",
            ">>> Step 2381 metrics: {'loss': 0.4322, 'grad_norm': 9.9375, 'learning_rate': 4.450284448007093e-05, 'epoch': 3.239455782312925}\n",
            "\n",
            ">>> Step 2382 metrics: {'loss': 0.3064, 'grad_norm': 7.15625, 'learning_rate': 4.449838272752568e-05, 'epoch': 3.240816326530612}\n",
            "\n",
            ">>> Step 2383 metrics: {'loss': 0.1382, 'grad_norm': 5.1875, 'learning_rate': 4.4493919388884945e-05, 'epoch': 3.2421768707482994}\n",
            "\n",
            ">>> Step 2384 metrics: {'loss': 0.5661, 'grad_norm': 7.78125, 'learning_rate': 4.448945446451179e-05, 'epoch': 3.2435374149659864}\n",
            "\n",
            ">>> Step 2385 metrics: {'loss': 0.2724, 'grad_norm': 7.25, 'learning_rate': 4.4484987954769415e-05, 'epoch': 3.2448979591836733}\n",
            "\n",
            ">>> Step 2386 metrics: {'loss': 0.197, 'grad_norm': 6.375, 'learning_rate': 4.448051986002114e-05, 'epoch': 3.2462585034013607}\n",
            "\n",
            ">>> Step 2387 metrics: {'loss': 0.2023, 'grad_norm': 7.125, 'learning_rate': 4.447605018063044e-05, 'epoch': 3.2476190476190476}\n",
            "\n",
            ">>> Step 2388 metrics: {'loss': 0.1818, 'grad_norm': 6.84375, 'learning_rate': 4.447157891696089e-05, 'epoch': 3.2489795918367346}\n",
            "\n",
            ">>> Step 2389 metrics: {'loss': 0.2062, 'grad_norm': 5.75, 'learning_rate': 4.4467106069376196e-05, 'epoch': 3.250340136054422}\n",
            "\n",
            ">>> Step 2390 metrics: {'loss': 0.4661, 'grad_norm': 7.25, 'learning_rate': 4.4462631638240215e-05, 'epoch': 3.251700680272109}\n",
            "\n",
            ">>> Step 2391 metrics: {'loss': 0.1592, 'grad_norm': 6.40625, 'learning_rate': 4.445815562391692e-05, 'epoch': 3.253061224489796}\n",
            "\n",
            ">>> Step 2392 metrics: {'loss': 0.3996, 'grad_norm': 7.40625, 'learning_rate': 4.4453678026770406e-05, 'epoch': 3.2544217687074832}\n",
            "\n",
            ">>> Step 2393 metrics: {'loss': 0.2208, 'grad_norm': 6.34375, 'learning_rate': 4.444919884716491e-05, 'epoch': 3.25578231292517}\n",
            "\n",
            ">>> Step 2394 metrics: {'loss': 0.2569, 'grad_norm': 6.5625, 'learning_rate': 4.444471808546478e-05, 'epoch': 3.257142857142857}\n",
            "\n",
            ">>> Step 2395 metrics: {'loss': 0.3399, 'grad_norm': 7.96875, 'learning_rate': 4.4440235742034503e-05, 'epoch': 3.258503401360544}\n",
            "\n",
            ">>> Step 2396 metrics: {'loss': 0.224, 'grad_norm': 6.0, 'learning_rate': 4.4435751817238704e-05, 'epoch': 3.2598639455782314}\n",
            "\n",
            ">>> Step 2397 metrics: {'loss': 0.1679, 'grad_norm': 5.9375, 'learning_rate': 4.443126631144213e-05, 'epoch': 3.2612244897959184}\n",
            "\n",
            ">>> Step 2398 metrics: {'loss': 0.2066, 'grad_norm': 7.21875, 'learning_rate': 4.442677922500965e-05, 'epoch': 3.2625850340136053}\n",
            "\n",
            ">>> Step 2399 metrics: {'loss': 0.2955, 'grad_norm': 6.375, 'learning_rate': 4.442229055830626e-05, 'epoch': 3.2639455782312927}\n",
            "\n",
            ">>> Step 2400 metrics: {'loss': 0.2522, 'grad_norm': 9.625, 'learning_rate': 4.441780031169709e-05, 'epoch': 3.2653061224489797}\n",
            "\n",
            ">>> Step 2401 metrics: {'loss': 0.2128, 'grad_norm': 6.59375, 'learning_rate': 4.441330848554742e-05, 'epoch': 3.2666666666666666}\n",
            "\n",
            ">>> Step 2402 metrics: {'loss': 0.4615, 'grad_norm': 9.625, 'learning_rate': 4.440881508022261e-05, 'epoch': 3.2680272108843536}\n",
            "\n",
            ">>> Step 2403 metrics: {'loss': 0.2677, 'grad_norm': 7.09375, 'learning_rate': 4.440432009608819e-05, 'epoch': 3.269387755102041}\n",
            "\n",
            ">>> Step 2404 metrics: {'loss': 0.4182, 'grad_norm': 20.0, 'learning_rate': 4.439982353350981e-05, 'epoch': 3.270748299319728}\n",
            "\n",
            ">>> Step 2405 metrics: {'loss': 0.1879, 'grad_norm': 6.34375, 'learning_rate': 4.439532539285323e-05, 'epoch': 3.272108843537415}\n",
            "\n",
            ">>> Step 2406 metrics: {'loss': 0.2176, 'grad_norm': 6.46875, 'learning_rate': 4.439082567448436e-05, 'epoch': 3.273469387755102}\n",
            "\n",
            ">>> Step 2407 metrics: {'loss': 0.1775, 'grad_norm': 5.25, 'learning_rate': 4.438632437876922e-05, 'epoch': 3.274829931972789}\n",
            "\n",
            ">>> Step 2408 metrics: {'loss': 0.22, 'grad_norm': 5.9375, 'learning_rate': 4.4381821506073976e-05, 'epoch': 3.276190476190476}\n",
            "\n",
            ">>> Step 2409 metrics: {'loss': 0.3663, 'grad_norm': 6.625, 'learning_rate': 4.437731705676491e-05, 'epoch': 3.277551020408163}\n",
            "\n",
            ">>> Step 2410 metrics: {'loss': 0.4315, 'grad_norm': 7.90625, 'learning_rate': 4.437281103120845e-05, 'epoch': 3.2789115646258504}\n",
            "\n",
            ">>> Step 2411 metrics: {'loss': 0.1176, 'grad_norm': 8.6875, 'learning_rate': 4.4368303429771116e-05, 'epoch': 3.2802721088435374}\n",
            "\n",
            ">>> Step 2412 metrics: {'loss': 0.1923, 'grad_norm': 6.0625, 'learning_rate': 4.436379425281958e-05, 'epoch': 3.2816326530612243}\n",
            "\n",
            ">>> Step 2413 metrics: {'loss': 0.247, 'grad_norm': 7.03125, 'learning_rate': 4.4359283500720675e-05, 'epoch': 3.2829931972789117}\n",
            "\n",
            ">>> Step 2414 metrics: {'loss': 0.5865, 'grad_norm': 8.0, 'learning_rate': 4.435477117384128e-05, 'epoch': 3.2843537414965986}\n",
            "\n",
            ">>> Step 2415 metrics: {'loss': 0.2996, 'grad_norm': 5.75, 'learning_rate': 4.435025727254849e-05, 'epoch': 3.2857142857142856}\n",
            "\n",
            ">>> Step 2416 metrics: {'loss': 0.1975, 'grad_norm': 7.625, 'learning_rate': 4.434574179720946e-05, 'epoch': 3.287074829931973}\n",
            "\n",
            ">>> Step 2417 metrics: {'loss': 0.1432, 'grad_norm': 5.34375, 'learning_rate': 4.4341224748191514e-05, 'epoch': 3.28843537414966}\n",
            "\n",
            ">>> Step 2418 metrics: {'loss': 0.5168, 'grad_norm': 8.25, 'learning_rate': 4.433670612586209e-05, 'epoch': 3.289795918367347}\n",
            "\n",
            ">>> Step 2419 metrics: {'loss': 0.2808, 'grad_norm': 7.1875, 'learning_rate': 4.4332185930588764e-05, 'epoch': 3.2911564625850342}\n",
            "\n",
            ">>> Step 2420 metrics: {'loss': 0.3611, 'grad_norm': 6.65625, 'learning_rate': 4.432766416273922e-05, 'epoch': 3.292517006802721}\n",
            "\n",
            ">>> Step 2421 metrics: {'loss': 0.4567, 'grad_norm': 8.75, 'learning_rate': 4.432314082268128e-05, 'epoch': 3.293877551020408}\n",
            "\n",
            ">>> Step 2422 metrics: {'loss': 0.1339, 'grad_norm': 5.34375, 'learning_rate': 4.4318615910782904e-05, 'epoch': 3.295238095238095}\n",
            "\n",
            ">>> Step 2423 metrics: {'loss': 0.3934, 'grad_norm': 8.1875, 'learning_rate': 4.431408942741216e-05, 'epoch': 3.2965986394557825}\n",
            "\n",
            ">>> Step 2424 metrics: {'loss': 0.2936, 'grad_norm': 8.75, 'learning_rate': 4.4309561372937257e-05, 'epoch': 3.2979591836734694}\n",
            "\n",
            ">>> Step 2425 metrics: {'loss': 0.3595, 'grad_norm': 5.96875, 'learning_rate': 4.430503174772654e-05, 'epoch': 3.2993197278911564}\n",
            "\n",
            ">>> Step 2426 metrics: {'loss': 0.2401, 'grad_norm': 5.96875, 'learning_rate': 4.430050055214846e-05, 'epoch': 3.3006802721088437}\n",
            "\n",
            ">>> Step 2427 metrics: {'loss': 0.2511, 'grad_norm': 7.09375, 'learning_rate': 4.4295967786571614e-05, 'epoch': 3.3020408163265307}\n",
            "\n",
            ">>> Step 2428 metrics: {'loss': 0.272, 'grad_norm': 6.5625, 'learning_rate': 4.429143345136472e-05, 'epoch': 3.3034013605442176}\n",
            "\n",
            ">>> Step 2429 metrics: {'loss': 0.5637, 'grad_norm': 8.6875, 'learning_rate': 4.4286897546896604e-05, 'epoch': 3.3047619047619046}\n",
            "\n",
            ">>> Step 2430 metrics: {'loss': 0.23, 'grad_norm': 5.96875, 'learning_rate': 4.428236007353627e-05, 'epoch': 3.306122448979592}\n",
            "\n",
            ">>> Step 2431 metrics: {'loss': 0.9086, 'grad_norm': 10.9375, 'learning_rate': 4.42778210316528e-05, 'epoch': 3.307482993197279}\n",
            "\n",
            ">>> Step 2432 metrics: {'loss': 0.3654, 'grad_norm': 8.125, 'learning_rate': 4.427328042161543e-05, 'epoch': 3.308843537414966}\n",
            "\n",
            ">>> Step 2433 metrics: {'loss': 0.2295, 'grad_norm': 5.90625, 'learning_rate': 4.42687382437935e-05, 'epoch': 3.3102040816326532}\n",
            "\n",
            ">>> Step 2434 metrics: {'loss': 0.0892, 'grad_norm': 7.125, 'learning_rate': 4.426419449855651e-05, 'epoch': 3.31156462585034}\n",
            "\n",
            ">>> Step 2435 metrics: {'loss': 0.1491, 'grad_norm': 7.53125, 'learning_rate': 4.425964918627407e-05, 'epoch': 3.312925170068027}\n",
            "\n",
            ">>> Step 2436 metrics: {'loss': 0.1589, 'grad_norm': 7.34375, 'learning_rate': 4.4255102307315915e-05, 'epoch': 3.314285714285714}\n",
            "\n",
            ">>> Step 2437 metrics: {'loss': 0.2803, 'grad_norm': 7.78125, 'learning_rate': 4.425055386205191e-05, 'epoch': 3.3156462585034014}\n",
            "\n",
            ">>> Step 2438 metrics: {'loss': 0.283, 'grad_norm': 7.03125, 'learning_rate': 4.424600385085205e-05, 'epoch': 3.3170068027210884}\n",
            "\n",
            ">>> Step 2439 metrics: {'loss': 0.2045, 'grad_norm': 5.4375, 'learning_rate': 4.424145227408644e-05, 'epoch': 3.3183673469387753}\n",
            "\n",
            ">>> Step 2440 metrics: {'loss': 0.1499, 'grad_norm': 4.875, 'learning_rate': 4.4236899132125356e-05, 'epoch': 3.3197278911564627}\n",
            "\n",
            ">>> Step 2441 metrics: {'loss': 0.1701, 'grad_norm': 6.28125, 'learning_rate': 4.423234442533915e-05, 'epoch': 3.3210884353741497}\n",
            "\n",
            ">>> Step 2442 metrics: {'loss': 0.0765, 'grad_norm': 4.5, 'learning_rate': 4.422778815409834e-05, 'epoch': 3.3224489795918366}\n",
            "\n",
            ">>> Step 2443 metrics: {'loss': 0.6474, 'grad_norm': 9.25, 'learning_rate': 4.4223230318773554e-05, 'epoch': 3.323809523809524}\n",
            "\n",
            ">>> Step 2444 metrics: {'loss': 0.302, 'grad_norm': 8.375, 'learning_rate': 4.4218670919735535e-05, 'epoch': 3.325170068027211}\n",
            "\n",
            ">>> Step 2445 metrics: {'loss': 0.1985, 'grad_norm': 6.6875, 'learning_rate': 4.421410995735518e-05, 'epoch': 3.326530612244898}\n",
            "\n",
            ">>> Step 2446 metrics: {'loss': 0.3886, 'grad_norm': 7.21875, 'learning_rate': 4.420954743200349e-05, 'epoch': 3.3278911564625853}\n",
            "\n",
            ">>> Step 2447 metrics: {'loss': 0.1921, 'grad_norm': 6.03125, 'learning_rate': 4.420498334405162e-05, 'epoch': 3.329251700680272}\n",
            "\n",
            ">>> Step 2448 metrics: {'loss': 0.2206, 'grad_norm': 5.8125, 'learning_rate': 4.420041769387082e-05, 'epoch': 3.330612244897959}\n",
            "\n",
            ">>> Step 2449 metrics: {'loss': 0.2952, 'grad_norm': 6.875, 'learning_rate': 4.419585048183249e-05, 'epoch': 3.331972789115646}\n",
            "\n",
            ">>> Step 2450 metrics: {'loss': 0.5618, 'grad_norm': 7.9375, 'learning_rate': 4.4191281708308154e-05, 'epoch': 3.3333333333333335}\n",
            "\n",
            ">>> Step 2451 metrics: {'loss': 0.1781, 'grad_norm': 5.65625, 'learning_rate': 4.418671137366945e-05, 'epoch': 3.3346938775510204}\n",
            "\n",
            ">>> Step 2452 metrics: {'loss': 0.2247, 'grad_norm': 5.875, 'learning_rate': 4.418213947828816e-05, 'epoch': 3.3360544217687074}\n",
            "\n",
            ">>> Step 2453 metrics: {'loss': 0.2418, 'grad_norm': 7.125, 'learning_rate': 4.417756602253617e-05, 'epoch': 3.3374149659863948}\n",
            "\n",
            ">>> Step 2454 metrics: {'loss': 0.3889, 'grad_norm': 6.40625, 'learning_rate': 4.4172991006785514e-05, 'epoch': 3.3387755102040817}\n",
            "\n",
            ">>> Step 2455 metrics: {'loss': 0.2081, 'grad_norm': 6.6875, 'learning_rate': 4.416841443140836e-05, 'epoch': 3.3401360544217686}\n",
            "\n",
            ">>> Step 2456 metrics: {'loss': 0.2475, 'grad_norm': 6.84375, 'learning_rate': 4.416383629677697e-05, 'epoch': 3.3414965986394556}\n",
            "\n",
            ">>> Step 2457 metrics: {'loss': 0.1792, 'grad_norm': 6.8125, 'learning_rate': 4.415925660326378e-05, 'epoch': 3.342857142857143}\n",
            "\n",
            ">>> Step 2458 metrics: {'loss': 0.2297, 'grad_norm': 5.625, 'learning_rate': 4.415467535124129e-05, 'epoch': 3.34421768707483}\n",
            "\n",
            ">>> Step 2459 metrics: {'loss': 0.1819, 'grad_norm': 6.21875, 'learning_rate': 4.4150092541082174e-05, 'epoch': 3.345578231292517}\n",
            "\n",
            ">>> Step 2460 metrics: {'loss': 0.0878, 'grad_norm': 5.28125, 'learning_rate': 4.4145508173159245e-05, 'epoch': 3.3469387755102042}\n",
            "\n",
            ">>> Step 2461 metrics: {'loss': 0.3202, 'grad_norm': 8.625, 'learning_rate': 4.414092224784539e-05, 'epoch': 3.348299319727891}\n",
            "\n",
            ">>> Step 2462 metrics: {'loss': 0.1983, 'grad_norm': 6.59375, 'learning_rate': 4.4136334765513655e-05, 'epoch': 3.349659863945578}\n",
            "\n",
            ">>> Step 2463 metrics: {'loss': 0.474, 'grad_norm': 8.6875, 'learning_rate': 4.413174572653721e-05, 'epoch': 3.351020408163265}\n",
            "\n",
            ">>> Step 2464 metrics: {'loss': 0.5674, 'grad_norm': 9.0, 'learning_rate': 4.412715513128936e-05, 'epoch': 3.3523809523809525}\n",
            "\n",
            ">>> Step 2465 metrics: {'loss': 0.2247, 'grad_norm': 6.71875, 'learning_rate': 4.412256298014352e-05, 'epoch': 3.3537414965986394}\n",
            "\n",
            ">>> Step 2466 metrics: {'loss': 0.2601, 'grad_norm': 6.59375, 'learning_rate': 4.4117969273473236e-05, 'epoch': 3.3551020408163263}\n",
            "\n",
            ">>> Step 2467 metrics: {'loss': 0.3483, 'grad_norm': 7.0, 'learning_rate': 4.4113374011652184e-05, 'epoch': 3.3564625850340137}\n",
            "\n",
            ">>> Step 2468 metrics: {'loss': 0.3634, 'grad_norm': 8.625, 'learning_rate': 4.4108777195054175e-05, 'epoch': 3.3578231292517007}\n",
            "\n",
            ">>> Step 2469 metrics: {'loss': 0.5899, 'grad_norm': 8.8125, 'learning_rate': 4.410417882405312e-05, 'epoch': 3.3591836734693876}\n",
            "\n",
            ">>> Step 2470 metrics: {'loss': 0.4466, 'grad_norm': 7.5, 'learning_rate': 4.4099578899023084e-05, 'epoch': 3.360544217687075}\n",
            "\n",
            ">>> Step 2471 metrics: {'loss': 0.5855, 'grad_norm': 8.5625, 'learning_rate': 4.4094977420338244e-05, 'epoch': 3.361904761904762}\n",
            "\n",
            ">>> Step 2472 metrics: {'loss': 0.3099, 'grad_norm': 7.125, 'learning_rate': 4.409037438837292e-05, 'epoch': 3.363265306122449}\n",
            "\n",
            ">>> Step 2473 metrics: {'loss': 0.2315, 'grad_norm': 6.84375, 'learning_rate': 4.4085769803501524e-05, 'epoch': 3.3646258503401363}\n",
            "\n",
            ">>> Step 2474 metrics: {'loss': 0.5507, 'grad_norm': 9.75, 'learning_rate': 4.408116366609863e-05, 'epoch': 3.3659863945578232}\n",
            "\n",
            ">>> Step 2475 metrics: {'loss': 0.3837, 'grad_norm': 7.84375, 'learning_rate': 4.4076555976538926e-05, 'epoch': 3.36734693877551}\n",
            "\n",
            ">>> Step 2476 metrics: {'loss': 0.6097, 'grad_norm': 10.625, 'learning_rate': 4.407194673519721e-05, 'epoch': 3.368707482993197}\n",
            "\n",
            ">>> Step 2477 metrics: {'loss': 0.6669, 'grad_norm': 7.8125, 'learning_rate': 4.406733594244844e-05, 'epoch': 3.3700680272108845}\n",
            "\n",
            ">>> Step 2478 metrics: {'loss': 0.5249, 'grad_norm': 9.4375, 'learning_rate': 4.4062723598667665e-05, 'epoch': 3.3714285714285714}\n",
            "\n",
            ">>> Step 2479 metrics: {'loss': 0.1587, 'grad_norm': 4.875, 'learning_rate': 4.405810970423008e-05, 'epoch': 3.3727891156462584}\n",
            "\n",
            ">>> Step 2480 metrics: {'loss': 0.282, 'grad_norm': 5.84375, 'learning_rate': 4.4053494259511005e-05, 'epoch': 3.3741496598639458}\n",
            "\n",
            ">>> Step 2481 metrics: {'loss': 0.1429, 'grad_norm': 5.5625, 'learning_rate': 4.404887726488588e-05, 'epoch': 3.3755102040816327}\n",
            "\n",
            ">>> Step 2482 metrics: {'loss': 0.1646, 'grad_norm': 5.1875, 'learning_rate': 4.404425872073028e-05, 'epoch': 3.3768707482993197}\n",
            "\n",
            ">>> Step 2483 metrics: {'loss': 0.2068, 'grad_norm': 5.78125, 'learning_rate': 4.40396386274199e-05, 'epoch': 3.3782312925170066}\n",
            "\n",
            ">>> Step 2484 metrics: {'loss': 0.2445, 'grad_norm': 7.21875, 'learning_rate': 4.403501698533055e-05, 'epoch': 3.379591836734694}\n",
            "\n",
            ">>> Step 2485 metrics: {'loss': 0.3797, 'grad_norm': 7.65625, 'learning_rate': 4.403039379483819e-05, 'epoch': 3.380952380952381}\n",
            "\n",
            ">>> Step 2486 metrics: {'loss': 0.1786, 'grad_norm': 6.21875, 'learning_rate': 4.402576905631889e-05, 'epoch': 3.382312925170068}\n",
            "\n",
            ">>> Step 2487 metrics: {'loss': 0.2748, 'grad_norm': 8.375, 'learning_rate': 4.402114277014885e-05, 'epoch': 3.3836734693877553}\n",
            "\n",
            ">>> Step 2488 metrics: {'loss': 0.365, 'grad_norm': 6.90625, 'learning_rate': 4.401651493670439e-05, 'epoch': 3.385034013605442}\n",
            "\n",
            ">>> Step 2489 metrics: {'loss': 0.2667, 'grad_norm': 6.625, 'learning_rate': 4.401188555636197e-05, 'epoch': 3.386394557823129}\n",
            "\n",
            ">>> Step 2490 metrics: {'loss': 0.1233, 'grad_norm': 6.25, 'learning_rate': 4.400725462949815e-05, 'epoch': 3.387755102040816}\n",
            "\n",
            ">>> Step 2491 metrics: {'loss': 0.1122, 'grad_norm': 5.59375, 'learning_rate': 4.400262215648965e-05, 'epoch': 3.3891156462585035}\n",
            "\n",
            ">>> Step 2492 metrics: {'loss': 0.2225, 'grad_norm': 6.8125, 'learning_rate': 4.399798813771329e-05, 'epoch': 3.3904761904761904}\n",
            "\n",
            ">>> Step 2493 metrics: {'loss': 0.2388, 'grad_norm': 6.03125, 'learning_rate': 4.399335257354602e-05, 'epoch': 3.3918367346938774}\n",
            "\n",
            ">>> Step 2494 metrics: {'loss': 0.446, 'grad_norm': 8.0, 'learning_rate': 4.398871546436494e-05, 'epoch': 3.3931972789115648}\n"
          ]
        }
      ]
    }
  ]
}