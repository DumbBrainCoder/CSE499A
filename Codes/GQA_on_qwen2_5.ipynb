{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFRwdQ9bZIk6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling, AutoConfig, AutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "from transformers.models.qwen2.modeling_qwen2 import Qwen2DecoderLayer\n",
        "from torch import nn\n",
        "import math\n",
        "\n",
        "# Custom Grouped Query Attention (GQA) Layer\n",
        "class GroupedQueryAttention(Qwen2DecoderLayer):\n",
        "    def __init__(self, config, layer_idx):\n",
        "        super().__init__(config, layer_idx)\n",
        "        self.num_key_value_groups = self.self_attn.num_heads // self.self_attn.num_key_value_heads\n",
        "        # Adjust projections for GQA\n",
        "        self.self_attn.k_proj = nn.Linear(config.hidden_size, self.self_attn.num_key_value_heads * self.self_attn.head_dim, bias=True)\n",
        "        self.self_attn.v_proj = nn.Linear(config.hidden_size, self.self_attn.num_key_value_heads * self.self_attn.head_dim, bias=True)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None, position_ids=None, **kwargs):\n",
        "        bsz, q_len, _ = hidden_states.size()\n",
        "        query_states = self.self_attn.q_proj(hidden_states)\n",
        "        key_states = self.self_attn.k_proj(hidden_states)\n",
        "        value_states = self.self_attn.v_proj(hidden_states)\n",
        "\n",
        "        # Reshape for GQA: (batch, num_heads, seq_length, head_dim)\n",
        "        query_states = query_states.view(bsz, q_len, self.self_attn.num_heads, self.self_attn.head_dim).transpose(1, 2)\n",
        "        key_states = key_states.view(bsz, q_len, self.self_attn.num_key_value_heads, self.self_attn.head_dim).transpose(1, 2)\n",
        "        value_states = value_states.view(bsz, q_len, self.self_attn.num_key_value_heads, self.self_attn.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Repeat key/value heads to match number of query heads if necessary\n",
        "        key_states = key_states.repeat_interleave(self.num_key_value_groups, dim=1)\n",
        "        value_states = value_states.repeat_interleave(self.num_key_value_groups, dim=1)\n",
        "\n",
        "        # Apply rotary embeddings if available\n",
        "        if hasattr(self.self_attn, 'rotary_emb'):\n",
        "            query_states, key_states = self.self_attn.rotary_emb(query_states, key_states, position_ids)\n",
        "\n",
        "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.self_attn.head_dim)\n",
        "        if attention_mask is not None:\n",
        "            attn_weights = attn_weights + attention_mask\n",
        "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
        "        attn_output = torch.matmul(attn_weights, value_states)\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(bsz, q_len, self.self_attn.hidden_size)\n",
        "        attn_output = self.self_attn.o_proj(attn_output)\n",
        "\n",
        "        return attn_output, attn_weights\n",
        "\n",
        "# Custom Model with GQA integrated into decoder layers\n",
        "class Qwen2GQAForCausalLM(AutoModelForCausalLM):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.model.layers = nn.ModuleList(\n",
        "            [GroupedQueryAttention(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
        "        )\n",
        "\n",
        "# Set model name and update configuration to match checkpoint dimensions\n",
        "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "config.num_attention_heads = 14       # as required by the technical report\n",
        "config.num_key_value_heads = 2        # as required by the technical report\n",
        "config.hidden_size = 896              # updated to match checkpoint (was 512 before)\n",
        "config.intermediate_size = 4864       # updated to match checkpoint (was 13696 before)\n",
        "\n",
        "# Initialize model with GQA\n",
        "model = Qwen2GQAForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    config=config,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "model = model.to(\"cuda\")\n",
        "\n",
        "# Load tokenizer and dataset\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "dataset_name = \"GAIR/LIMO\"\n",
        "dataset = load_dataset(dataset_name)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    prompts = [\n",
        "        f\"Question: {q}\\nReasoning: {s}\\nAnswer: {a}\"\n",
        "        for q, s, a in zip(examples[\"question\"], examples[\"solution\"], examples[\"answer\"])\n",
        "    ]\n",
        "    return tokenizer(prompts, truncation=True, padding=\"max_length\", max_length=8192)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./qwen2.5_finetuned_limo\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=15,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=1,\n",
        "    learning_rate=5.0e-6,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.0,\n",
        "    logging_steps=1,\n",
        "    save_strategy=\"epoch\",\n",
        "    ddp_timeout=180000000,\n",
        "    bf16=True,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "train_dataset = tokenized_datasets[\"train\"]\n",
        "eval_dataset = tokenized_datasets.get(\"validation\")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "model.save_pretrained(\"./qwen2.5_finetuned_limo\")\n",
        "tokenizer.save_pretrained(\"./qwen2.5_finetuned_limo\")\n"
      ]
    }
  ]
}